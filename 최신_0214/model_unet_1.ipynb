{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "import openslide\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "# 이부분 python 에서는 뺴주기\n",
    "\n",
    "from skimage.filters import threshold_otsu\n",
    "from openslide.deepzoom import DeepZoomGenerator\n",
    "import cv2\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# network\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Lambda, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.models import load_model\n",
    "\n",
    "# Unet\n",
    "import numpy as np \n",
    "import os\n",
    "\n",
    "import skimage.transform as trans\n",
    "#import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras\n",
    "\n",
    "\n",
    "# train\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from datetime import datetime\n",
    "\n",
    "# evaluate\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "from PIL import Image\n",
    "from xml.etree.ElementTree import ElementTree, Element, SubElement\n",
    "from io import BytesIO\n",
    "import skimage.io as io\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing.image import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 256\n",
    "IS_TRAIN = True\n",
    "def find_patches_from_slide(slide_path, truth_path, patch_size=PATCH_SIZE,filter_non_tissue=True,filter_only_all_tumor=True):\n",
    "    \n",
    "    slide_contains_tumor = 'pos' in slide_path\n",
    "    \n",
    "    ############### read_region을 위한 start, level, size를 구함 #######################\n",
    "    BOUNDS_OFFSET_PROPS = (openslide.PROPERTY_NAME_BOUNDS_X, openslide.PROPERTY_NAME_BOUNDS_Y)\n",
    "    BOUNDS_SIZE_PROPS = (openslide.PROPERTY_NAME_BOUNDS_WIDTH, openslide.PROPERTY_NAME_BOUNDS_HEIGHT)\n",
    "\n",
    "    if slide_contains_tumor:\n",
    "        with openslide.open_slide(slide_path) as slide:\n",
    "            start = (int(slide.properties.get('openslide.bounds-x',0)),int(slide.properties.get('openslide.bounds-y',0)))\n",
    "            level = np.log2(patch_size) \n",
    "            level = int(level)\n",
    "            \n",
    "            size_scale = tuple(int(slide.properties.get(prop, l0_lim)) / l0_lim\n",
    "                            for prop, l0_lim in zip(BOUNDS_SIZE_PROPS,\n",
    "                            slide.dimensions))\n",
    "            _l_dimensions = tuple(tuple(int(math.ceil(l_lim * scale))\n",
    "                            for l_lim, scale in zip(l_size, size_scale))\n",
    "                            for l_size in slide.level_dimensions)\n",
    "            size = _l_dimensions[level]\n",
    "            \n",
    "            slide4 = slide.read_region(start,level,size) \n",
    "    else :\n",
    "        with openslide.open_slide(slide_path) as slide:\n",
    "            start = (0,0)\n",
    "            level = np.log2(patch_size) \n",
    "            level = int(level)\n",
    "            \n",
    "            size_scale = (1,1)\n",
    "            _l_dimensions = tuple(tuple(int(math.ceil(l_lim * scale))\n",
    "                            for l_lim, scale in zip(l_size, size_scale))\n",
    "                            for l_size in slide.level_dimensions)\n",
    "            size = _l_dimensions[level]\n",
    "            \n",
    "            slide4 = slide.read_region(start,level,size) \n",
    "    ####################################################################################\n",
    "    \n",
    "    \n",
    "    # is_tissue 부분 \n",
    "    slide4_grey = np.array(slide4.convert('L'))\n",
    "    binary = slide4_grey > 0  # black이면 0임\n",
    "    \n",
    "    # 검은색 제외하고 흰색영역(배경이라고 여겨지는)에 대해서도 작업해주어야함.\n",
    "    slide4_not_black = slide4_grey[slide4_grey>0]\n",
    "    thresh = threshold_otsu(slide4_not_black)\n",
    "    \n",
    "    I, J = slide4_grey.shape\n",
    "    for i in range(I):\n",
    "        for j in range(J):\n",
    "            if slide4_grey[i,j] > thresh :\n",
    "                binary[i,j] = False\n",
    "    patches = pd.DataFrame(pd.DataFrame(binary).stack())\n",
    "    patches['is_tissue'] = patches[0]\n",
    "    patches.drop(0, axis=1,inplace =True)\n",
    "    patches.loc[:,'slide_path'] = slide_path\n",
    "    \n",
    "#     # Test 이면 \n",
    "#     if IS_TEST:\n",
    "#         return patches\n",
    "\n",
    "    # is_tumor 부분\n",
    "    if slide_contains_tumor:\n",
    "        with openslide.open_slide(truth_path) as truth:\n",
    "            thumbnail_truth = truth.get_thumbnail(size) \n",
    "        \n",
    "        patches_y = pd.DataFrame(pd.DataFrame(np.array(thumbnail_truth.convert(\"L\"))).stack())\n",
    "        patches_y['is_tumor'] = patches_y[0] > 0\n",
    "        \n",
    "        # mask된 영역이 애매할 수도 있으므로\n",
    "        patches_y['is_all_tumor'] = patches_y[0] == 255\n",
    "        patches_y.drop(0, axis=1, inplace=True)\n",
    "        samples = pd.concat([patches, patches_y], axis=1) #len(samples)\n",
    "    else:\n",
    "        samples = patches\n",
    "        #dfmi.loc[:,('one','second')] = value\n",
    "        samples.loc[:,'is_tumor'] = False\n",
    "        samples.loc[:,'is_all_tumor'] = False\n",
    "    \n",
    "    if filter_non_tissue:\n",
    "        samples = samples[samples.is_tissue == True] # remove patches with no tissue #samples = samples[samples.is_tissue == True]\n",
    "    \n",
    "    if filter_only_all_tumor :\n",
    "        samples['tile_loc'] = list(samples.index)\n",
    "        all_tissue_samples1 = samples[samples.is_tumor==False]\n",
    "        all_tissue_samples1 = all_tissue_samples1.append(samples[samples.is_all_tumor==True])\n",
    "        \n",
    "        all_tissue_samples1.reset_index(inplace=True, drop=True)\n",
    "    else :\n",
    "        return samples\n",
    "    \n",
    "    return all_tissue_samples1\n",
    "NUM_CLASSES = 2 # not_tumor, tumor\n",
    "\n",
    "file_handles=[]\n",
    "def gen_imgs(all_image_path, all_mask_path, samples, batch_size, patch_size = PATCH_SIZE, shuffle=True):\n",
    "   \n",
    "    num_samples = len(samples)\n",
    "    # 특정 몇개의 slide만 open 해서 쓰기\n",
    "    # 4개씩 묶었으니까 \n",
    "  \n",
    "    slide_path0 = all_image_path[0]\n",
    "    slide_path1 = all_image_path[1]\n",
    "    slide_path2 = all_image_path[2]\n",
    "    slide_path3 = all_image_path[3]\n",
    "    \n",
    "    \n",
    "    # slide 0~3 까지 미리 열어두기\n",
    "    slide0 = openslide.open_slide(slide_path0)\n",
    "    slide1 = openslide.open_slide(slide_path1)\n",
    "    slide2 = openslide.open_slide(slide_path2)\n",
    "    slide3 = openslide.open_slide(slide_path3)\n",
    "    file_handles.append(slide0)\n",
    "    file_handles.append(slide1)\n",
    "    file_handles.append(slide2)\n",
    "    file_handles.append(slide3)\n",
    "    \n",
    "    # with openslide.open_slide(slide_path) as slide\n",
    "    tiles0 = DeepZoomGenerator(slide0,tile_size=patch_size, overlap=0, limit_bounds=False) \n",
    "    tiles1 = DeepZoomGenerator(slide1,tile_size=patch_size, overlap=0, limit_bounds=False)\n",
    "    tiles2 = DeepZoomGenerator(slide2,tile_size=patch_size, overlap=0, limit_bounds=False)\n",
    "    tiles3 = DeepZoomGenerator(slide3,tile_size=patch_size, overlap=0, limit_bounds=False)\n",
    "    \n",
    "    \n",
    "    if 'pos' in slide_path0:\n",
    "        start_x0 = int(slide0.properties.get('openslide.bounds-x',0))\n",
    "        start_y0 = int(slide0.properties.get('openslide.bounds-y',0))\n",
    "        start_x0 = start_x0 / patch_size\n",
    "        start_y0 = start_y0 / patch_size\n",
    "        \n",
    "        truth0 = openslide.open_slide(all_mask_path[0])\n",
    "        truth_tiles0 = DeepZoomGenerator(truth0, tile_size=16, overlap=0, limit_bounds=False) \n",
    "        \n",
    "    else : \n",
    "        start_x0 = 0\n",
    "        start_y0 = 0\n",
    "    \n",
    "    if 'pos' in slide_path1:\n",
    "        start_x1 = int(slide1.properties.get('openslide.bounds-x',0))\n",
    "        start_y1 = int(slide1.properties.get('openslide.bounds-y',0))\n",
    "        start_x1 = start_x1 / patch_size\n",
    "        start_y1 = start_y1 / patch_size\n",
    "        \n",
    "        truth1 = openslide.open_slide(all_mask_path[1])\n",
    "        truth_tiles1 = DeepZoomGenerator(truth1, tile_size=16, overlap=0, limit_bounds=False) \n",
    "        \n",
    "    else : \n",
    "        start_x1 = 0\n",
    "        start_y1 = 0\n",
    "    \n",
    "    if 'pos' in slide_path2:\n",
    "        start_x2 = int(slide2.properties.get('openslide.bounds-x',0))\n",
    "        start_y2 = int(slide2.properties.get('openslide.bounds-y',0))\n",
    "        start_x2 = start_x2 / patch_size\n",
    "        start_y2 = start_y2 / patch_size\n",
    "        \n",
    "        truth2 = openslide.open_slide(all_mask_path[2])\n",
    "        truth_tiles2 = DeepZoomGenerator(truth2, tile_size=16, overlap=0, limit_bounds=False) \n",
    "        \n",
    "    else : \n",
    "        start_x2 = 0\n",
    "        start_y2 = 0\n",
    "        \n",
    "    if 'pos' in slide_path3:\n",
    "        start_x3 = int(slide3.properties.get('openslide.bounds-x',0))\n",
    "        start_y3 = int(slide3.properties.get('openslide.bounds-y',0))\n",
    "        start_x3 = start_x3 / patch_size\n",
    "        start_y3 = start_y3 / patch_size\n",
    "        \n",
    "        truth3 = openslide.open_slide(all_mask_path[3])\n",
    "        truth_tiles3 = DeepZoomGenerator(truth3, tile_size=16, overlap=0, limit_bounds=False) \n",
    "        \n",
    "    else : \n",
    "        start_x3 = 0\n",
    "        start_y3 = 0\n",
    "    \n",
    "\n",
    "    \n",
    "    for epo in range(10): # Loop forever so the generator never terminates\n",
    "        if shuffle:\n",
    "            samples = samples.sample(frac=1) # shuffle samples\n",
    "\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples.iloc[offset:offset+batch_size]\n",
    "            images = []\n",
    "            masks = []\n",
    "            for _, batch_sample in batch_samples.iterrows(): # 배치마다 deep zoom 하네 약간 비효율적\n",
    "                \n",
    "                # 여기서 하나씩 4개 체크해서 해당되는 부분으로 가야지. for 4번 돌리면서 가야한다.\n",
    "                mask_size_up = np.zeros((patch_size,patch_size))\n",
    "                a,b=mask_size_up.shape\n",
    "                \n",
    "                if batch_sample.slide_path == slide_path0:\n",
    "                    x, y = batch_sample.tile_loc[::-1]\n",
    "                    x += start_x0\n",
    "                    y += start_y0\n",
    "                    img = tiles0.get_tile(tiles0.level_count-1, (x,y))\n",
    "                    if 'pos' in slide_path0:\n",
    "                        mask = truth_tiles0.get_tile(truth_tiles0.level_count-1, batch_sample.tile_loc[::-1])\n",
    "                        mask = (cv2.cvtColor(np.array(mask), cv2.COLOR_RGB2GRAY) > 0).astype(int)\n",
    "                            # mask_size_up , 16 to 256\n",
    "                        for i in range(a):\n",
    "                            for j in range(b) :\n",
    "                                k = i//16\n",
    "                                l = j//16\n",
    "                                mask_size_up[i,j] = mask[k,l]\n",
    "                    \n",
    "                elif batch_sample.slide_path == slide_path1:\n",
    "                    x, y = batch_sample.tile_loc[::-1]\n",
    "                    x += start_x1\n",
    "                    y += start_y1\n",
    "                    img = tiles1.get_tile(tiles1.level_count-1, (x,y))\n",
    "                    if 'pos' in slide_path1:\n",
    "                        mask = truth_tiles1.get_tile(truth_tiles1.level_count-1, batch_sample.tile_loc[::-1])\n",
    "                        mask = (cv2.cvtColor(np.array(mask), cv2.COLOR_RGB2GRAY) > 0).astype(int)\n",
    "                            # mask_size_up , 16 to 256\n",
    "                        for i in range(a):\n",
    "                            for j in range(b) :\n",
    "                                k = i//16\n",
    "                                l = j//16\n",
    "                                mask_size_up[i,j] = mask[k,l]\n",
    "                \n",
    "                elif batch_sample.slide_path == slide_path2:\n",
    "                    x, y = batch_sample.tile_loc[::-1]\n",
    "                    x += start_x2\n",
    "                    y += start_y2\n",
    "                    img = tiles2.get_tile(tiles2.level_count-1, (x,y))\n",
    "                    if 'pos' in slide_path2:\n",
    "                        mask = truth_tiles2.get_tile(truth_tiles2.level_count-1, batch_sample.tile_loc[::-1])\n",
    "                        mask = (cv2.cvtColor(np.array(mask), cv2.COLOR_RGB2GRAY) > 0).astype(int)\n",
    "                            # mask_size_up , 16 to 256\n",
    "                        for i in range(a):\n",
    "                            for j in range(b) :\n",
    "                                k = i//16\n",
    "                                l = j//16\n",
    "                                mask_size_up[i,j] = mask[k,l]\n",
    "                \n",
    "                else:\n",
    "                    x, y = batch_sample.tile_loc[::-1]\n",
    "                    x += start_x3\n",
    "                    y += start_y3\n",
    "                    img = tiles3.get_tile(tiles3.level_count-1, (x,y))\n",
    "                    if 'pos' in slide_path3:\n",
    "                        mask = truth_tiles3.get_tile(truth_tiles3.level_count-1, batch_sample.tile_loc[::-1])\n",
    "                        mask = (cv2.cvtColor(np.array(mask), cv2.COLOR_RGB2GRAY) > 0).astype(int)\n",
    "                            # mask_size_up , 16 to 256\n",
    "                        for i in range(a):\n",
    "                            for j in range(b) :\n",
    "                                k = i//16\n",
    "                                l = j//16\n",
    "                                mask_size_up[i,j] = mask[k,l]\n",
    "                \n",
    "                    \n",
    "\n",
    "                images.append(np.array(img))\n",
    "                masks.append(mask_size_up)\n",
    "\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(masks)\n",
    "            #print('x_train_shape :', X_train.shape)\n",
    "            \n",
    "            y_train = to_categorical(y_train, num_classes=2).reshape(y_train.shape[0], patch_size, patch_size, 2) \n",
    "            #print('y_train_shape : ',y_train.shape)\n",
    "            \n",
    "            #X_train, y_train = datagen().flow(X_train,y = y_train,batch_size = batch_size)\n",
    "            X_train, y_train = next(ImageDataGenerator(\n",
    "                rotation_range=90,\n",
    "                horizontal_flip=True,\n",
    "                vertical_flip=True,\n",
    "                brightness_range =(0.25,1.)).flow(X_train,y=y_train,batch_size=batch_size))\n",
    "            #print(X_train.shape)\n",
    "            #print(y_train.shape)\n",
    "            yield X_train, y_train\n",
    "            \n",
    "def predict_batch_from_model(patches, model):\n",
    "    \"\"\"Predict which pixels are tumor.\n",
    "    \n",
    "    input: patch: `batch_size`x256x256x3, rgb image\n",
    "    input: model: keras model\n",
    "    output: prediction: 256x256x1, per-pixel tumor probability\n",
    "    \"\"\"\n",
    "    predictions = model.predict(patches)\n",
    "    predictions = predictions[:, :, :, 1]\n",
    "    return predictions\n",
    "def predict_from_model(patch, model):\n",
    "    \"\"\"Predict which pixels are tumor.\n",
    "    \n",
    "    input: patch: 256x256x3, rgb image\n",
    "    input: model: keras model\n",
    "    output: prediction: 256x256x1, per-pixel tumor probability\n",
    "    \"\"\"\n",
    "    \n",
    "    prediction = model.predict(patch.reshape(1, 256, 256, 3))\n",
    "    prediction = prediction[:, :, :, 1].reshape(256, 256)\n",
    "    return prediction\n",
    "\n",
    "def predict_from_model_n(patch, model):\n",
    "    \"\"\"Predict which pixels are tumor.\n",
    "    \n",
    "    input: patch: 256x256x3, rgb image\n",
    "    input: model: keras model\n",
    "    output: prediction: 256x256x1, per-pixel tumor probability\n",
    "    \"\"\"\n",
    "    \n",
    "    prediction = model.predict(patch.reshape(1, 256, 256, 3))\n",
    "    prediction = prediction[:, :, :, 0].reshape(256, 256)\n",
    "    return prediction\n",
    "\n",
    "def simple_model(pretrained_weights = None):\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(256, 256, 3)))\n",
    "    model.add(Convolution2D(100, (3, 3), strides=(2, 2), activation='elu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Convolution2D(200, (3, 3), strides=(2, 2), activation='elu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Convolution2D(300, (3, 3), activation='elu', padding='same'))\n",
    "    model.add(Convolution2D(300, (3, 3), activation='elu',  padding='same'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Convolution2D(2, (1, 1))) # this is called upscore layer for some reason?\n",
    "    model.add(Conv2DTranspose(2, (31, 31), strides=(16, 16), activation='softmax', padding='same'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    if(pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_path # :  157\n",
      "mask_patch # :  157\n"
     ]
    }
   ],
   "source": [
    "def read_data_path():\n",
    "    image_paths = []\n",
    "    with open('train.txt','r') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip('\\n')\n",
    "            image_paths.append(line)\n",
    "    #print('image_path # : ',len(image_paths))\n",
    "\n",
    "    tumor_mask_paths = []\n",
    "\n",
    "    with open('train_mask.txt','r') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip('\\n')\n",
    "            tumor_mask_paths.append(line)\n",
    "    #print('mask_patch # : ',len(tumor_mask_paths))\n",
    "    \n",
    "    return image_paths, tumor_mask_paths\n",
    "\n",
    "def read_test_data_path():\n",
    "    image_paths = []\n",
    "    with open('test.txt','r') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip('\\n')\n",
    "            image_paths.append(line)\n",
    "    #print('image_path # : ',len(image_paths))\n",
    "    \n",
    "    return image_paths\n",
    "\n",
    "test_image_paths = read_test_data_path()\n",
    "image_paths, tumor_mask_paths = read_data_path()\n",
    "image_paths = []\n",
    "with open('train.txt','r') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip('\\n')\n",
    "        image_paths.append(line)\n",
    "print('image_path # : ',len(image_paths))\n",
    "\n",
    "tumor_mask_paths = []\n",
    "with open('train_mask.txt','r') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip('\\n')\n",
    "        tumor_mask_paths.append(line)\n",
    "print('mask_patch # : ',len(tumor_mask_paths))\n",
    "\n",
    "slide_4_list_1 = [[102,104,29,44],[144,55,30,18],[125,56,35,40],[54,65,21,36],[139,82,1,49],[73,108,7,23],[107,117,24,52],[106,103,27,13]\n",
    "               ,[105,151,15,2],[75,100,41,9],[156,113,32,37],[150,88,39,10],[84,122,5,50],[93,118,53,47],[87,78,45,34],[116,98,48,46],\n",
    "                [72,131,22,42]]\n",
    "slide_4_list_2 = [[109,58,14,28],[101,69,11,43],[94,74,3,20],[64,140,17,16],[92,154,8,26],[99,60,0,33],[86,146,25,19],[68,112,38,51],\n",
    "                 [71,136,31,4],[59,91,12,6]]\n",
    "slide_4_list_3 = [[143,132,124,85],[95,120,81,77],[97,96,110,83],[152,128,149,155],[153,111,57,138],[134,135,114,76],\n",
    "                  [123,90,121,61],[147,148,119,142],[66,137,63,80],[70,79,115,133],[129,141,127,145]]\n",
    "slide_4_test = [[55,55,0,0],[55,55,0,0]]\n",
    "\n",
    "all_image_path = []\n",
    "all_mask_path = []\n",
    "for j in range(4):\n",
    "    image_path = image_paths[slide_4_test[0][j]][1:] # 이 부분은 data 읽을때 고치자 ( [1:] 빼야함)\n",
    "    mask_path = tumor_mask_paths[slide_4_test[0][j]][1:] # 이 부분은 data 읽을때 고치자\n",
    "    all_image_path.append(image_path)\n",
    "    all_mask_path.append(mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet(pretrained_weights = None,input_size = (256,256,3)):\n",
    "    inputs = Input(input_size)\n",
    "    inputs_norm = Lambda(lambda x: x /255.0 - 0.5)(inputs)\n",
    "    conv1 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs_norm)\n",
    "    conv1 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(32, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(16, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    conv9 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv10 = Conv2D(2, 1, activation = 'softmax')(conv9)\n",
    "\n",
    "    model = Model(input = inputs, output = conv10)\n",
    "\n",
    "    #model.compile(optimizer = Adam(lr = 1e-2), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    #model.summary()\n",
    "\n",
    "    if(pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:44: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n"
     ]
    }
   ],
   "source": [
    "model = unet('unet_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:86: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61446\n",
      "160136\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/704 [=======>......................] - ETA: 1:34:28 - loss: 0.1088 - acc: 0.96 - ETA: 58:30 - loss: 0.1176 - acc: 0.9651 - ETA: 56:46 - loss: 0.1220 - acc: 0.96 - ETA: 55:22 - loss: 0.1288 - acc: 0.95 - ETA: 54:43 - loss: 0.1557 - acc: 0.95 - ETA: 54:05 - loss: 0.1658 - acc: 0.94 - ETA: 53:32 - loss: 0.1753 - acc: 0.94 - ETA: 53:15 - loss: 0.1661 - acc: 0.94 - ETA: 53:09 - loss: 0.1610 - acc: 0.95 - ETA: 52:53 - loss: 0.1531 - acc: 0.95 - ETA: 52:41 - loss: 0.1482 - acc: 0.95 - ETA: 51:12 - loss: 0.1563 - acc: 0.94 - ETA: 49:39 - loss: 0.1526 - acc: 0.95 - ETA: 48:21 - loss: 0.1519 - acc: 0.95 - ETA: 47:14 - loss: 0.1475 - acc: 0.95 - ETA: 46:14 - loss: 0.1476 - acc: 0.95 - ETA: 45:22 - loss: 0.1465 - acc: 0.95 - ETA: 44:36 - loss: 0.1441 - acc: 0.95 - ETA: 43:53 - loss: 0.1392 - acc: 0.95 - ETA: 43:15 - loss: 0.1363 - acc: 0.95 - ETA: 42:39 - loss: 0.1381 - acc: 0.95 - ETA: 42:07 - loss: 0.1376 - acc: 0.95 - ETA: 41:39 - loss: 0.1355 - acc: 0.95 - ETA: 41:10 - loss: 0.1364 - acc: 0.95 - ETA: 40:44 - loss: 0.1349 - acc: 0.95 - ETA: 40:22 - loss: 0.1350 - acc: 0.95 - ETA: 39:59 - loss: 0.1391 - acc: 0.95 - ETA: 39:37 - loss: 0.1376 - acc: 0.95 - ETA: 39:17 - loss: 0.1337 - acc: 0.95 - ETA: 38:58 - loss: 0.1321 - acc: 0.95 - ETA: 38:39 - loss: 0.1296 - acc: 0.95 - ETA: 38:22 - loss: 0.1289 - acc: 0.95 - ETA: 38:08 - loss: 0.1296 - acc: 0.95 - ETA: 37:54 - loss: 0.1288 - acc: 0.95 - ETA: 37:40 - loss: 0.1268 - acc: 0.95 - ETA: 37:25 - loss: 0.1261 - acc: 0.95 - ETA: 37:13 - loss: 0.1261 - acc: 0.95 - ETA: 37:01 - loss: 0.1261 - acc: 0.95 - ETA: 36:50 - loss: 0.1268 - acc: 0.95 - ETA: 36:39 - loss: 0.1257 - acc: 0.95 - ETA: 36:27 - loss: 0.1247 - acc: 0.95 - ETA: 36:18 - loss: 0.1249 - acc: 0.95 - ETA: 36:13 - loss: 0.1239 - acc: 0.95 - ETA: 36:03 - loss: 0.1243 - acc: 0.95 - ETA: 35:54 - loss: 0.1224 - acc: 0.95 - ETA: 35:47 - loss: 0.1214 - acc: 0.95 - ETA: 35:38 - loss: 0.1203 - acc: 0.95 - ETA: 35:29 - loss: 0.1201 - acc: 0.95 - ETA: 35:21 - loss: 0.1189 - acc: 0.95 - ETA: 35:13 - loss: 0.1176 - acc: 0.95 - ETA: 35:06 - loss: 0.1176 - acc: 0.95 - ETA: 34:58 - loss: 0.1163 - acc: 0.95 - ETA: 34:51 - loss: 0.1165 - acc: 0.95 - ETA: 34:43 - loss: 0.1168 - acc: 0.95 - ETA: 34:36 - loss: 0.1162 - acc: 0.95 - ETA: 34:28 - loss: 0.1151 - acc: 0.95 - ETA: 34:21 - loss: 0.1141 - acc: 0.95 - ETA: 34:14 - loss: 0.1128 - acc: 0.96 - ETA: 34:07 - loss: 0.1120 - acc: 0.96 - ETA: 34:00 - loss: 0.1110 - acc: 0.96 - ETA: 33:54 - loss: 0.1105 - acc: 0.96 - ETA: 33:47 - loss: 0.1097 - acc: 0.96 - ETA: 33:41 - loss: 0.1098 - acc: 0.96 - ETA: 33:35 - loss: 0.1094 - acc: 0.96 - ETA: 33:29 - loss: 0.1092 - acc: 0.96 - ETA: 33:23 - loss: 0.1084 - acc: 0.96 - ETA: 33:17 - loss: 0.1079 - acc: 0.96 - ETA: 33:10 - loss: 0.1067 - acc: 0.96 - ETA: 33:05 - loss: 0.1076 - acc: 0.96 - ETA: 33:00 - loss: 0.1071 - acc: 0.96 - ETA: 32:54 - loss: 0.1067 - acc: 0.96 - ETA: 32:49 - loss: 0.1057 - acc: 0.96 - ETA: 32:44 - loss: 0.1062 - acc: 0.96 - ETA: 32:41 - loss: 0.1052 - acc: 0.96 - ETA: 32:36 - loss: 0.1045 - acc: 0.96 - ETA: 32:31 - loss: 0.1036 - acc: 0.96 - ETA: 32:25 - loss: 0.1031 - acc: 0.96 - ETA: 32:21 - loss: 0.1037 - acc: 0.96 - ETA: 32:15 - loss: 0.1030 - acc: 0.96 - ETA: 32:10 - loss: 0.1031 - acc: 0.96 - ETA: 32:06 - loss: 0.1027 - acc: 0.96 - ETA: 32:01 - loss: 0.1024 - acc: 0.96 - ETA: 31:56 - loss: 0.1021 - acc: 0.96 - ETA: 31:50 - loss: 0.1018 - acc: 0.96 - ETA: 31:45 - loss: 0.1021 - acc: 0.96 - ETA: 31:41 - loss: 0.1014 - acc: 0.96 - ETA: 31:37 - loss: 0.1009 - acc: 0.96 - ETA: 31:32 - loss: 0.1007 - acc: 0.96 - ETA: 31:27 - loss: 0.1002 - acc: 0.96 - ETA: 31:23 - loss: 0.1013 - acc: 0.96 - ETA: 31:18 - loss: 0.1005 - acc: 0.96 - ETA: 31:15 - loss: 0.1012 - acc: 0.96 - ETA: 31:10 - loss: 0.1013 - acc: 0.96 - ETA: 31:06 - loss: 0.1008 - acc: 0.96 - ETA: 31:01 - loss: 0.1003 - acc: 0.96 - ETA: 30:57 - loss: 0.0997 - acc: 0.96 - ETA: 30:53 - loss: 0.1003 - acc: 0.96 - ETA: 30:49 - loss: 0.0999 - acc: 0.96 - ETA: 30:45 - loss: 0.1010 - acc: 0.96 - ETA: 30:41 - loss: 0.1004 - acc: 0.96 - ETA: 30:37 - loss: 0.1001 - acc: 0.96 - ETA: 30:33 - loss: 0.1002 - acc: 0.96 - ETA: 30:29 - loss: 0.0998 - acc: 0.96 - ETA: 30:25 - loss: 0.0999 - acc: 0.96 - ETA: 30:22 - loss: 0.0995 - acc: 0.96 - ETA: 30:18 - loss: 0.0993 - acc: 0.96 - ETA: 30:14 - loss: 0.0987 - acc: 0.96 - ETA: 30:11 - loss: 0.0987 - acc: 0.96 - ETA: 30:07 - loss: 0.0983 - acc: 0.96 - ETA: 30:03 - loss: 0.0978 - acc: 0.96 - ETA: 29:59 - loss: 0.0978 - acc: 0.96 - ETA: 29:55 - loss: 0.0986 - acc: 0.96 - ETA: 29:52 - loss: 0.0984 - acc: 0.96 - ETA: 29:48 - loss: 0.0980 - acc: 0.96 - ETA: 29:44 - loss: 0.0978 - acc: 0.96 - ETA: 29:40 - loss: 0.0977 - acc: 0.96 - ETA: 29:36 - loss: 0.0973 - acc: 0.96 - ETA: 29:32 - loss: 0.0971 - acc: 0.96 - ETA: 29:29 - loss: 0.0971 - acc: 0.96 - ETA: 29:25 - loss: 0.0973 - acc: 0.96 - ETA: 29:21 - loss: 0.0979 - acc: 0.96 - ETA: 29:17 - loss: 0.0977 - acc: 0.96 - ETA: 29:13 - loss: 0.0978 - acc: 0.96 - ETA: 29:09 - loss: 0.0975 - acc: 0.96 - ETA: 29:06 - loss: 0.0975 - acc: 0.96 - ETA: 29:02 - loss: 0.0979 - acc: 0.96 - ETA: 28:59 - loss: 0.0980 - acc: 0.96 - ETA: 28:55 - loss: 0.0978 - acc: 0.96 - ETA: 28:51 - loss: 0.0976 - acc: 0.96 - ETA: 28:47 - loss: 0.0974 - acc: 0.96 - ETA: 28:43 - loss: 0.0971 - acc: 0.96 - ETA: 28:40 - loss: 0.0971 - acc: 0.96 - ETA: 28:36 - loss: 0.0968 - acc: 0.96 - ETA: 28:34 - loss: 0.0972 - acc: 0.96 - ETA: 28:30 - loss: 0.0975 - acc: 0.96 - ETA: 28:28 - loss: 0.0974 - acc: 0.96 - ETA: 28:24 - loss: 0.0972 - acc: 0.96 - ETA: 28:21 - loss: 0.0969 - acc: 0.96 - ETA: 28:17 - loss: 0.0970 - acc: 0.96 - ETA: 28:14 - loss: 0.0971 - acc: 0.96 - ETA: 28:10 - loss: 0.0968 - acc: 0.96 - ETA: 28:07 - loss: 0.0964 - acc: 0.96 - ETA: 28:03 - loss: 0.0966 - acc: 0.96 - ETA: 28:00 - loss: 0.0973 - acc: 0.96 - ETA: 27:57 - loss: 0.0969 - acc: 0.96 - ETA: 27:53 - loss: 0.0968 - acc: 0.96 - ETA: 27:50 - loss: 0.0965 - acc: 0.96 - ETA: 27:46 - loss: 0.0969 - acc: 0.96 - ETA: 27:42 - loss: 0.0968 - acc: 0.96 - ETA: 27:39 - loss: 0.0966 - acc: 0.96 - ETA: 27:35 - loss: 0.0963 - acc: 0.96 - ETA: 27:32 - loss: 0.0968 - acc: 0.96 - ETA: 27:28 - loss: 0.0973 - acc: 0.96 - ETA: 27:24 - loss: 0.0973 - acc: 0.96 - ETA: 27:21 - loss: 0.0975 - acc: 0.96 - ETA: 27:17 - loss: 0.0971 - acc: 0.96 - ETA: 27:14 - loss: 0.0967 - acc: 0.96 - ETA: 27:10 - loss: 0.0966 - acc: 0.96 - ETA: 27:07 - loss: 0.0964 - acc: 0.96 - ETA: 27:03 - loss: 0.0962 - acc: 0.96 - ETA: 26:59 - loss: 0.0963 - acc: 0.96 - ETA: 26:56 - loss: 0.0962 - acc: 0.96 - ETA: 26:52 - loss: 0.0959 - acc: 0.96 - ETA: 26:49 - loss: 0.0959 - acc: 0.96 - ETA: 26:46 - loss: 0.0957 - acc: 0.96 - ETA: 26:42 - loss: 0.0956 - acc: 0.96 - ETA: 26:40 - loss: 0.0956 - acc: 0.96 - ETA: 26:37 - loss: 0.0958 - acc: 0.96 - ETA: 26:33 - loss: 0.0959 - acc: 0.96 - ETA: 26:30 - loss: 0.0963 - acc: 0.96 - ETA: 26:27 - loss: 0.0963 - acc: 0.96 - ETA: 26:23 - loss: 0.0963 - acc: 0.96 - ETA: 26:20 - loss: 0.0960 - acc: 0.96 - ETA: 26:17 - loss: 0.0968 - acc: 0.96 - ETA: 26:14 - loss: 0.0974 - acc: 0.96 - ETA: 26:11 - loss: 0.0979 - acc: 0.96 - ETA: 26:08 - loss: 0.0979 - acc: 0.96 - ETA: 26:04 - loss: 0.0980 - acc: 0.96 - ETA: 26:01 - loss: 0.0983 - acc: 0.96 - ETA: 25:58 - loss: 0.0988 - acc: 0.96 - ETA: 25:55 - loss: 0.0988 - acc: 0.96 - ETA: 25:52 - loss: 0.0988 - acc: 0.96 - ETA: 25:48 - loss: 0.0988 - acc: 0.96 - ETA: 25:45 - loss: 0.0985 - acc: 0.96 - ETA: 25:42 - loss: 0.0990 - acc: 0.96 - ETA: 25:38 - loss: 0.0987 - acc: 0.96 - ETA: 25:35 - loss: 0.0992 - acc: 0.96 - ETA: 25:32 - loss: 0.0989 - acc: 0.96 - ETA: 25:28 - loss: 0.0988 - acc: 0.96 - ETA: 25:25 - loss: 0.0988 - acc: 0.96 - ETA: 25:22 - loss: 0.0985 - acc: 0.96 - ETA: 25:18 - loss: 0.0982 - acc: 0.96 - ETA: 25:15 - loss: 0.0987 - acc: 0.96 - ETA: 25:12 - loss: 0.0987 - acc: 0.96 - ETA: 25:08 - loss: 0.0985 - acc: 0.96 - ETA: 25:05 - loss: 0.0985 - acc: 0.96 - ETA: 25:02 - loss: 0.0986 - acc: 0.96 - ETA: 25:00 - loss: 0.0985 - acc: 0.96 - ETA: 24:56 - loss: 0.0982 - acc: 0.96 - ETA: 24:53 - loss: 0.0983 - acc: 0.96 - ETA: 24:50 - loss: 0.0982 - acc: 0.96 - ETA: 24:46 - loss: 0.0980 - acc: 0.96 - ETA: 24:43 - loss: 0.0979 - acc: 0.96 - ETA: 24:40 - loss: 0.0979 - acc: 0.9651\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408/704 [================>.............] - ETA: 24:37 - loss: 0.0976 - acc: 0.96 - ETA: 24:34 - loss: 0.0978 - acc: 0.96 - ETA: 24:30 - loss: 0.0978 - acc: 0.96 - ETA: 24:27 - loss: 0.0979 - acc: 0.96 - ETA: 24:24 - loss: 0.0977 - acc: 0.96 - ETA: 24:21 - loss: 0.0977 - acc: 0.96 - ETA: 24:17 - loss: 0.0976 - acc: 0.96 - ETA: 24:14 - loss: 0.0978 - acc: 0.96 - ETA: 24:11 - loss: 0.0980 - acc: 0.96 - ETA: 24:08 - loss: 0.0978 - acc: 0.96 - ETA: 24:05 - loss: 0.0974 - acc: 0.96 - ETA: 24:02 - loss: 0.0974 - acc: 0.96 - ETA: 23:58 - loss: 0.0974 - acc: 0.96 - ETA: 23:55 - loss: 0.0975 - acc: 0.96 - ETA: 23:52 - loss: 0.0975 - acc: 0.96 - ETA: 23:49 - loss: 0.0975 - acc: 0.96 - ETA: 23:46 - loss: 0.0975 - acc: 0.96 - ETA: 23:43 - loss: 0.0972 - acc: 0.96 - ETA: 23:40 - loss: 0.0972 - acc: 0.96 - ETA: 23:37 - loss: 0.0972 - acc: 0.96 - ETA: 23:34 - loss: 0.0971 - acc: 0.96 - ETA: 23:31 - loss: 0.0970 - acc: 0.96 - ETA: 23:27 - loss: 0.0971 - acc: 0.96 - ETA: 23:24 - loss: 0.0968 - acc: 0.96 - ETA: 23:22 - loss: 0.0970 - acc: 0.96 - ETA: 23:18 - loss: 0.0966 - acc: 0.96 - ETA: 23:15 - loss: 0.0966 - acc: 0.96 - ETA: 23:12 - loss: 0.0966 - acc: 0.96 - ETA: 23:09 - loss: 0.0970 - acc: 0.96 - ETA: 23:06 - loss: 0.0969 - acc: 0.96 - ETA: 23:03 - loss: 0.0968 - acc: 0.96 - ETA: 23:00 - loss: 0.0969 - acc: 0.96 - ETA: 22:56 - loss: 0.0970 - acc: 0.96 - ETA: 22:53 - loss: 0.0971 - acc: 0.96 - ETA: 22:50 - loss: 0.0970 - acc: 0.96 - ETA: 22:47 - loss: 0.0967 - acc: 0.96 - ETA: 22:44 - loss: 0.0971 - acc: 0.96 - ETA: 22:41 - loss: 0.0971 - acc: 0.96 - ETA: 22:38 - loss: 0.0976 - acc: 0.96 - ETA: 22:35 - loss: 0.0975 - acc: 0.96 - ETA: 22:32 - loss: 0.0973 - acc: 0.96 - ETA: 22:28 - loss: 0.0973 - acc: 0.96 - ETA: 22:25 - loss: 0.0974 - acc: 0.96 - ETA: 22:22 - loss: 0.0973 - acc: 0.96 - ETA: 22:19 - loss: 0.0973 - acc: 0.96 - ETA: 22:15 - loss: 0.0972 - acc: 0.96 - ETA: 22:12 - loss: 0.0971 - acc: 0.96 - ETA: 22:09 - loss: 0.0970 - acc: 0.96 - ETA: 22:06 - loss: 0.0974 - acc: 0.96 - ETA: 22:03 - loss: 0.0972 - acc: 0.96 - ETA: 22:00 - loss: 0.0973 - acc: 0.96 - ETA: 21:56 - loss: 0.0972 - acc: 0.96 - ETA: 21:53 - loss: 0.0970 - acc: 0.96 - ETA: 21:50 - loss: 0.0970 - acc: 0.96 - ETA: 21:47 - loss: 0.0968 - acc: 0.96 - ETA: 21:45 - loss: 0.0967 - acc: 0.96 - ETA: 21:42 - loss: 0.0967 - acc: 0.96 - ETA: 21:39 - loss: 0.0966 - acc: 0.96 - ETA: 21:36 - loss: 0.0965 - acc: 0.96 - ETA: 21:33 - loss: 0.0965 - acc: 0.96 - ETA: 21:30 - loss: 0.0967 - acc: 0.96 - ETA: 21:27 - loss: 0.0968 - acc: 0.96 - ETA: 21:24 - loss: 0.0969 - acc: 0.96 - ETA: 21:20 - loss: 0.0968 - acc: 0.96 - ETA: 21:17 - loss: 0.0967 - acc: 0.96 - ETA: 21:14 - loss: 0.0967 - acc: 0.96 - ETA: 21:11 - loss: 0.0968 - acc: 0.96 - ETA: 21:08 - loss: 0.0966 - acc: 0.96 - ETA: 21:05 - loss: 0.0970 - acc: 0.96 - ETA: 21:02 - loss: 0.0971 - acc: 0.96 - ETA: 20:59 - loss: 0.0969 - acc: 0.96 - ETA: 20:56 - loss: 0.0970 - acc: 0.96 - ETA: 20:53 - loss: 0.0970 - acc: 0.96 - ETA: 20:50 - loss: 0.0970 - acc: 0.96 - ETA: 20:47 - loss: 0.0969 - acc: 0.96 - ETA: 20:44 - loss: 0.0968 - acc: 0.96 - ETA: 20:41 - loss: 0.0966 - acc: 0.96 - ETA: 20:38 - loss: 0.0965 - acc: 0.96 - ETA: 20:35 - loss: 0.0966 - acc: 0.96 - ETA: 20:32 - loss: 0.0964 - acc: 0.96 - ETA: 20:29 - loss: 0.0963 - acc: 0.96 - ETA: 20:26 - loss: 0.0962 - acc: 0.96 - ETA: 20:23 - loss: 0.0962 - acc: 0.96 - ETA: 20:20 - loss: 0.0964 - acc: 0.96 - ETA: 20:16 - loss: 0.0961 - acc: 0.96 - ETA: 20:13 - loss: 0.0960 - acc: 0.96 - ETA: 20:10 - loss: 0.0959 - acc: 0.96 - ETA: 20:07 - loss: 0.0959 - acc: 0.96 - ETA: 20:04 - loss: 0.0960 - acc: 0.96 - ETA: 20:01 - loss: 0.0960 - acc: 0.96 - ETA: 19:58 - loss: 0.0959 - acc: 0.96 - ETA: 19:55 - loss: 0.0958 - acc: 0.96 - ETA: 19:52 - loss: 0.0955 - acc: 0.96 - ETA: 19:49 - loss: 0.0961 - acc: 0.96 - ETA: 19:46 - loss: 0.0961 - acc: 0.96 - ETA: 19:43 - loss: 0.0959 - acc: 0.96 - ETA: 19:40 - loss: 0.0959 - acc: 0.96 - ETA: 19:37 - loss: 0.0960 - acc: 0.96 - ETA: 19:34 - loss: 0.0959 - acc: 0.96 - ETA: 19:31 - loss: 0.0958 - acc: 0.96 - ETA: 19:28 - loss: 0.0960 - acc: 0.96 - ETA: 19:25 - loss: 0.0959 - acc: 0.96 - ETA: 19:22 - loss: 0.0960 - acc: 0.96 - ETA: 19:19 - loss: 0.0961 - acc: 0.96 - ETA: 19:16 - loss: 0.0963 - acc: 0.96 - ETA: 19:12 - loss: 0.0962 - acc: 0.96 - ETA: 19:09 - loss: 0.0965 - acc: 0.96 - ETA: 19:06 - loss: 0.0965 - acc: 0.96 - ETA: 19:03 - loss: 0.0965 - acc: 0.96 - ETA: 19:00 - loss: 0.0965 - acc: 0.96 - ETA: 18:57 - loss: 0.0965 - acc: 0.96 - ETA: 18:54 - loss: 0.0966 - acc: 0.96 - ETA: 18:51 - loss: 0.0967 - acc: 0.96 - ETA: 18:48 - loss: 0.0966 - acc: 0.96 - ETA: 18:45 - loss: 0.0967 - acc: 0.96 - ETA: 18:42 - loss: 0.0965 - acc: 0.96 - ETA: 18:39 - loss: 0.0964 - acc: 0.96 - ETA: 18:37 - loss: 0.0963 - acc: 0.96 - ETA: 18:34 - loss: 0.0964 - acc: 0.96 - ETA: 18:31 - loss: 0.0963 - acc: 0.96 - ETA: 18:28 - loss: 0.0963 - acc: 0.96 - ETA: 18:25 - loss: 0.0960 - acc: 0.96 - ETA: 18:22 - loss: 0.0961 - acc: 0.96 - ETA: 18:19 - loss: 0.0960 - acc: 0.96 - ETA: 18:16 - loss: 0.0960 - acc: 0.96 - ETA: 18:13 - loss: 0.0961 - acc: 0.96 - ETA: 18:10 - loss: 0.0961 - acc: 0.96 - ETA: 18:07 - loss: 0.0960 - acc: 0.96 - ETA: 18:04 - loss: 0.0959 - acc: 0.96 - ETA: 18:01 - loss: 0.0958 - acc: 0.96 - ETA: 17:58 - loss: 0.0958 - acc: 0.96 - ETA: 17:55 - loss: 0.0959 - acc: 0.96 - ETA: 17:52 - loss: 0.0960 - acc: 0.96 - ETA: 17:49 - loss: 0.0960 - acc: 0.96 - ETA: 17:46 - loss: 0.0959 - acc: 0.96 - ETA: 17:43 - loss: 0.0959 - acc: 0.96 - ETA: 17:40 - loss: 0.0960 - acc: 0.96 - ETA: 17:37 - loss: 0.0959 - acc: 0.96 - ETA: 17:34 - loss: 0.0959 - acc: 0.96 - ETA: 17:31 - loss: 0.0958 - acc: 0.96 - ETA: 17:28 - loss: 0.0958 - acc: 0.96 - ETA: 17:25 - loss: 0.0958 - acc: 0.96 - ETA: 17:22 - loss: 0.0957 - acc: 0.96 - ETA: 17:19 - loss: 0.0959 - acc: 0.96 - ETA: 17:16 - loss: 0.0957 - acc: 0.96 - ETA: 17:13 - loss: 0.0956 - acc: 0.96 - ETA: 17:10 - loss: 0.0955 - acc: 0.96 - ETA: 17:07 - loss: 0.0957 - acc: 0.96 - ETA: 17:04 - loss: 0.0959 - acc: 0.96 - ETA: 17:01 - loss: 0.0958 - acc: 0.96 - ETA: 16:58 - loss: 0.0958 - acc: 0.96 - ETA: 16:55 - loss: 0.0958 - acc: 0.96 - ETA: 16:52 - loss: 0.0959 - acc: 0.96 - ETA: 16:49 - loss: 0.0959 - acc: 0.96 - ETA: 16:46 - loss: 0.0961 - acc: 0.96 - ETA: 16:43 - loss: 0.0960 - acc: 0.96 - ETA: 16:40 - loss: 0.0959 - acc: 0.96 - ETA: 16:37 - loss: 0.0959 - acc: 0.96 - ETA: 16:34 - loss: 0.0959 - acc: 0.96 - ETA: 16:31 - loss: 0.0960 - acc: 0.96 - ETA: 16:28 - loss: 0.0963 - acc: 0.96 - ETA: 16:25 - loss: 0.0962 - acc: 0.96 - ETA: 16:22 - loss: 0.0962 - acc: 0.96 - ETA: 16:19 - loss: 0.0961 - acc: 0.96 - ETA: 16:16 - loss: 0.0961 - acc: 0.96 - ETA: 16:13 - loss: 0.0960 - acc: 0.96 - ETA: 16:10 - loss: 0.0959 - acc: 0.96 - ETA: 16:07 - loss: 0.0959 - acc: 0.96 - ETA: 16:04 - loss: 0.0959 - acc: 0.96 - ETA: 16:01 - loss: 0.0959 - acc: 0.96 - ETA: 15:58 - loss: 0.0961 - acc: 0.96 - ETA: 15:55 - loss: 0.0961 - acc: 0.96 - ETA: 15:52 - loss: 0.0960 - acc: 0.96 - ETA: 15:49 - loss: 0.0959 - acc: 0.96 - ETA: 15:46 - loss: 0.0958 - acc: 0.96 - ETA: 15:43 - loss: 0.0956 - acc: 0.96 - ETA: 15:40 - loss: 0.0955 - acc: 0.96 - ETA: 15:38 - loss: 0.0956 - acc: 0.96 - ETA: 15:35 - loss: 0.0956 - acc: 0.96 - ETA: 15:32 - loss: 0.0955 - acc: 0.96 - ETA: 15:29 - loss: 0.0956 - acc: 0.96 - ETA: 15:26 - loss: 0.0954 - acc: 0.96 - ETA: 15:23 - loss: 0.0955 - acc: 0.96 - ETA: 15:20 - loss: 0.0953 - acc: 0.96 - ETA: 15:17 - loss: 0.0952 - acc: 0.96 - ETA: 15:14 - loss: 0.0951 - acc: 0.96 - ETA: 15:11 - loss: 0.0951 - acc: 0.96 - ETA: 15:08 - loss: 0.0949 - acc: 0.96 - ETA: 15:05 - loss: 0.0948 - acc: 0.96 - ETA: 15:02 - loss: 0.0947 - acc: 0.96 - ETA: 15:00 - loss: 0.0947 - acc: 0.96 - ETA: 14:57 - loss: 0.0947 - acc: 0.96 - ETA: 14:54 - loss: 0.0947 - acc: 0.96 - ETA: 14:51 - loss: 0.0946 - acc: 0.96 - ETA: 14:48 - loss: 0.0945 - acc: 0.96 - ETA: 14:45 - loss: 0.0947 - acc: 0.96 - ETA: 14:42 - loss: 0.0945 - acc: 0.96 - ETA: 14:39 - loss: 0.0950 - acc: 0.96 - ETA: 14:36 - loss: 0.0950 - acc: 0.96 - ETA: 14:33 - loss: 0.0951 - acc: 0.96 - ETA: 14:30 - loss: 0.0951 - acc: 0.96 - ETA: 14:27 - loss: 0.0953 - acc: 0.96 - ETA: 14:24 - loss: 0.0952 - acc: 0.96 - ETA: 14:21 - loss: 0.0952 - acc: 0.9661"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "612/704 [=========================>....] - ETA: 14:18 - loss: 0.0954 - acc: 0.96 - ETA: 14:15 - loss: 0.0954 - acc: 0.96 - ETA: 14:12 - loss: 0.0955 - acc: 0.96 - ETA: 14:09 - loss: 0.0960 - acc: 0.96 - ETA: 14:07 - loss: 0.0961 - acc: 0.96 - ETA: 14:04 - loss: 0.0961 - acc: 0.96 - ETA: 14:01 - loss: 0.0961 - acc: 0.96 - ETA: 13:58 - loss: 0.0961 - acc: 0.96 - ETA: 13:55 - loss: 0.0962 - acc: 0.96 - ETA: 13:52 - loss: 0.0961 - acc: 0.96 - ETA: 13:49 - loss: 0.0961 - acc: 0.96 - ETA: 13:46 - loss: 0.0961 - acc: 0.96 - ETA: 13:43 - loss: 0.0960 - acc: 0.96 - ETA: 13:40 - loss: 0.0960 - acc: 0.96 - ETA: 13:37 - loss: 0.0960 - acc: 0.96 - ETA: 13:35 - loss: 0.0964 - acc: 0.96 - ETA: 13:32 - loss: 0.0964 - acc: 0.96 - ETA: 13:29 - loss: 0.0964 - acc: 0.96 - ETA: 13:26 - loss: 0.0963 - acc: 0.96 - ETA: 13:23 - loss: 0.0965 - acc: 0.96 - ETA: 13:20 - loss: 0.0965 - acc: 0.96 - ETA: 13:17 - loss: 0.0965 - acc: 0.96 - ETA: 13:14 - loss: 0.0965 - acc: 0.96 - ETA: 13:11 - loss: 0.0964 - acc: 0.96 - ETA: 13:08 - loss: 0.0963 - acc: 0.96 - ETA: 13:05 - loss: 0.0963 - acc: 0.96 - ETA: 13:02 - loss: 0.0962 - acc: 0.96 - ETA: 12:59 - loss: 0.0962 - acc: 0.96 - ETA: 12:56 - loss: 0.0962 - acc: 0.96 - ETA: 12:54 - loss: 0.0961 - acc: 0.96 - ETA: 12:51 - loss: 0.0960 - acc: 0.96 - ETA: 12:48 - loss: 0.0960 - acc: 0.96 - ETA: 12:45 - loss: 0.0959 - acc: 0.96 - ETA: 12:42 - loss: 0.0962 - acc: 0.96 - ETA: 12:39 - loss: 0.0962 - acc: 0.96 - ETA: 12:36 - loss: 0.0961 - acc: 0.96 - ETA: 12:33 - loss: 0.0962 - acc: 0.96 - ETA: 12:30 - loss: 0.0961 - acc: 0.96 - ETA: 12:27 - loss: 0.0961 - acc: 0.96 - ETA: 12:24 - loss: 0.0960 - acc: 0.96 - ETA: 12:21 - loss: 0.0961 - acc: 0.96 - ETA: 12:19 - loss: 0.0960 - acc: 0.96 - ETA: 12:16 - loss: 0.0959 - acc: 0.96 - ETA: 12:13 - loss: 0.0958 - acc: 0.96 - ETA: 12:10 - loss: 0.0961 - acc: 0.96 - ETA: 12:07 - loss: 0.0962 - acc: 0.96 - ETA: 12:04 - loss: 0.0961 - acc: 0.96 - ETA: 12:01 - loss: 0.0961 - acc: 0.96 - ETA: 11:58 - loss: 0.0961 - acc: 0.96 - ETA: 11:55 - loss: 0.0961 - acc: 0.96 - ETA: 11:52 - loss: 0.0960 - acc: 0.96 - ETA: 11:49 - loss: 0.0959 - acc: 0.96 - ETA: 11:46 - loss: 0.0959 - acc: 0.96 - ETA: 11:43 - loss: 0.0959 - acc: 0.96 - ETA: 11:40 - loss: 0.0958 - acc: 0.96 - ETA: 11:38 - loss: 0.0958 - acc: 0.96 - ETA: 11:35 - loss: 0.0957 - acc: 0.96 - ETA: 11:32 - loss: 0.0957 - acc: 0.96 - ETA: 11:29 - loss: 0.0957 - acc: 0.96 - ETA: 11:26 - loss: 0.0957 - acc: 0.96 - ETA: 11:23 - loss: 0.0957 - acc: 0.96 - ETA: 11:20 - loss: 0.0956 - acc: 0.96 - ETA: 11:17 - loss: 0.0957 - acc: 0.96 - ETA: 11:14 - loss: 0.0956 - acc: 0.96 - ETA: 11:11 - loss: 0.0956 - acc: 0.96 - ETA: 11:08 - loss: 0.0957 - acc: 0.96 - ETA: 11:06 - loss: 0.0956 - acc: 0.96 - ETA: 11:03 - loss: 0.0956 - acc: 0.96 - ETA: 11:00 - loss: 0.0955 - acc: 0.96 - ETA: 10:57 - loss: 0.0954 - acc: 0.96 - ETA: 10:54 - loss: 0.0953 - acc: 0.96 - ETA: 10:51 - loss: 0.0952 - acc: 0.96 - ETA: 10:48 - loss: 0.0952 - acc: 0.96 - ETA: 10:45 - loss: 0.0951 - acc: 0.96 - ETA: 10:42 - loss: 0.0950 - acc: 0.96 - ETA: 10:39 - loss: 0.0951 - acc: 0.96 - ETA: 10:36 - loss: 0.0951 - acc: 0.96 - ETA: 10:34 - loss: 0.0950 - acc: 0.96 - ETA: 10:31 - loss: 0.0950 - acc: 0.96 - ETA: 10:28 - loss: 0.0950 - acc: 0.96 - ETA: 10:25 - loss: 0.0949 - acc: 0.96 - ETA: 10:22 - loss: 0.0948 - acc: 0.96 - ETA: 10:19 - loss: 0.0947 - acc: 0.96 - ETA: 10:16 - loss: 0.0946 - acc: 0.96 - ETA: 10:13 - loss: 0.0945 - acc: 0.96 - ETA: 10:10 - loss: 0.0945 - acc: 0.96 - ETA: 10:07 - loss: 0.0944 - acc: 0.96 - ETA: 10:04 - loss: 0.0945 - acc: 0.96 - ETA: 10:01 - loss: 0.0944 - acc: 0.96 - ETA: 9:59 - loss: 0.0946 - acc: 0.9665 - ETA: 9:56 - loss: 0.0947 - acc: 0.966 - ETA: 9:53 - loss: 0.0947 - acc: 0.966 - ETA: 9:50 - loss: 0.0947 - acc: 0.966 - ETA: 9:47 - loss: 0.0946 - acc: 0.966 - ETA: 9:44 - loss: 0.0945 - acc: 0.966 - ETA: 9:41 - loss: 0.0947 - acc: 0.966 - ETA: 9:38 - loss: 0.0946 - acc: 0.966 - ETA: 9:35 - loss: 0.0947 - acc: 0.966 - ETA: 9:32 - loss: 0.0946 - acc: 0.966 - ETA: 9:29 - loss: 0.0947 - acc: 0.966 - ETA: 9:26 - loss: 0.0948 - acc: 0.966 - ETA: 9:24 - loss: 0.0948 - acc: 0.966 - ETA: 9:21 - loss: 0.0947 - acc: 0.966 - ETA: 9:18 - loss: 0.0947 - acc: 0.966 - ETA: 9:15 - loss: 0.0947 - acc: 0.966 - ETA: 9:12 - loss: 0.0946 - acc: 0.966 - ETA: 9:09 - loss: 0.0946 - acc: 0.966 - ETA: 9:06 - loss: 0.0946 - acc: 0.966 - ETA: 9:03 - loss: 0.0946 - acc: 0.966 - ETA: 9:00 - loss: 0.0946 - acc: 0.966 - ETA: 8:57 - loss: 0.0947 - acc: 0.966 - ETA: 8:54 - loss: 0.0946 - acc: 0.966 - ETA: 8:52 - loss: 0.0948 - acc: 0.966 - ETA: 8:49 - loss: 0.0948 - acc: 0.966 - ETA: 8:46 - loss: 0.0948 - acc: 0.966 - ETA: 8:43 - loss: 0.0948 - acc: 0.966 - ETA: 8:40 - loss: 0.0950 - acc: 0.966 - ETA: 8:37 - loss: 0.0951 - acc: 0.966 - ETA: 8:34 - loss: 0.0951 - acc: 0.966 - ETA: 8:31 - loss: 0.0952 - acc: 0.966 - ETA: 8:28 - loss: 0.0951 - acc: 0.966 - ETA: 8:25 - loss: 0.0951 - acc: 0.966 - ETA: 8:22 - loss: 0.0951 - acc: 0.966 - ETA: 8:19 - loss: 0.0950 - acc: 0.966 - ETA: 8:17 - loss: 0.0950 - acc: 0.966 - ETA: 8:14 - loss: 0.0950 - acc: 0.966 - ETA: 8:11 - loss: 0.0950 - acc: 0.966 - ETA: 8:08 - loss: 0.0950 - acc: 0.966 - ETA: 8:05 - loss: 0.0949 - acc: 0.966 - ETA: 8:02 - loss: 0.0950 - acc: 0.966 - ETA: 7:59 - loss: 0.0949 - acc: 0.966 - ETA: 7:56 - loss: 0.0949 - acc: 0.966 - ETA: 7:53 - loss: 0.0950 - acc: 0.966 - ETA: 7:50 - loss: 0.0949 - acc: 0.966 - ETA: 7:48 - loss: 0.0949 - acc: 0.966 - ETA: 7:45 - loss: 0.0950 - acc: 0.966 - ETA: 7:42 - loss: 0.0950 - acc: 0.966 - ETA: 7:39 - loss: 0.0949 - acc: 0.966 - ETA: 7:36 - loss: 0.0949 - acc: 0.966 - ETA: 7:33 - loss: 0.0949 - acc: 0.966 - ETA: 7:30 - loss: 0.0948 - acc: 0.966 - ETA: 7:27 - loss: 0.0948 - acc: 0.966 - ETA: 7:24 - loss: 0.0948 - acc: 0.966 - ETA: 7:21 - loss: 0.0947 - acc: 0.966 - ETA: 7:18 - loss: 0.0946 - acc: 0.966 - ETA: 7:16 - loss: 0.0952 - acc: 0.966 - ETA: 7:13 - loss: 0.0952 - acc: 0.966 - ETA: 7:10 - loss: 0.0952 - acc: 0.966 - ETA: 7:07 - loss: 0.0952 - acc: 0.966 - ETA: 7:04 - loss: 0.0952 - acc: 0.966 - ETA: 7:01 - loss: 0.0953 - acc: 0.966 - ETA: 6:58 - loss: 0.0954 - acc: 0.966 - ETA: 6:55 - loss: 0.0954 - acc: 0.966 - ETA: 6:52 - loss: 0.0954 - acc: 0.966 - ETA: 6:49 - loss: 0.0954 - acc: 0.966 - ETA: 6:46 - loss: 0.0954 - acc: 0.966 - ETA: 6:43 - loss: 0.0954 - acc: 0.966 - ETA: 6:41 - loss: 0.0954 - acc: 0.966 - ETA: 6:38 - loss: 0.0953 - acc: 0.966 - ETA: 6:35 - loss: 0.0952 - acc: 0.966 - ETA: 6:32 - loss: 0.0951 - acc: 0.966 - ETA: 6:29 - loss: 0.0952 - acc: 0.966 - ETA: 6:26 - loss: 0.0952 - acc: 0.966 - ETA: 6:23 - loss: 0.0952 - acc: 0.966 - ETA: 6:20 - loss: 0.0951 - acc: 0.966 - ETA: 6:17 - loss: 0.0952 - acc: 0.966 - ETA: 6:14 - loss: 0.0952 - acc: 0.966 - ETA: 6:12 - loss: 0.0951 - acc: 0.966 - ETA: 6:09 - loss: 0.0951 - acc: 0.966 - ETA: 6:06 - loss: 0.0951 - acc: 0.966 - ETA: 6:03 - loss: 0.0951 - acc: 0.966 - ETA: 6:00 - loss: 0.0951 - acc: 0.966 - ETA: 5:57 - loss: 0.0950 - acc: 0.966 - ETA: 5:54 - loss: 0.0949 - acc: 0.966 - ETA: 5:51 - loss: 0.0948 - acc: 0.966 - ETA: 5:48 - loss: 0.0948 - acc: 0.966 - ETA: 5:45 - loss: 0.0948 - acc: 0.966 - ETA: 5:42 - loss: 0.0949 - acc: 0.966 - ETA: 5:39 - loss: 0.0950 - acc: 0.966 - ETA: 5:37 - loss: 0.0950 - acc: 0.966 - ETA: 5:34 - loss: 0.0951 - acc: 0.966 - ETA: 5:31 - loss: 0.0951 - acc: 0.966 - ETA: 5:28 - loss: 0.0952 - acc: 0.966 - ETA: 5:25 - loss: 0.0952 - acc: 0.966 - ETA: 5:22 - loss: 0.0953 - acc: 0.966 - ETA: 5:19 - loss: 0.0953 - acc: 0.966 - ETA: 5:16 - loss: 0.0954 - acc: 0.966 - ETA: 5:13 - loss: 0.0953 - acc: 0.966 - ETA: 5:10 - loss: 0.0953 - acc: 0.966 - ETA: 5:07 - loss: 0.0953 - acc: 0.966 - ETA: 5:05 - loss: 0.0953 - acc: 0.966 - ETA: 5:02 - loss: 0.0952 - acc: 0.966 - ETA: 4:59 - loss: 0.0953 - acc: 0.966 - ETA: 4:56 - loss: 0.0954 - acc: 0.966 - ETA: 4:53 - loss: 0.0955 - acc: 0.966 - ETA: 4:50 - loss: 0.0954 - acc: 0.966 - ETA: 4:47 - loss: 0.0954 - acc: 0.966 - ETA: 4:44 - loss: 0.0954 - acc: 0.966 - ETA: 4:41 - loss: 0.0954 - acc: 0.966 - ETA: 4:38 - loss: 0.0954 - acc: 0.966 - ETA: 4:35 - loss: 0.0954 - acc: 0.966 - ETA: 4:33 - loss: 0.0954 - acc: 0.966 - ETA: 4:30 - loss: 0.0954 - acc: 0.966 - ETA: 4:27 - loss: 0.0954 - acc: 0.9661"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704/704 [==============================] - ETA: 4:24 - loss: 0.0954 - acc: 0.966 - ETA: 4:21 - loss: 0.0954 - acc: 0.966 - ETA: 4:18 - loss: 0.0955 - acc: 0.966 - ETA: 4:15 - loss: 0.0955 - acc: 0.966 - ETA: 4:12 - loss: 0.0954 - acc: 0.966 - ETA: 4:09 - loss: 0.0955 - acc: 0.966 - ETA: 4:06 - loss: 0.0955 - acc: 0.966 - ETA: 4:03 - loss: 0.0954 - acc: 0.966 - ETA: 4:00 - loss: 0.0954 - acc: 0.966 - ETA: 3:58 - loss: 0.0953 - acc: 0.966 - ETA: 3:55 - loss: 0.0953 - acc: 0.966 - ETA: 3:52 - loss: 0.0953 - acc: 0.966 - ETA: 3:49 - loss: 0.0953 - acc: 0.966 - ETA: 3:46 - loss: 0.0954 - acc: 0.966 - ETA: 3:43 - loss: 0.0954 - acc: 0.966 - ETA: 3:40 - loss: 0.0954 - acc: 0.966 - ETA: 3:37 - loss: 0.0956 - acc: 0.965 - ETA: 3:34 - loss: 0.0956 - acc: 0.965 - ETA: 3:31 - loss: 0.0957 - acc: 0.965 - ETA: 3:28 - loss: 0.0957 - acc: 0.965 - ETA: 3:26 - loss: 0.0957 - acc: 0.965 - ETA: 3:23 - loss: 0.0957 - acc: 0.965 - ETA: 3:20 - loss: 0.0957 - acc: 0.965 - ETA: 3:17 - loss: 0.0956 - acc: 0.965 - ETA: 3:14 - loss: 0.0955 - acc: 0.965 - ETA: 3:11 - loss: 0.0956 - acc: 0.965 - ETA: 3:08 - loss: 0.0956 - acc: 0.965 - ETA: 3:05 - loss: 0.0957 - acc: 0.965 - ETA: 3:02 - loss: 0.0957 - acc: 0.965 - ETA: 2:59 - loss: 0.0956 - acc: 0.965 - ETA: 2:56 - loss: 0.0956 - acc: 0.965 - ETA: 2:54 - loss: 0.0957 - acc: 0.965 - ETA: 2:51 - loss: 0.0956 - acc: 0.965 - ETA: 2:48 - loss: 0.0955 - acc: 0.965 - ETA: 2:45 - loss: 0.0955 - acc: 0.965 - ETA: 2:42 - loss: 0.0956 - acc: 0.965 - ETA: 2:39 - loss: 0.0957 - acc: 0.965 - ETA: 2:36 - loss: 0.0957 - acc: 0.965 - ETA: 2:33 - loss: 0.0957 - acc: 0.965 - ETA: 2:30 - loss: 0.0956 - acc: 0.965 - ETA: 2:27 - loss: 0.0956 - acc: 0.965 - ETA: 2:24 - loss: 0.0957 - acc: 0.965 - ETA: 2:22 - loss: 0.0958 - acc: 0.965 - ETA: 2:19 - loss: 0.0958 - acc: 0.965 - ETA: 2:16 - loss: 0.0958 - acc: 0.965 - ETA: 2:13 - loss: 0.0958 - acc: 0.965 - ETA: 2:10 - loss: 0.0958 - acc: 0.965 - ETA: 2:07 - loss: 0.0959 - acc: 0.965 - ETA: 2:04 - loss: 0.0959 - acc: 0.965 - ETA: 2:01 - loss: 0.0959 - acc: 0.965 - ETA: 1:58 - loss: 0.0959 - acc: 0.965 - ETA: 1:55 - loss: 0.0959 - acc: 0.965 - ETA: 1:53 - loss: 0.0959 - acc: 0.965 - ETA: 1:50 - loss: 0.0958 - acc: 0.965 - ETA: 1:47 - loss: 0.0958 - acc: 0.965 - ETA: 1:44 - loss: 0.0957 - acc: 0.965 - ETA: 1:41 - loss: 0.0956 - acc: 0.965 - ETA: 1:38 - loss: 0.0956 - acc: 0.965 - ETA: 1:35 - loss: 0.0956 - acc: 0.965 - ETA: 1:32 - loss: 0.0957 - acc: 0.965 - ETA: 1:29 - loss: 0.0959 - acc: 0.965 - ETA: 1:26 - loss: 0.0960 - acc: 0.965 - ETA: 1:24 - loss: 0.0960 - acc: 0.965 - ETA: 1:21 - loss: 0.0960 - acc: 0.965 - ETA: 1:18 - loss: 0.0961 - acc: 0.965 - ETA: 1:15 - loss: 0.0960 - acc: 0.965 - ETA: 1:12 - loss: 0.0961 - acc: 0.965 - ETA: 1:09 - loss: 0.0961 - acc: 0.965 - ETA: 1:06 - loss: 0.0961 - acc: 0.965 - ETA: 1:03 - loss: 0.0961 - acc: 0.965 - ETA: 1:00 - loss: 0.0961 - acc: 0.965 - ETA: 57s - loss: 0.0961 - acc: 0.965 - ETA: 55s - loss: 0.0961 - acc: 0.96 - ETA: 52s - loss: 0.0961 - acc: 0.96 - ETA: 49s - loss: 0.0960 - acc: 0.96 - ETA: 46s - loss: 0.0960 - acc: 0.96 - ETA: 43s - loss: 0.0959 - acc: 0.96 - ETA: 40s - loss: 0.0961 - acc: 0.96 - ETA: 37s - loss: 0.0961 - acc: 0.96 - ETA: 34s - loss: 0.0961 - acc: 0.96 - ETA: 31s - loss: 0.0960 - acc: 0.96 - ETA: 28s - loss: 0.0959 - acc: 0.96 - ETA: 26s - loss: 0.0958 - acc: 0.96 - ETA: 23s - loss: 0.0959 - acc: 0.96 - ETA: 20s - loss: 0.0958 - acc: 0.96 - ETA: 17s - loss: 0.0959 - acc: 0.96 - ETA: 14s - loss: 0.0959 - acc: 0.96 - ETA: 11s - loss: 0.0959 - acc: 0.96 - ETA: 8s - loss: 0.0959 - acc: 0.9658 - ETA: 5s - loss: 0.0958 - acc: 0.965 - ETA: 2s - loss: 0.0957 - acc: 0.965 - 2248s 3s/step - loss: 0.0958 - acc: 0.9659 - val_loss: 0.1142 - val_acc: 0.9557\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/704 [=======>......................] - ETA: 5:08 - loss: 0.1100 - acc: 0.969 - ETA: 5:26 - loss: 0.0808 - acc: 0.974 - ETA: 5:37 - loss: 0.0912 - acc: 0.969 - ETA: 5:53 - loss: 0.1013 - acc: 0.962 - ETA: 5:55 - loss: 0.1126 - acc: 0.957 - ETA: 5:58 - loss: 0.1122 - acc: 0.956 - ETA: 6:02 - loss: 0.1093 - acc: 0.957 - ETA: 5:53 - loss: 0.1062 - acc: 0.959 - ETA: 5:47 - loss: 0.1075 - acc: 0.960 - ETA: 5:41 - loss: 0.1168 - acc: 0.957 - ETA: 5:45 - loss: 0.1115 - acc: 0.959 - ETA: 5:42 - loss: 0.1113 - acc: 0.958 - ETA: 8:40 - loss: 0.1070 - acc: 0.960 - ETA: 11:36 - loss: 0.1010 - acc: 0.96 - ETA: 14:12 - loss: 0.1034 - acc: 0.96 - ETA: 16:31 - loss: 0.1174 - acc: 0.95 - ETA: 18:32 - loss: 0.1168 - acc: 0.96 - ETA: 20:22 - loss: 0.1125 - acc: 0.96 - ETA: 22:00 - loss: 0.1137 - acc: 0.96 - ETA: 23:24 - loss: 0.1116 - acc: 0.96 - ETA: 24:41 - loss: 0.1142 - acc: 0.96 - ETA: 25:45 - loss: 0.1130 - acc: 0.96 - ETA: 26:01 - loss: 0.1152 - acc: 0.95 - ETA: 26:14 - loss: 0.1126 - acc: 0.96 - ETA: 26:26 - loss: 0.1105 - acc: 0.96 - ETA: 26:37 - loss: 0.1116 - acc: 0.96 - ETA: 26:45 - loss: 0.1112 - acc: 0.96 - ETA: 26:54 - loss: 0.1087 - acc: 0.96 - ETA: 27:04 - loss: 0.1077 - acc: 0.96 - ETA: 27:12 - loss: 0.1058 - acc: 0.96 - ETA: 27:19 - loss: 0.1032 - acc: 0.96 - ETA: 27:26 - loss: 0.1022 - acc: 0.96 - ETA: 27:37 - loss: 0.1032 - acc: 0.96 - ETA: 27:43 - loss: 0.1024 - acc: 0.96 - ETA: 27:48 - loss: 0.1039 - acc: 0.96 - ETA: 27:54 - loss: 0.1029 - acc: 0.96 - ETA: 27:58 - loss: 0.1020 - acc: 0.96 - ETA: 28:01 - loss: 0.1016 - acc: 0.96 - ETA: 28:03 - loss: 0.1013 - acc: 0.96 - ETA: 28:08 - loss: 0.1019 - acc: 0.96 - ETA: 28:09 - loss: 0.1004 - acc: 0.96 - ETA: 28:13 - loss: 0.1000 - acc: 0.96 - ETA: 28:15 - loss: 0.0985 - acc: 0.96 - ETA: 28:17 - loss: 0.0997 - acc: 0.96 - ETA: 28:18 - loss: 0.0983 - acc: 0.96 - ETA: 28:20 - loss: 0.0981 - acc: 0.96 - ETA: 28:23 - loss: 0.0990 - acc: 0.96 - ETA: 28:24 - loss: 0.1002 - acc: 0.96 - ETA: 28:25 - loss: 0.1025 - acc: 0.96 - ETA: 28:26 - loss: 0.1020 - acc: 0.96 - ETA: 28:27 - loss: 0.1025 - acc: 0.96 - ETA: 28:27 - loss: 0.1019 - acc: 0.96 - ETA: 28:27 - loss: 0.1011 - acc: 0.96 - ETA: 28:28 - loss: 0.1001 - acc: 0.96 - ETA: 28:28 - loss: 0.0998 - acc: 0.96 - ETA: 28:27 - loss: 0.0996 - acc: 0.96 - ETA: 28:28 - loss: 0.0994 - acc: 0.96 - ETA: 28:28 - loss: 0.0984 - acc: 0.96 - ETA: 28:28 - loss: 0.0987 - acc: 0.96 - ETA: 28:27 - loss: 0.1026 - acc: 0.96 - ETA: 28:27 - loss: 0.1028 - acc: 0.96 - ETA: 28:26 - loss: 0.1022 - acc: 0.96 - ETA: 28:25 - loss: 0.1028 - acc: 0.96 - ETA: 28:26 - loss: 0.1034 - acc: 0.96 - ETA: 28:26 - loss: 0.1037 - acc: 0.96 - ETA: 28:24 - loss: 0.1042 - acc: 0.96 - ETA: 28:23 - loss: 0.1036 - acc: 0.96 - ETA: 28:23 - loss: 0.1030 - acc: 0.96 - ETA: 28:23 - loss: 0.1036 - acc: 0.96 - ETA: 28:21 - loss: 0.1035 - acc: 0.96 - ETA: 28:21 - loss: 0.1042 - acc: 0.96 - ETA: 28:20 - loss: 0.1053 - acc: 0.96 - ETA: 28:19 - loss: 0.1055 - acc: 0.96 - ETA: 28:19 - loss: 0.1062 - acc: 0.96 - ETA: 28:17 - loss: 0.1067 - acc: 0.96 - ETA: 28:16 - loss: 0.1070 - acc: 0.96 - ETA: 28:14 - loss: 0.1069 - acc: 0.96 - ETA: 28:13 - loss: 0.1064 - acc: 0.96 - ETA: 28:12 - loss: 0.1064 - acc: 0.96 - ETA: 28:11 - loss: 0.1060 - acc: 0.96 - ETA: 28:10 - loss: 0.1072 - acc: 0.96 - ETA: 28:07 - loss: 0.1078 - acc: 0.96 - ETA: 28:05 - loss: 0.1078 - acc: 0.96 - ETA: 28:04 - loss: 0.1075 - acc: 0.96 - ETA: 28:02 - loss: 0.1075 - acc: 0.96 - ETA: 28:00 - loss: 0.1068 - acc: 0.96 - ETA: 27:58 - loss: 0.1065 - acc: 0.96 - ETA: 27:56 - loss: 0.1068 - acc: 0.96 - ETA: 27:52 - loss: 0.1075 - acc: 0.96 - ETA: 27:51 - loss: 0.1071 - acc: 0.96 - ETA: 27:49 - loss: 0.1067 - acc: 0.96 - ETA: 27:48 - loss: 0.1064 - acc: 0.96 - ETA: 27:46 - loss: 0.1058 - acc: 0.96 - ETA: 27:45 - loss: 0.1061 - acc: 0.96 - ETA: 27:45 - loss: 0.1058 - acc: 0.96 - ETA: 27:43 - loss: 0.1054 - acc: 0.96 - ETA: 27:40 - loss: 0.1051 - acc: 0.96 - ETA: 27:38 - loss: 0.1051 - acc: 0.96 - ETA: 27:36 - loss: 0.1053 - acc: 0.96 - ETA: 27:34 - loss: 0.1047 - acc: 0.96 - ETA: 27:32 - loss: 0.1046 - acc: 0.96 - ETA: 27:29 - loss: 0.1046 - acc: 0.96 - ETA: 27:28 - loss: 0.1043 - acc: 0.96 - ETA: 27:26 - loss: 0.1040 - acc: 0.96 - ETA: 27:24 - loss: 0.1036 - acc: 0.96 - ETA: 27:22 - loss: 0.1030 - acc: 0.96 - ETA: 27:20 - loss: 0.1023 - acc: 0.96 - ETA: 27:18 - loss: 0.1019 - acc: 0.96 - ETA: 27:16 - loss: 0.1015 - acc: 0.96 - ETA: 27:14 - loss: 0.1018 - acc: 0.96 - ETA: 27:13 - loss: 0.1019 - acc: 0.96 - ETA: 27:10 - loss: 0.1019 - acc: 0.96 - ETA: 27:09 - loss: 0.1021 - acc: 0.96 - ETA: 27:06 - loss: 0.1019 - acc: 0.96 - ETA: 27:05 - loss: 0.1014 - acc: 0.96 - ETA: 27:02 - loss: 0.1012 - acc: 0.96 - ETA: 27:00 - loss: 0.1006 - acc: 0.96 - ETA: 26:58 - loss: 0.1003 - acc: 0.96 - ETA: 26:56 - loss: 0.0998 - acc: 0.96 - ETA: 26:54 - loss: 0.0995 - acc: 0.96 - ETA: 26:51 - loss: 0.0991 - acc: 0.96 - ETA: 26:49 - loss: 0.0988 - acc: 0.96 - ETA: 26:47 - loss: 0.0986 - acc: 0.96 - ETA: 26:44 - loss: 0.0984 - acc: 0.96 - ETA: 26:42 - loss: 0.0982 - acc: 0.96 - ETA: 26:40 - loss: 0.0985 - acc: 0.96 - ETA: 26:38 - loss: 0.0983 - acc: 0.96 - ETA: 26:36 - loss: 0.0982 - acc: 0.96 - ETA: 26:33 - loss: 0.0986 - acc: 0.96 - ETA: 26:31 - loss: 0.0986 - acc: 0.96 - ETA: 26:29 - loss: 0.0992 - acc: 0.96 - ETA: 26:27 - loss: 0.0989 - acc: 0.96 - ETA: 26:25 - loss: 0.0984 - acc: 0.96 - ETA: 26:22 - loss: 0.0983 - acc: 0.96 - ETA: 26:20 - loss: 0.0988 - acc: 0.96 - ETA: 26:18 - loss: 0.0988 - acc: 0.96 - ETA: 26:16 - loss: 0.0987 - acc: 0.96 - ETA: 26:13 - loss: 0.0990 - acc: 0.96 - ETA: 26:11 - loss: 0.0986 - acc: 0.96 - ETA: 26:09 - loss: 0.0983 - acc: 0.96 - ETA: 26:07 - loss: 0.0986 - acc: 0.96 - ETA: 26:04 - loss: 0.0986 - acc: 0.96 - ETA: 26:02 - loss: 0.0985 - acc: 0.96 - ETA: 25:59 - loss: 0.0983 - acc: 0.96 - ETA: 25:56 - loss: 0.0978 - acc: 0.96 - ETA: 25:54 - loss: 0.0980 - acc: 0.96 - ETA: 25:52 - loss: 0.0976 - acc: 0.96 - ETA: 25:49 - loss: 0.0975 - acc: 0.96 - ETA: 25:47 - loss: 0.0972 - acc: 0.96 - ETA: 25:44 - loss: 0.0969 - acc: 0.96 - ETA: 25:42 - loss: 0.0969 - acc: 0.96 - ETA: 25:39 - loss: 0.0978 - acc: 0.96 - ETA: 25:37 - loss: 0.0977 - acc: 0.96 - ETA: 25:34 - loss: 0.0974 - acc: 0.96 - ETA: 25:32 - loss: 0.0974 - acc: 0.96 - ETA: 25:29 - loss: 0.0972 - acc: 0.96 - ETA: 25:28 - loss: 0.0971 - acc: 0.96 - ETA: 25:25 - loss: 0.0968 - acc: 0.96 - ETA: 25:22 - loss: 0.0967 - acc: 0.96 - ETA: 25:19 - loss: 0.0968 - acc: 0.96 - ETA: 25:16 - loss: 0.0966 - acc: 0.96 - ETA: 25:14 - loss: 0.0963 - acc: 0.96 - ETA: 25:11 - loss: 0.0966 - acc: 0.96 - ETA: 25:09 - loss: 0.0962 - acc: 0.96 - ETA: 25:06 - loss: 0.0967 - acc: 0.96 - ETA: 25:03 - loss: 0.0965 - acc: 0.96 - ETA: 25:01 - loss: 0.0965 - acc: 0.96 - ETA: 24:58 - loss: 0.0963 - acc: 0.96 - ETA: 24:56 - loss: 0.0963 - acc: 0.96 - ETA: 24:53 - loss: 0.0959 - acc: 0.96 - ETA: 24:50 - loss: 0.0955 - acc: 0.96 - ETA: 24:47 - loss: 0.0952 - acc: 0.96 - ETA: 24:45 - loss: 0.0955 - acc: 0.96 - ETA: 24:42 - loss: 0.0958 - acc: 0.96 - ETA: 24:39 - loss: 0.0956 - acc: 0.96 - ETA: 24:37 - loss: 0.0960 - acc: 0.96 - ETA: 24:34 - loss: 0.0958 - acc: 0.96 - ETA: 24:31 - loss: 0.0957 - acc: 0.96 - ETA: 24:28 - loss: 0.0962 - acc: 0.96 - ETA: 24:26 - loss: 0.0972 - acc: 0.96 - ETA: 24:23 - loss: 0.0970 - acc: 0.96 - ETA: 24:20 - loss: 0.0968 - acc: 0.96 - ETA: 24:17 - loss: 0.0966 - acc: 0.96 - ETA: 24:15 - loss: 0.0968 - acc: 0.96 - ETA: 24:12 - loss: 0.0968 - acc: 0.96 - ETA: 24:09 - loss: 0.0970 - acc: 0.96 - ETA: 24:07 - loss: 0.0971 - acc: 0.96 - ETA: 24:04 - loss: 0.0969 - acc: 0.96 - ETA: 24:01 - loss: 0.0968 - acc: 0.96 - ETA: 23:59 - loss: 0.0967 - acc: 0.96 - ETA: 23:56 - loss: 0.0966 - acc: 0.96 - ETA: 23:53 - loss: 0.0968 - acc: 0.96 - ETA: 23:50 - loss: 0.0967 - acc: 0.96 - ETA: 23:48 - loss: 0.0966 - acc: 0.96 - ETA: 23:45 - loss: 0.0963 - acc: 0.96 - ETA: 23:42 - loss: 0.0959 - acc: 0.96 - ETA: 23:40 - loss: 0.0958 - acc: 0.96 - ETA: 23:37 - loss: 0.0956 - acc: 0.96 - ETA: 23:34 - loss: 0.0955 - acc: 0.96 - ETA: 23:32 - loss: 0.0958 - acc: 0.9667"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-059f7ea0476e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_samples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         epochs=N_EPOCHS)\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfile_handles\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfh\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile_handles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "columns = ['is_tissue','slide_path','is_tumor','is_all_tumor','tile_loc']\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 2\n",
    "\n",
    "for i in range(len(slide_4_test)):\n",
    "    \n",
    "    # [1] dataset , 2 pos, 2 neg, mean ratio = 3:1\n",
    "    four_samples = pd.DataFrame(columns = columns)\n",
    "    four_image_path = list()\n",
    "    four_mask_path = list()    \n",
    "    for j in range(4):\n",
    "        image_path = image_paths[slide_4_test[i][j]][1:] # 이 부분은 data 읽을때 고치자 ( [1:] 빼야함)\n",
    "        mask_path = tumor_mask_paths[slide_4_test[i][j]][1:] # 이 부분은 data 읽을때 고치자\n",
    "        samples = find_patches_from_slide(image_path, mask_path)\n",
    "        \n",
    "        four_samples = four_samples.append(samples)   \n",
    "        four_image_path.append(image_path)\n",
    "        four_mask_path.append(mask_path)\n",
    "    NUM_SAMPLES = len(four_samples)\n",
    "    if NUM_SAMPLES > 50000:\n",
    "        NUM_SAMPLES = 50000\n",
    "    \n",
    "    samples = four_samples.sample(NUM_SAMPLES, random_state=42)\n",
    "    samples.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    tumor_samples = four_samples[four_samples.is_tumor == True]\n",
    "    print(len(tumor_samples))\n",
    "    non_tumor_samples = four_samples[four_samples.is_tumor == False]\n",
    "    print(len(non_tumor_samples))\n",
    "    non_tumor_samples_3_ratio = non_tumor_samples.sample(len(tumor_samples) * 3, random_state = 42,replace=True)\n",
    "    \n",
    "    all_sample = tumor_samples.append(non_tumor_samples_3_ratio)\n",
    "\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "    for train_index, test_index in split.split(samples, samples[\"is_tumor\"]):\n",
    "            train_samples = samples.loc[train_index]\n",
    "            validation_samples = samples.loc[test_index]\n",
    "    \n",
    "    train_generator = gen_imgs(four_image_path,four_mask_path,train_samples, BATCH_SIZE)\n",
    "    validation_generator = gen_imgs(four_image_path,four_mask_path,validation_samples, BATCH_SIZE)\n",
    "    \n",
    "    train_start_time = datetime.now()\n",
    "    history = model.fit_generator(train_generator, np.ceil(len(train_samples) / BATCH_SIZE),\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=np.ceil(len(validation_samples) / BATCH_SIZE),\n",
    "        epochs=N_EPOCHS)\n",
    "    if file_handles != []:\n",
    "        for fh in file_handles:\n",
    "            fh.close()\n",
    "    file_handles=[]\n",
    "    #del train_generator\n",
    "    #del validation_generator\n",
    "    train_end_time = datetime.now()\n",
    "    print(\"Model training time: %.1f minutes\" % ((train_end_time - train_start_time).seconds / 60,))\n",
    "    model.save('unet_1.h5')\n",
    "    # split\n",
    "    # data gen : all_image_path, all_mask_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('unet_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches in slide: 105213\n",
      "Wall time: 4min 25s\n",
      "5000 gen time: 4.4 minutes\n",
      "Model test time: 2.0 minutes\n",
      "0.9951332079414998\n"
     ]
    }
   ],
   "source": [
    "ipath = all_image_path[0]\n",
    "tpath = all_mask_path[0]\n",
    "\n",
    "all_tissue_samples = find_patches_from_slide(ipath,tpath)\n",
    "print('Total patches in slide: %d' % len(all_tissue_samples)) \n",
    "all_tissue_samples.iloc[:10]\n",
    "all_tissue_samples.is_tumor.value_counts() \n",
    "test_start_time = datetime.now()\n",
    "sample_gen = gen_imgs(all_image_path,all_mask_path,all_tissue_samples, 5000, shuffle=True)\n",
    "%time example_X, example_y  = next(sample_gen)\n",
    "test_end_time = datetime.now()\n",
    "print(\"5000 gen time: %.1f minutes\" % ((test_end_time - test_start_time).seconds / 60,))\n",
    "\n",
    "start_x = PATCH_SIZE//4\n",
    "start_y = PATCH_SIZE//4\n",
    "pred_size = PATCH_SIZE//2\n",
    "\n",
    "test_start_time = datetime.now()\n",
    "preds = []\n",
    "labels = []\n",
    "for i in range(5000):\n",
    "    prediction = predict_from_model(example_X[i],model)\n",
    "    pred_X = np.zeros((pred_size,pred_size))\n",
    "    y = example_y[i].argmax\n",
    "    for x in range(start_x,start_x+pred_size):\n",
    "        for y in range(start_y, start_y+pred_size):\n",
    "            pred_X[x-start_x][y-start_y] = prediction[x][y]\n",
    "            \n",
    "    pred_s = pd.Series(pred_X.flatten())\n",
    "    max_p = np.max(pred_s)\n",
    "    \n",
    "    y = np.max(example_y[i].argmax(axis=2))\n",
    "    preds.append(max_p)\n",
    "    labels.append(y)\n",
    "test_end_time = datetime.now()\n",
    "print(\"Model test time: %.1f minutes\" % ((test_end_time - test_start_time).seconds / 60,))    \n",
    "fpr, tpr, thresholds = metrics.roc_curve(labels,preds,pos_label=1)\n",
    "print(metrics.auc(fpr,tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches in slide: 105213\n",
      "Wall time: 4min 33s\n",
      "5000 gen time: 4.6 minutes\n",
      "Model test time: 0.8 minutes\n",
      "0.9957573195312768\n"
     ]
    }
   ],
   "source": [
    "ipath = all_image_path[0]\n",
    "tpath = all_mask_path[0]\n",
    "\n",
    "all_tissue_samples = find_patches_from_slide(ipath,tpath)\n",
    "print('Total patches in slide: %d' % len(all_tissue_samples)) \n",
    "all_tissue_samples.iloc[:10]\n",
    "all_tissue_samples.is_tumor.value_counts() \n",
    "test_start_time = datetime.now()\n",
    "sample_gen = gen_imgs(all_image_path,all_mask_path,all_tissue_samples, 5000, shuffle=True)\n",
    "%time example_X, example_y  = next(sample_gen)\n",
    "test_end_time = datetime.now()\n",
    "print(\"5000 gen time: %.1f minutes\" % ((test_end_time - test_start_time).seconds / 60,))\n",
    "\n",
    "start_x = PATCH_SIZE//4\n",
    "start_y = PATCH_SIZE//4\n",
    "pred_size = PATCH_SIZE//2\n",
    "\n",
    "test_start_time = datetime.now()\n",
    "preds = []\n",
    "labels = []\n",
    "for i in range(5000):\n",
    "    prediction = predict_from_model(example_X[i],model)\n",
    "    pred_X = np.zeros((pred_size,pred_size))\n",
    "    y = example_y[i].argmax\n",
    "    pred_s = pd.Series(prediction.flatten())\n",
    "    max_p = np.max(pred_s)\n",
    "    \n",
    "    y = np.max(example_y[i].argmax(axis=2))\n",
    "    preds.append(max_p)\n",
    "    labels.append(y)\n",
    "test_end_time = datetime.now()\n",
    "print(\"Model test time: %.1f minutes\" % ((test_end_time - test_start_time).seconds / 60,))    \n",
    "fpr, tpr, thresholds = metrics.roc_curve(labels,preds,pos_label=1)\n",
    "print(metrics.auc(fpr,tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches in slide: 105213\n",
      "Wall time: 4min 37s\n",
      "5000 gen time: 4.7 minutes\n",
      "Model test time: 3.7 minutes\n",
      "0.9960936531019771\n"
     ]
    }
   ],
   "source": [
    "ipath = all_image_path[0]\n",
    "tpath = all_mask_path[0]\n",
    "\n",
    "all_tissue_samples = find_patches_from_slide(ipath,tpath)\n",
    "print('Total patches in slide: %d' % len(all_tissue_samples)) \n",
    "all_tissue_samples.iloc[:10]\n",
    "all_tissue_samples.is_tumor.value_counts() \n",
    "test_start_time = datetime.now()\n",
    "sample_gen = gen_imgs(all_image_path,all_mask_path,all_tissue_samples, 5000, shuffle=True)\n",
    "%time example_X, example_y  = next(sample_gen)\n",
    "test_end_time = datetime.now()\n",
    "print(\"5000 gen time: %.1f minutes\" % ((test_end_time - test_start_time).seconds / 60,))\n",
    "\n",
    "start_x = 32\n",
    "start_y = 32\n",
    "pred_size = 192\n",
    "\n",
    "test_start_time = datetime.now()\n",
    "preds = []\n",
    "labels = []\n",
    "for i in range(5000):\n",
    "    prediction = predict_from_model(example_X[i],model)\n",
    "    pred_X = np.zeros((pred_size,pred_size))\n",
    "    y = example_y[i].argmax\n",
    "    for x in range(start_x,start_x+pred_size):\n",
    "        for y in range(start_y, start_y+pred_size):\n",
    "            pred_X[x-start_x][y-start_y] = prediction[x][y]\n",
    "            \n",
    "    pred_s = pd.Series(pred_X.flatten())\n",
    "    max_p = np.max(pred_s)\n",
    "    \n",
    "    y = np.max(example_y[i].argmax(axis=2))\n",
    "    preds.append(max_p)\n",
    "    labels.append(y)\n",
    "test_end_time = datetime.now()\n",
    "print(\"Model test time: %.1f minutes\" % ((test_end_time - test_start_time).seconds / 60,))    \n",
    "fpr, tpr, thresholds = metrics.roc_curve(labels,preds,pos_label=1)\n",
    "print(metrics.auc(fpr,tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004315432452131063"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_pred_x = np.max(preds)\n",
    "max_pred_x\n",
    "\n",
    "min_pred_x = np.min(preds)\n",
    "min_pred_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model test time: 0.0 minutes\n"
     ]
    }
   ],
   "source": [
    "test_start_time = datetime.now()\n",
    "slide = openslide.open_slide(ipath)\n",
    "tiles = DeepZoomGenerator(slide,tile_size=256,overlap=0, limit_bounds=False) \n",
    "test_end_time = datetime.now()\n",
    "print(\"Model test time: %.1f minutes\" % ((test_end_time - test_start_time).seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
