{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "import openslide\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "# 이부분 python 에서는 뺴주기\n",
    "\n",
    "from skimage.filters import threshold_otsu\n",
    "from openslide.deepzoom import DeepZoomGenerator\n",
    "import cv2\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# network\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Lambda, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.models import load_model\n",
    "\n",
    "# Unet\n",
    "import numpy as np \n",
    "import os\n",
    "\n",
    "import skimage.transform as trans\n",
    "#import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras\n",
    "\n",
    "\n",
    "# train\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from datetime import datetime\n",
    "\n",
    "# evaluate\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "from PIL import Image\n",
    "from xml.etree.ElementTree import ElementTree, Element, SubElement\n",
    "from io import BytesIO\n",
    "import skimage.io as io\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing.image import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 256\n",
    "IS_TRAIN = True\n",
    "def find_patches_from_slide(slide_path, truth_path, patch_size=PATCH_SIZE,filter_non_tissue=True,filter_only_all_tumor=True):\n",
    "    \n",
    "    slide_contains_tumor = 'pos' in slide_path\n",
    "    \n",
    "    ############### read_region을 위한 start, level, size를 구함 #######################\n",
    "    BOUNDS_OFFSET_PROPS = (openslide.PROPERTY_NAME_BOUNDS_X, openslide.PROPERTY_NAME_BOUNDS_Y)\n",
    "    BOUNDS_SIZE_PROPS = (openslide.PROPERTY_NAME_BOUNDS_WIDTH, openslide.PROPERTY_NAME_BOUNDS_HEIGHT)\n",
    "\n",
    "    if slide_contains_tumor:\n",
    "        with openslide.open_slide(slide_path) as slide:\n",
    "            start = (int(slide.properties.get('openslide.bounds-x',0)),int(slide.properties.get('openslide.bounds-y',0)))\n",
    "            level = np.log2(patch_size) \n",
    "            level = int(level)\n",
    "            \n",
    "            size_scale = tuple(int(slide.properties.get(prop, l0_lim)) / l0_lim\n",
    "                            for prop, l0_lim in zip(BOUNDS_SIZE_PROPS,\n",
    "                            slide.dimensions))\n",
    "            _l_dimensions = tuple(tuple(int(math.ceil(l_lim * scale))\n",
    "                            for l_lim, scale in zip(l_size, size_scale))\n",
    "                            for l_size in slide.level_dimensions)\n",
    "            size = _l_dimensions[level]\n",
    "            \n",
    "            slide4 = slide.read_region(start,level,size) \n",
    "    else :\n",
    "        with openslide.open_slide(slide_path) as slide:\n",
    "            start = (0,0)\n",
    "            level = np.log2(patch_size) \n",
    "            level = int(level)\n",
    "            \n",
    "            size_scale = (1,1)\n",
    "            _l_dimensions = tuple(tuple(int(math.ceil(l_lim * scale))\n",
    "                            for l_lim, scale in zip(l_size, size_scale))\n",
    "                            for l_size in slide.level_dimensions)\n",
    "            size = _l_dimensions[level]\n",
    "            \n",
    "            slide4 = slide.read_region(start,level,size) \n",
    "    ####################################################################################\n",
    "    \n",
    "    \n",
    "    # is_tissue 부분 \n",
    "    slide4_grey = np.array(slide4.convert('L'))\n",
    "    binary = slide4_grey > 0  # black이면 0임\n",
    "    \n",
    "    # 검은색 제외하고 흰색영역(배경이라고 여겨지는)에 대해서도 작업해주어야함.\n",
    "    slide4_not_black = slide4_grey[slide4_grey>0]\n",
    "    thresh = threshold_otsu(slide4_not_black)\n",
    "    \n",
    "    I, J = slide4_grey.shape\n",
    "    for i in range(I):\n",
    "        for j in range(J):\n",
    "            if slide4_grey[i,j] > thresh :\n",
    "                binary[i,j] = False\n",
    "    patches = pd.DataFrame(pd.DataFrame(binary).stack())\n",
    "    patches['is_tissue'] = patches[0]\n",
    "    patches.drop(0, axis=1,inplace =True)\n",
    "    patches.loc[:,'slide_path'] = slide_path\n",
    "    \n",
    "#     # Test 이면 \n",
    "#     if IS_TEST:\n",
    "#         return patches\n",
    "\n",
    "    # is_tumor 부분\n",
    "    if slide_contains_tumor:\n",
    "        with openslide.open_slide(truth_path) as truth:\n",
    "            thumbnail_truth = truth.get_thumbnail(size) \n",
    "        \n",
    "        patches_y = pd.DataFrame(pd.DataFrame(np.array(thumbnail_truth.convert(\"L\"))).stack())\n",
    "        patches_y['is_tumor'] = patches_y[0] > 0\n",
    "        \n",
    "        # mask된 영역이 애매할 수도 있으므로\n",
    "        patches_y['is_all_tumor'] = patches_y[0] == 255\n",
    "        patches_y.drop(0, axis=1, inplace=True)\n",
    "        samples = pd.concat([patches, patches_y], axis=1) #len(samples)\n",
    "    else:\n",
    "        samples = patches\n",
    "        #dfmi.loc[:,('one','second')] = value\n",
    "        samples.loc[:,'is_tumor'] = False\n",
    "        samples.loc[:,'is_all_tumor'] = False\n",
    "    \n",
    "    if filter_non_tissue:\n",
    "        samples = samples[samples.is_tissue == True] # remove patches with no tissue #samples = samples[samples.is_tissue == True]\n",
    "    \n",
    "    if filter_only_all_tumor :\n",
    "        samples['tile_loc'] = list(samples.index)\n",
    "        all_tissue_samples1 = samples[samples.is_tumor==False]\n",
    "        all_tissue_samples1 = all_tissue_samples1.append(samples[samples.is_all_tumor==True])\n",
    "        \n",
    "        all_tissue_samples1.reset_index(inplace=True, drop=True)\n",
    "    else :\n",
    "        return samples\n",
    "    \n",
    "    return all_tissue_samples1\n",
    "NUM_CLASSES = 2 # not_tumor, tumor\n",
    "\n",
    "file_handles=[]\n",
    "def gen_imgs(all_image_path, all_mask_path, samples, batch_size, patch_size = PATCH_SIZE, shuffle=True):\n",
    "   \n",
    "    num_samples = len(samples)\n",
    "    # 특정 몇개의 slide만 open 해서 쓰기\n",
    "    # 4개씩 묶었으니까 \n",
    "  \n",
    "    slide_path0 = all_image_path[0]\n",
    "    slide_path1 = all_image_path[1]\n",
    "    slide_path2 = all_image_path[2]\n",
    "    slide_path3 = all_image_path[3]\n",
    "    \n",
    "    \n",
    "    # slide 0~3 까지 미리 열어두기\n",
    "    slide0 = openslide.open_slide(slide_path0)\n",
    "    slide1 = openslide.open_slide(slide_path1)\n",
    "    slide2 = openslide.open_slide(slide_path2)\n",
    "    slide3 = openslide.open_slide(slide_path3)\n",
    "    file_handles.append(slide0)\n",
    "    file_handles.append(slide1)\n",
    "    file_handles.append(slide2)\n",
    "    file_handles.append(slide3)\n",
    "    \n",
    "    # with openslide.open_slide(slide_path) as slide\n",
    "    tiles0 = DeepZoomGenerator(slide0,tile_size=patch_size, overlap=0, limit_bounds=False) \n",
    "    tiles1 = DeepZoomGenerator(slide1,tile_size=patch_size, overlap=0, limit_bounds=False)\n",
    "    tiles2 = DeepZoomGenerator(slide2,tile_size=patch_size, overlap=0, limit_bounds=False)\n",
    "    tiles3 = DeepZoomGenerator(slide3,tile_size=patch_size, overlap=0, limit_bounds=False)\n",
    "    \n",
    "    \n",
    "    if 'pos' in slide_path0:\n",
    "        start_x0 = int(slide0.properties.get('openslide.bounds-x',0))\n",
    "        start_y0 = int(slide0.properties.get('openslide.bounds-y',0))\n",
    "        start_x0 = start_x0 / patch_size\n",
    "        start_y0 = start_y0 / patch_size\n",
    "        \n",
    "        truth0 = openslide.open_slide(all_mask_path[0])\n",
    "        truth_tiles0 = DeepZoomGenerator(truth0, tile_size=16, overlap=0, limit_bounds=False) \n",
    "        \n",
    "    else : \n",
    "        start_x0 = 0\n",
    "        start_y0 = 0\n",
    "    \n",
    "    if 'pos' in slide_path1:\n",
    "        start_x1 = int(slide1.properties.get('openslide.bounds-x',0))\n",
    "        start_y1 = int(slide1.properties.get('openslide.bounds-y',0))\n",
    "        start_x1 = start_x1 / patch_size\n",
    "        start_y1 = start_y1 / patch_size\n",
    "        \n",
    "        truth1 = openslide.open_slide(all_mask_path[1])\n",
    "        truth_tiles1 = DeepZoomGenerator(truth1, tile_size=16, overlap=0, limit_bounds=False) \n",
    "        \n",
    "    else : \n",
    "        start_x1 = 0\n",
    "        start_y1 = 0\n",
    "    \n",
    "    if 'pos' in slide_path2:\n",
    "        start_x2 = int(slide2.properties.get('openslide.bounds-x',0))\n",
    "        start_y2 = int(slide2.properties.get('openslide.bounds-y',0))\n",
    "        start_x2 = start_x2 / patch_size\n",
    "        start_y2 = start_y2 / patch_size\n",
    "        \n",
    "        truth2 = openslide.open_slide(all_mask_path[2])\n",
    "        truth_tiles2 = DeepZoomGenerator(truth2, tile_size=16, overlap=0, limit_bounds=False) \n",
    "        \n",
    "    else : \n",
    "        start_x2 = 0\n",
    "        start_y2 = 0\n",
    "        \n",
    "    if 'pos' in slide_path3:\n",
    "        start_x3 = int(slide3.properties.get('openslide.bounds-x',0))\n",
    "        start_y3 = int(slide3.properties.get('openslide.bounds-y',0))\n",
    "        start_x3 = start_x3 / patch_size\n",
    "        start_y3 = start_y3 / patch_size\n",
    "        \n",
    "        truth3 = openslide.open_slide(all_mask_path[3])\n",
    "        truth_tiles3 = DeepZoomGenerator(truth3, tile_size=16, overlap=0, limit_bounds=False) \n",
    "        \n",
    "    else : \n",
    "        start_x3 = 0\n",
    "        start_y3 = 0\n",
    "    \n",
    "\n",
    "    \n",
    "    for epo in range(10): # Loop forever so the generator never terminates\n",
    "        if shuffle:\n",
    "            samples = samples.sample(frac=1) # shuffle samples\n",
    "\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples.iloc[offset:offset+batch_size]\n",
    "            images = []\n",
    "            masks = []\n",
    "            for _, batch_sample in batch_samples.iterrows(): # 배치마다 deep zoom 하네 약간 비효율적\n",
    "                \n",
    "                # 여기서 하나씩 4개 체크해서 해당되는 부분으로 가야지. for 4번 돌리면서 가야한다.\n",
    "                mask_size_up = np.zeros((patch_size,patch_size))\n",
    "                a,b=mask_size_up.shape\n",
    "                \n",
    "                if batch_sample.slide_path == slide_path0:\n",
    "                    x, y = batch_sample.tile_loc[::-1]\n",
    "                    x += start_x0\n",
    "                    y += start_y0\n",
    "                    img = tiles0.get_tile(tiles0.level_count-1, (x,y))\n",
    "                    if 'pos' in slide_path0:\n",
    "                        mask = truth_tiles0.get_tile(truth_tiles0.level_count-1, batch_sample.tile_loc[::-1])\n",
    "                        mask = (cv2.cvtColor(np.array(mask), cv2.COLOR_RGB2GRAY) > 0).astype(int)\n",
    "                            # mask_size_up , 16 to 256\n",
    "                        for i in range(a):\n",
    "                            for j in range(b) :\n",
    "                                k = i//16\n",
    "                                l = j//16\n",
    "                                mask_size_up[i,j] = mask[k,l]\n",
    "                    \n",
    "                elif batch_sample.slide_path == slide_path1:\n",
    "                    x, y = batch_sample.tile_loc[::-1]\n",
    "                    x += start_x1\n",
    "                    y += start_y1\n",
    "                    img = tiles1.get_tile(tiles1.level_count-1, (x,y))\n",
    "                    if 'pos' in slide_path1:\n",
    "                        mask = truth_tiles1.get_tile(truth_tiles1.level_count-1, batch_sample.tile_loc[::-1])\n",
    "                        mask = (cv2.cvtColor(np.array(mask), cv2.COLOR_RGB2GRAY) > 0).astype(int)\n",
    "                            # mask_size_up , 16 to 256\n",
    "                        for i in range(a):\n",
    "                            for j in range(b) :\n",
    "                                k = i//16\n",
    "                                l = j//16\n",
    "                                mask_size_up[i,j] = mask[k,l]\n",
    "                \n",
    "                elif batch_sample.slide_path == slide_path2:\n",
    "                    x, y = batch_sample.tile_loc[::-1]\n",
    "                    x += start_x2\n",
    "                    y += start_y2\n",
    "                    img = tiles2.get_tile(tiles2.level_count-1, (x,y))\n",
    "                    if 'pos' in slide_path2:\n",
    "                        mask = truth_tiles2.get_tile(truth_tiles2.level_count-1, batch_sample.tile_loc[::-1])\n",
    "                        mask = (cv2.cvtColor(np.array(mask), cv2.COLOR_RGB2GRAY) > 0).astype(int)\n",
    "                            # mask_size_up , 16 to 256\n",
    "                        for i in range(a):\n",
    "                            for j in range(b) :\n",
    "                                k = i//16\n",
    "                                l = j//16\n",
    "                                mask_size_up[i,j] = mask[k,l]\n",
    "                \n",
    "                else:\n",
    "                    x, y = batch_sample.tile_loc[::-1]\n",
    "                    x += start_x3\n",
    "                    y += start_y3\n",
    "                    img = tiles3.get_tile(tiles3.level_count-1, (x,y))\n",
    "                    if 'pos' in slide_path3:\n",
    "                        mask = truth_tiles3.get_tile(truth_tiles3.level_count-1, batch_sample.tile_loc[::-1])\n",
    "                        mask = (cv2.cvtColor(np.array(mask), cv2.COLOR_RGB2GRAY) > 0).astype(int)\n",
    "                            # mask_size_up , 16 to 256\n",
    "                        for i in range(a):\n",
    "                            for j in range(b) :\n",
    "                                k = i//16\n",
    "                                l = j//16\n",
    "                                mask_size_up[i,j] = mask[k,l]\n",
    "                \n",
    "                    \n",
    "\n",
    "                images.append(np.array(img))\n",
    "                masks.append(mask_size_up)\n",
    "\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(masks)\n",
    "            #print('x_train_shape :', X_train.shape)\n",
    "            \n",
    "            y_train = to_categorical(y_train, num_classes=2).reshape(y_train.shape[0], patch_size, patch_size, 2) \n",
    "            #print('y_train_shape : ',y_train.shape)\n",
    "            \n",
    "            #X_train, y_train = datagen().flow(X_train,y = y_train,batch_size = batch_size)\n",
    "            X_train, y_train = next(ImageDataGenerator(\n",
    "                rotation_range=90,\n",
    "                horizontal_flip=True,\n",
    "                vertical_flip=True,\n",
    "                brightness_range =(0.25,1.)).flow(X_train,y=y_train,batch_size=batch_size))\n",
    "            #print(X_train.shape)\n",
    "            #print(y_train.shape)\n",
    "            yield X_train, y_train\n",
    "            \n",
    "def predict_batch_from_model(patches, model):\n",
    "    \"\"\"Predict which pixels are tumor.\n",
    "    \n",
    "    input: patch: `batch_size`x256x256x3, rgb image\n",
    "    input: model: keras model\n",
    "    output: prediction: 256x256x1, per-pixel tumor probability\n",
    "    \"\"\"\n",
    "    predictions = model.predict(patches)\n",
    "    predictions = predictions[:, :, :, 1]\n",
    "    return predictions\n",
    "def predict_from_model(patch, model):\n",
    "    \"\"\"Predict which pixels are tumor.\n",
    "    \n",
    "    input: patch: 256x256x3, rgb image\n",
    "    input: model: keras model\n",
    "    output: prediction: 256x256x1, per-pixel tumor probability\n",
    "    \"\"\"\n",
    "    \n",
    "    prediction = model.predict(patch.reshape(1, 256, 256, 3))\n",
    "    prediction = prediction[:, :, :, 1].reshape(256, 256)\n",
    "    return prediction\n",
    "\n",
    "def predict_from_model_n(patch, model):\n",
    "    \"\"\"Predict which pixels are tumor.\n",
    "    \n",
    "    input: patch: 256x256x3, rgb image\n",
    "    input: model: keras model\n",
    "    output: prediction: 256x256x1, per-pixel tumor probability\n",
    "    \"\"\"\n",
    "    \n",
    "    prediction = model.predict(patch.reshape(1, 256, 256, 3))\n",
    "    prediction = prediction[:, :, :, 0].reshape(256, 256)\n",
    "    return prediction\n",
    "\n",
    "def simple_model(pretrained_weights = None):\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(256, 256, 3)))\n",
    "    model.add(Convolution2D(100, (3, 3), strides=(2, 2), activation='elu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Convolution2D(200, (3, 3), strides=(2, 2), activation='elu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Convolution2D(300, (3, 3), activation='elu', padding='same'))\n",
    "    model.add(Convolution2D(300, (3, 3), activation='elu',  padding='same'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Convolution2D(2, (1, 1))) # this is called upscore layer for some reason?\n",
    "    model.add(Conv2DTranspose(2, (31, 31), strides=(16, 16), activation='softmax', padding='same'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    if(pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_path # :  157\n",
      "mask_patch # :  157\n"
     ]
    }
   ],
   "source": [
    "def read_data_path():\n",
    "    image_paths = []\n",
    "    with open('train.txt','r') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip('\\n')\n",
    "            image_paths.append(line)\n",
    "    #print('image_path # : ',len(image_paths))\n",
    "\n",
    "    tumor_mask_paths = []\n",
    "\n",
    "    with open('train_mask.txt','r') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip('\\n')\n",
    "            tumor_mask_paths.append(line)\n",
    "    #print('mask_patch # : ',len(tumor_mask_paths))\n",
    "    \n",
    "    return image_paths, tumor_mask_paths\n",
    "\n",
    "def read_test_data_path():\n",
    "    image_paths = []\n",
    "    with open('test.txt','r') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip('\\n')\n",
    "            image_paths.append(line)\n",
    "    #print('image_path # : ',len(image_paths))\n",
    "    \n",
    "    return image_paths\n",
    "\n",
    "test_image_paths = read_test_data_path()\n",
    "image_paths, tumor_mask_paths = read_data_path()\n",
    "image_paths = []\n",
    "with open('train.txt','r') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip('\\n')\n",
    "        image_paths.append(line)\n",
    "print('image_path # : ',len(image_paths))\n",
    "\n",
    "tumor_mask_paths = []\n",
    "with open('train_mask.txt','r') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip('\\n')\n",
    "        tumor_mask_paths.append(line)\n",
    "print('mask_patch # : ',len(tumor_mask_paths))\n",
    "\n",
    "slide_4_list_1 = [[102,104,29,44],[144,55,30,18],[125,56,35,40],[54,65,21,36],[139,82,1,49],[73,108,7,23],[107,117,24,52],[106,103,27,13]\n",
    "               ,[105,151,15,2],[75,100,41,9],[156,113,32,37],[150,88,39,10],[84,122,5,50],[93,118,53,47],[87,78,45,34],[116,98,48,46],\n",
    "                [72,131,22,42]]\n",
    "slide_4_list_2 = [[109,58,14,28],[101,69,11,43],[94,74,3,20],[64,140,17,16],[92,154,8,26],[99,60,0,33],[86,146,25,19],[68,112,38,51],\n",
    "                 [71,136,31,4],[59,91,12,6]]\n",
    "slide_4_list_3 = [[143,132,124,85],[95,120,81,77],[97,96,110,83],[152,128,149,155],[153,111,57,138],[134,135,114,76],\n",
    "                  [123,90,121,61],[147,148,119,142],[66,137,63,80],[70,79,115,133],[129,141,127,145]]\n",
    "slide_4_test = [[55,55,0,0],[55,55,0,0]]\n",
    "\n",
    "all_image_path = []\n",
    "all_mask_path = []\n",
    "for j in range(4):\n",
    "    image_path = image_paths[slide_4_test[0][j]][1:] # 이 부분은 data 읽을때 고치자 ( [1:] 빼야함)\n",
    "    mask_path = tumor_mask_paths[slide_4_test[0][j]][1:] # 이 부분은 data 읽을때 고치자\n",
    "    all_image_path.append(image_path)\n",
    "    all_mask_path.append(mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inception\n",
    "# imagenet_utils에 있는 함수들\n",
    "\"\"\"Utilities for ImageNet data preprocessing & prediction decoding.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras_applications import imagenet_utils\n",
    "import keras\n",
    "_KERAS_BACKEND = keras.backend\n",
    "_KERAS_LAYERS = keras.layers\n",
    "_KERAS_MODELS = keras.models\n",
    "_KERAS_UTILS = keras.utils\n",
    "\n",
    "import os.path as osp\n",
    "import openslide\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# 이부분 python 에서는 뺴주기\n",
    "\n",
    "from skimage.filters import threshold_otsu\n",
    "from openslide.deepzoom import DeepZoomGenerator\n",
    "import cv2\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# network\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Lambda, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.models import load_model\n",
    "\n",
    "# Unet\n",
    "import numpy as np \n",
    "import os\n",
    "\n",
    "import skimage.transform as trans\n",
    "#import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras\n",
    "\n",
    "\n",
    "# train\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from datetime import datetime\n",
    "\n",
    "# evaluate\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "from PIL import Image\n",
    "from xml.etree.ElementTree import ElementTree, Element, SubElement\n",
    "from io import BytesIO\n",
    "import skimage.io as io\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from sklearn import metrics\n",
    "from keras_applications import imagenet_utils\n",
    "# network\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Lambda, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras import optimizers\n",
    "\n",
    "down_para = 2\n",
    "PATCH_SIZE = 256\n",
    "\n",
    "def set_keras_submodules(backend=None,\n",
    "                         layers=None,\n",
    "                         models=None,\n",
    "                         utils=None,\n",
    "                         engine=None):\n",
    "    # Deprecated, will be removed in the future.\n",
    "    global _KERAS_BACKEND\n",
    "    global _KERAS_LAYERS\n",
    "    global _KERAS_MODELS\n",
    "    global _KERAS_UTILS\n",
    "    _KERAS_BACKEND = backend\n",
    "    _KERAS_LAYERS = layers\n",
    "    _KERAS_MODELS = models\n",
    "    _KERAS_UTILS = utils\n",
    "def get_keras_submodule(name):\n",
    "    # Deprecated, will be removed in the future.\n",
    "    if name not in {'backend', 'layers', 'models', 'utils'}:\n",
    "        raise ImportError(\n",
    "            'Can only retrieve one of \"backend\", '\n",
    "            '\"layers\", \"models\", or \"utils\". '\n",
    "            'Requested: %s' % name)\n",
    "    if _KERAS_BACKEND is None:\n",
    "        raise ImportError('You need to first `import keras` '\n",
    "                          'in order to use `keras_applications`. '\n",
    "                          'For instance, you can do:\\n\\n'\n",
    "                          '```\\n'\n",
    "                          'import keras\\n'\n",
    "                          'from keras_applications import vgg16\\n'\n",
    "                          '```\\n\\n'\n",
    "                          'Or, preferably, this equivalent formulation:\\n\\n'\n",
    "                          '```\\n'\n",
    "                          'from keras import applications\\n'\n",
    "                          '```\\n')\n",
    "    if name == 'backend':\n",
    "        return _KERAS_BACKEND\n",
    "    elif name == 'layers':\n",
    "        return _KERAS_LAYERS\n",
    "    elif name == 'models':\n",
    "        return _KERAS_MODELS\n",
    "    elif name == 'utils':\n",
    "        return _KERAS_UTILS\n",
    "def get_submodules_from_kwargs(kwargs):\n",
    "    backend = kwargs.get('backend', _KERAS_BACKEND)\n",
    "    layers = kwargs.get('layers', _KERAS_LAYERS)\n",
    "    models = kwargs.get('models', _KERAS_MODELS)\n",
    "    utils = kwargs.get('utils', _KERAS_UTILS)\n",
    "    for key in kwargs.keys():\n",
    "        if key not in ['backend', 'layers', 'models', 'utils']:\n",
    "            raise TypeError('Invalid keyword argument: %s', key)\n",
    "    return backend, layers, models, utils\n",
    "def correct_pad(backend, inputs, kernel_size):\n",
    "    \"\"\"Returns a tuple for zero-padding for 2D convolution with downsampling.\n",
    "    # Arguments\n",
    "        input_size: An integer or tuple/list of 2 integers.\n",
    "        kernel_size: An integer or tuple/list of 2 integers.\n",
    "    # Returns\n",
    "        A tuple.\n",
    "    \"\"\"\n",
    "    img_dim = 2 if backend.image_data_format() == 'channels_first' else 1\n",
    "    input_size = backend.int_shape(inputs)[img_dim:(img_dim + 2)]\n",
    "\n",
    "    if isinstance(kernel_size, int):\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "\n",
    "    if input_size[0] is None:\n",
    "        adjust = (1, 1)\n",
    "    else:\n",
    "        adjust = (1 - input_size[0] % 2, 1 - input_size[1] % 2)\n",
    "\n",
    "    correct = (kernel_size[0] // 2, kernel_size[1] // 2)\n",
    "\n",
    "    return ((correct[0] - adjust[0], correct[0]),\n",
    "            (correct[1] - adjust[1], correct[1]))\n",
    "\n",
    "__version__ = '1.0.7'\n",
    "\n",
    "\"\"\"Inception V3 model for Keras.\n",
    "Note that the input image format for this model is different than for\n",
    "the VGG16 and ResNet models (299x299 instead of 224x224),\n",
    "and that the input preprocessing function is also different (same as Xception).\n",
    "# Reference\n",
    "- [Rethinking the Inception Architecture for Computer Vision](\n",
    "    http://arxiv.org/abs/1512.00567)\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "WEIGHTS_PATH = (\n",
    "    'https://github.com/fchollet/deep-learning-models/'\n",
    "    'releases/download/v0.5/'\n",
    "    'inception_v3_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "WEIGHTS_PATH_NO_TOP = (\n",
    "    'https://github.com/fchollet/deep-learning-models/'\n",
    "    'releases/download/v0.5/'\n",
    "    'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "\n",
    "# 이부분도\n",
    "def conv2d_bn(x,\n",
    "              filters,\n",
    "              num_row,\n",
    "              num_col,\n",
    "              padding='same',\n",
    "              strides=(1, 1),\n",
    "              name=None):\n",
    "    \"\"\"Utility function to apply conv + BN.\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        filters: filters in `Conv2D`.\n",
    "        num_row: height of the convolution kernel.\n",
    "        num_col: width of the convolution kernel.\n",
    "        padding: padding mode in `Conv2D`.\n",
    "        strides: strides in `Conv2D`.\n",
    "        name: name of the ops; will become `name + '_conv'`\n",
    "            for the convolution and `name + '_bn'` for the\n",
    "            batch norm layer.\n",
    "    # Returns\n",
    "        Output tensor after applying `Conv2D` and `BatchNormalization`.\n",
    "    \"\"\"\n",
    "    if name is not None:\n",
    "        bn_name = name + '_bn'\n",
    "        conv_name = name + '_conv'\n",
    "    else:\n",
    "        bn_name = None\n",
    "        conv_name = None\n",
    "    if backend.image_data_format() == 'channels_first':\n",
    "        bn_axis = 1\n",
    "    else:\n",
    "        bn_axis = 3\n",
    "    x = layers.Conv2D(\n",
    "        filters, (num_row, num_col),\n",
    "        strides=strides,\n",
    "        padding=padding,\n",
    "        use_bias=False,\n",
    "        name=conv_name)(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, scale=False, name=bn_name)(x)\n",
    "    x = layers.Activation('relu', name=name)(x)\n",
    "    return x\n",
    "def InceptionV3(include_top=True,\n",
    "                weights='imagenet',\n",
    "                input_tensor=None,\n",
    "                input_shape=None,\n",
    "                pooling=None,\n",
    "                classes=1000,\n",
    "                down_para = down_para,\n",
    "                **kwargs):\n",
    "    \"\"\"Instantiates the Inception v3 architecture.\n",
    "    Optionally loads weights pre-trained on ImageNet.\n",
    "    Note that the data format convention used by the model is\n",
    "    the one specified in your Keras config at `~/.keras/keras.json`.\n",
    "    # Arguments\n",
    "        include_top: whether to include the fully-connected\n",
    "            layer at the top of the network.\n",
    "        weights: one of `None` (random initialization),\n",
    "              'imagenet' (pre-training on ImageNet),\n",
    "              or the path to the weights file to be loaded.\n",
    "        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        input_shape: optional shape tuple, only to be specified\n",
    "            if `include_top` is False (otherwise the input shape\n",
    "            has to be `(299, 299, 3)` (with `channels_last` data format)\n",
    "            or `(3, 299, 299)` (with `channels_first` data format).\n",
    "            It should have exactly 3 inputs channels,\n",
    "            and width and height should be no smaller than 75.\n",
    "            E.g. `(150, 150, 3)` would be one valid value.\n",
    "        pooling: Optional pooling mode for feature extraction\n",
    "            when `include_top` is `False`.\n",
    "            - `None` means that the output of the model will be\n",
    "                the 4D tensor output of the\n",
    "                last convolutional block.\n",
    "            - `avg` means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional block, and thus\n",
    "                the output of the model will be a 2D tensor.\n",
    "            - `max` means that global max pooling will\n",
    "                be applied.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "    # Raises\n",
    "        ValueError: in case of invalid argument for `weights`,\n",
    "            or invalid input shape.\n",
    "    \"\"\"\n",
    "    global backend, layers, models, keras_utils\n",
    "    backend, layers, models, keras_utils = get_submodules_from_kwargs(kwargs)\n",
    "\n",
    "\n",
    "\n",
    "    # Determine proper input shape\n",
    "    input_shape = imagenet_utils._obtain_input_shape(\n",
    "        input_shape,\n",
    "        default_size=299,\n",
    "        min_size=75,\n",
    "        data_format=backend.image_data_format(),\n",
    "        require_flatten=include_top,\n",
    "        weights=weights)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = layers.Input(shape=input_shape)\n",
    "    else:\n",
    "        if not backend.is_keras_tensor(input_tensor):\n",
    "            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    if backend.image_data_format() == 'channels_first':\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = 3\n",
    "\n",
    "    x = conv2d_bn(img_input, 32//down_para, 3, 3, strides=(2, 2), padding='same')\n",
    "    x = conv2d_bn(x, 32//down_para, 3, 3, padding='same')\n",
    "    x = conv2d_bn(x, 64//down_para, 3, 3)\n",
    "    x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    x = conv2d_bn(x, 80//down_para, 1, 1, padding='same')\n",
    "    x = conv2d_bn(x, 192//down_para, 3, 3, padding='same')\n",
    "    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    # mixed 0: 35 x 35 x 256\n",
    "    branch1x1 = conv2d_bn(x, 64//down_para, 1, 1)\n",
    "\n",
    "    branch5x5 = conv2d_bn(x, 48//down_para, 1, 1)\n",
    "    branch5x5 = conv2d_bn(branch5x5, 64//down_para, 5, 5)\n",
    "\n",
    "    branch3x3dbl = conv2d_bn(x, 64//down_para, 1, 1)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96//down_para, 3, 3)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96//down_para, 3, 3)\n",
    "\n",
    "    branch_pool = layers.AveragePooling2D((3, 3),\n",
    "                                          strides=(1, 1),\n",
    "                                          padding='same')(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 32//down_para, 1, 1)\n",
    "    x = layers.concatenate(\n",
    "        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed0')\n",
    "\n",
    "    # mixed 1: 35 x 35 x 288\n",
    "    branch1x1 = conv2d_bn(x, 64//down_para, 1, 1)\n",
    "\n",
    "    branch5x5 = conv2d_bn(x, 48//down_para, 1, 1)\n",
    "    branch5x5 = conv2d_bn(branch5x5, 64//down_para, 5, 5)\n",
    "\n",
    "    branch3x3dbl = conv2d_bn(x, 64//down_para, 1, 1)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96//down_para, 3, 3)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96//down_para, 3, 3)\n",
    "\n",
    "    branch_pool = layers.AveragePooling2D((3, 3),\n",
    "                                          strides=(1, 1),\n",
    "                                          padding='same')(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 64//down_para, 1, 1)\n",
    "    x = layers.concatenate(\n",
    "        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed1')\n",
    "\n",
    "    # mixed 2: 35 x 35 x 288\n",
    "    branch1x1 = conv2d_bn(x, 64//down_para, 1, 1)\n",
    "\n",
    "    branch5x5 = conv2d_bn(x, 48//down_para, 1, 1)\n",
    "    branch5x5 = conv2d_bn(branch5x5, 64//down_para, 5, 5)\n",
    "\n",
    "    branch3x3dbl = conv2d_bn(x, 64//down_para, 1, 1)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96//down_para, 3, 3)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96//down_para, 3, 3)\n",
    "\n",
    "    branch_pool = layers.AveragePooling2D((3, 3),\n",
    "                                          strides=(1, 1),\n",
    "                                          padding='same')(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 64//down_para, 1, 1)\n",
    "    x = layers.concatenate(\n",
    "        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed2')\n",
    "\n",
    "    # mixed 3: 17 x 17 x 768\n",
    "    branch3x3 = conv2d_bn(x, 384//down_para, 3, 3, strides=(2, 2), padding='same')\n",
    "\n",
    "    branch3x3dbl = conv2d_bn(x, 64//down_para, 1, 1)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96//down_para, 3, 3)\n",
    "    branch3x3dbl = conv2d_bn(\n",
    "        branch3x3dbl, 96//down_para, 3, 3, strides=(2, 2), padding='same')\n",
    "\n",
    "    branch_pool = layers.MaxPooling2D((3, 3), strides=(2, 2),padding='same')(x)\n",
    "    x = layers.concatenate(\n",
    "        [branch3x3, branch3x3dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed3')\n",
    "\n",
    "    # mixed 4: 17 x 17 x 768\n",
    "    branch1x1 = conv2d_bn(x, 192//down_para, 1, 1)\n",
    "\n",
    "    branch7x7 = conv2d_bn(x, 128//down_para, 1, 1)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 128//down_para, 1, 7)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 192//down_para, 7, 1)\n",
    "\n",
    "    branch7x7dbl = conv2d_bn(x, 128//down_para, 1, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 128//down_para, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 128//down_para, 1, 7)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 128//down_para, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192//down_para, 1, 7)\n",
    "\n",
    "    branch_pool = layers.AveragePooling2D((3, 3),\n",
    "                                          strides=(1, 1),\n",
    "                                          padding='same')(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 192//down_para, 1, 1)\n",
    "    x = layers.concatenate(\n",
    "        [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed4')\n",
    "\n",
    "    # mixed 5, 6: 17 x 17 x 768\n",
    "    for i in range(2):\n",
    "        branch1x1 = conv2d_bn(x, 192//down_para, 1, 1)\n",
    "\n",
    "        branch7x7 = conv2d_bn(x, 160//down_para, 1, 1)\n",
    "        branch7x7 = conv2d_bn(branch7x7, 160//down_para, 1, 7)\n",
    "        branch7x7 = conv2d_bn(branch7x7, 192//down_para, 7, 1)\n",
    "\n",
    "        branch7x7dbl = conv2d_bn(x, 160//down_para, 1, 1)\n",
    "        branch7x7dbl = conv2d_bn(branch7x7dbl, 160//down_para, 7, 1)\n",
    "        branch7x7dbl = conv2d_bn(branch7x7dbl, 160//down_para, 1, 7)\n",
    "        branch7x7dbl = conv2d_bn(branch7x7dbl, 160//down_para, 7, 1)\n",
    "        branch7x7dbl = conv2d_bn(branch7x7dbl, 192//down_para, 1, 7)\n",
    "\n",
    "        branch_pool = layers.AveragePooling2D(\n",
    "            (3, 3), strides=(1, 1), padding='same')(x)\n",
    "        branch_pool = conv2d_bn(branch_pool, 192//down_para, 1, 1)\n",
    "        x = layers.concatenate(\n",
    "            [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "            axis=channel_axis,\n",
    "            name='mixed' + str(5 + i))\n",
    "\n",
    "    # mixed 7: 17 x 17 x 768\n",
    "    branch1x1 = conv2d_bn(x, 192//down_para, 1, 1)\n",
    "\n",
    "    branch7x7 = conv2d_bn(x, 192//down_para, 1, 1)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 192//down_para, 1, 7)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 192//down_para, 7, 1)\n",
    "\n",
    "    branch7x7dbl = conv2d_bn(x, 192//down_para, 1, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192//down_para, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192//down_para, 1, 7)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192//down_para, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192//down_para, 1, 7)\n",
    "\n",
    "    branch_pool = layers.AveragePooling2D((3, 3),\n",
    "                                          strides=(1, 1),\n",
    "                                          padding='same')(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 192//down_para, 1, 1)\n",
    "    x = layers.concatenate(\n",
    "        [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed7')\n",
    "\n",
    "    # 여기서부터 upsampling 해보자, 14,14,192\n",
    "    \n",
    "    up6 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(x))\n",
    "    #up6 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    #merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(up6)\n",
    "    conv6 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    #merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(up7)\n",
    "    conv7 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(32, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    #merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(up8)\n",
    "    conv8 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(16, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    #merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    conv9 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(up9)\n",
    "    conv9 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv10 = Conv2D(2, 1, activation = 'softmax')(conv9)\n",
    "\n",
    "    model = Model(input = img_input, output = conv10)\n",
    "\n",
    "    adam = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    #model.compile(optimizer = Adam(lr = 1e-2), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    if weights is not None:\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    return model \n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = keras_utils.get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = models.Model(inputs, x, name='inception_v3')\n",
    "\n",
    "    # Load weights.\n",
    "    if weights == 'imagenet':\n",
    "        if include_top:\n",
    "            weights_path = keras_utils.get_file(\n",
    "                'inception_v3_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                WEIGHTS_PATH,\n",
    "                cache_subdir='models',\n",
    "                file_hash='9a0d58056eeedaa3f26cb7ebd46da564')\n",
    "        else:\n",
    "            weights_path = keras_utils.get_file(\n",
    "                'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                WEIGHTS_PATH_NO_TOP,\n",
    "                cache_subdir='models',\n",
    "                file_hash='bcbd6486424b2319ff4ef7d526e38f63')\n",
    "        model.load_weights(weights_path)\n",
    "    elif weights is not None:\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:466: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n"
     ]
    }
   ],
   "source": [
    "model = InceptionV3(include_top=True,\n",
    "                weights='i_1.h5',\n",
    "                input_tensor=None,\n",
    "                input_shape=(256,256,3),\n",
    "                pooling=None,\n",
    "                classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:86: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61446\n",
      "160136\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203/704 [=======>......................] - ETA: 4:23:14 - loss: 0.1170 - acc: 0.95 - ETA: 2:14:12 - loss: 0.0760 - acc: 0.97 - ETA: 1:31:10 - loss: 0.1386 - acc: 0.95 - ETA: 1:09:40 - loss: 0.1248 - acc: 0.96 - ETA: 57:11 - loss: 0.1113 - acc: 0.9651 - ETA: 56:10 - loss: 0.1058 - acc: 0.96 - ETA: 55:45 - loss: 0.1054 - acc: 0.96 - ETA: 55:06 - loss: 0.1336 - acc: 0.96 - ETA: 54:52 - loss: 0.1203 - acc: 0.96 - ETA: 54:39 - loss: 0.1218 - acc: 0.96 - ETA: 53:17 - loss: 0.1248 - acc: 0.96 - ETA: 51:30 - loss: 0.1249 - acc: 0.96 - ETA: 50:06 - loss: 0.1163 - acc: 0.97 - ETA: 48:47 - loss: 0.1180 - acc: 0.97 - ETA: 47:42 - loss: 0.1115 - acc: 0.97 - ETA: 46:47 - loss: 0.1124 - acc: 0.97 - ETA: 45:52 - loss: 0.1088 - acc: 0.97 - ETA: 45:04 - loss: 0.1064 - acc: 0.97 - ETA: 44:21 - loss: 0.1044 - acc: 0.97 - ETA: 43:41 - loss: 0.1047 - acc: 0.97 - ETA: 43:10 - loss: 0.1056 - acc: 0.97 - ETA: 42:37 - loss: 0.1049 - acc: 0.97 - ETA: 42:08 - loss: 0.1066 - acc: 0.97 - ETA: 41:46 - loss: 0.1027 - acc: 0.97 - ETA: 41:19 - loss: 0.1035 - acc: 0.97 - ETA: 40:57 - loss: 0.1002 - acc: 0.97 - ETA: 40:37 - loss: 0.0977 - acc: 0.97 - ETA: 40:17 - loss: 0.1033 - acc: 0.97 - ETA: 39:57 - loss: 0.1022 - acc: 0.97 - ETA: 39:39 - loss: 0.1034 - acc: 0.97 - ETA: 39:21 - loss: 0.1005 - acc: 0.97 - ETA: 39:03 - loss: 0.0992 - acc: 0.97 - ETA: 38:48 - loss: 0.0987 - acc: 0.97 - ETA: 38:34 - loss: 0.1032 - acc: 0.97 - ETA: 38:19 - loss: 0.1068 - acc: 0.96 - ETA: 38:06 - loss: 0.1059 - acc: 0.96 - ETA: 37:54 - loss: 0.1110 - acc: 0.96 - ETA: 37:42 - loss: 0.1102 - acc: 0.96 - ETA: 37:29 - loss: 0.1092 - acc: 0.96 - ETA: 37:18 - loss: 0.1107 - acc: 0.96 - ETA: 37:07 - loss: 0.1084 - acc: 0.96 - ETA: 36:57 - loss: 0.1066 - acc: 0.96 - ETA: 36:47 - loss: 0.1070 - acc: 0.96 - ETA: 36:39 - loss: 0.1062 - acc: 0.96 - ETA: 36:30 - loss: 0.1046 - acc: 0.96 - ETA: 36:20 - loss: 0.1056 - acc: 0.96 - ETA: 36:11 - loss: 0.1055 - acc: 0.96 - ETA: 36:03 - loss: 0.1077 - acc: 0.96 - ETA: 35:56 - loss: 0.1067 - acc: 0.96 - ETA: 35:47 - loss: 0.1051 - acc: 0.96 - ETA: 35:39 - loss: 0.1046 - acc: 0.96 - ETA: 35:31 - loss: 0.1045 - acc: 0.96 - ETA: 35:24 - loss: 0.1040 - acc: 0.96 - ETA: 35:16 - loss: 0.1033 - acc: 0.96 - ETA: 35:11 - loss: 0.1026 - acc: 0.96 - ETA: 35:04 - loss: 0.1018 - acc: 0.96 - ETA: 34:56 - loss: 0.1008 - acc: 0.97 - ETA: 34:49 - loss: 0.1001 - acc: 0.97 - ETA: 34:43 - loss: 0.0999 - acc: 0.97 - ETA: 34:36 - loss: 0.1009 - acc: 0.96 - ETA: 34:29 - loss: 0.1004 - acc: 0.96 - ETA: 34:23 - loss: 0.1001 - acc: 0.96 - ETA: 34:16 - loss: 0.0998 - acc: 0.96 - ETA: 34:10 - loss: 0.0996 - acc: 0.96 - ETA: 34:05 - loss: 0.1000 - acc: 0.96 - ETA: 33:59 - loss: 0.0988 - acc: 0.96 - ETA: 33:53 - loss: 0.1005 - acc: 0.96 - ETA: 33:47 - loss: 0.1005 - acc: 0.96 - ETA: 33:41 - loss: 0.1007 - acc: 0.96 - ETA: 33:34 - loss: 0.1014 - acc: 0.96 - ETA: 33:29 - loss: 0.1015 - acc: 0.96 - ETA: 33:25 - loss: 0.1021 - acc: 0.96 - ETA: 33:19 - loss: 0.1012 - acc: 0.96 - ETA: 33:14 - loss: 0.1006 - acc: 0.96 - ETA: 33:08 - loss: 0.1001 - acc: 0.96 - ETA: 33:03 - loss: 0.0998 - acc: 0.96 - ETA: 32:58 - loss: 0.1003 - acc: 0.96 - ETA: 32:54 - loss: 0.0998 - acc: 0.96 - ETA: 32:49 - loss: 0.0990 - acc: 0.97 - ETA: 32:45 - loss: 0.0984 - acc: 0.97 - ETA: 32:40 - loss: 0.0979 - acc: 0.97 - ETA: 32:35 - loss: 0.0975 - acc: 0.97 - ETA: 32:31 - loss: 0.0976 - acc: 0.97 - ETA: 32:26 - loss: 0.0980 - acc: 0.96 - ETA: 32:21 - loss: 0.0975 - acc: 0.97 - ETA: 32:18 - loss: 0.0971 - acc: 0.97 - ETA: 32:14 - loss: 0.0970 - acc: 0.96 - ETA: 32:09 - loss: 0.0965 - acc: 0.97 - ETA: 32:05 - loss: 0.0959 - acc: 0.97 - ETA: 32:00 - loss: 0.0954 - acc: 0.97 - ETA: 31:55 - loss: 0.0951 - acc: 0.97 - ETA: 31:51 - loss: 0.0944 - acc: 0.97 - ETA: 31:47 - loss: 0.0937 - acc: 0.97 - ETA: 31:43 - loss: 0.0934 - acc: 0.97 - ETA: 31:39 - loss: 0.0928 - acc: 0.97 - ETA: 31:35 - loss: 0.0926 - acc: 0.97 - ETA: 31:31 - loss: 0.0922 - acc: 0.97 - ETA: 31:26 - loss: 0.0917 - acc: 0.97 - ETA: 31:22 - loss: 0.0918 - acc: 0.97 - ETA: 31:18 - loss: 0.0914 - acc: 0.97 - ETA: 31:14 - loss: 0.0923 - acc: 0.97 - ETA: 31:10 - loss: 0.0921 - acc: 0.97 - ETA: 31:06 - loss: 0.0923 - acc: 0.97 - ETA: 31:03 - loss: 0.0930 - acc: 0.97 - ETA: 30:59 - loss: 0.0923 - acc: 0.97 - ETA: 30:56 - loss: 0.0921 - acc: 0.97 - ETA: 30:52 - loss: 0.0919 - acc: 0.97 - ETA: 30:48 - loss: 0.0925 - acc: 0.97 - ETA: 30:45 - loss: 0.0926 - acc: 0.97 - ETA: 30:41 - loss: 0.0927 - acc: 0.97 - ETA: 30:37 - loss: 0.0922 - acc: 0.97 - ETA: 30:33 - loss: 0.0928 - acc: 0.97 - ETA: 30:30 - loss: 0.0931 - acc: 0.97 - ETA: 30:26 - loss: 0.0935 - acc: 0.97 - ETA: 30:23 - loss: 0.0936 - acc: 0.97 - ETA: 30:19 - loss: 0.0931 - acc: 0.97 - ETA: 30:16 - loss: 0.0931 - acc: 0.97 - ETA: 30:13 - loss: 0.0935 - acc: 0.96 - ETA: 30:09 - loss: 0.0932 - acc: 0.97 - ETA: 30:06 - loss: 0.0928 - acc: 0.97 - ETA: 30:02 - loss: 0.0934 - acc: 0.97 - ETA: 29:58 - loss: 0.0932 - acc: 0.97 - ETA: 29:54 - loss: 0.0928 - acc: 0.97 - ETA: 29:50 - loss: 0.0923 - acc: 0.97 - ETA: 29:46 - loss: 0.0923 - acc: 0.97 - ETA: 29:42 - loss: 0.0919 - acc: 0.97 - ETA: 29:38 - loss: 0.0926 - acc: 0.97 - ETA: 29:35 - loss: 0.0922 - acc: 0.97 - ETA: 29:31 - loss: 0.0920 - acc: 0.97 - ETA: 29:27 - loss: 0.0916 - acc: 0.97 - ETA: 29:24 - loss: 0.0915 - acc: 0.97 - ETA: 29:20 - loss: 0.0914 - acc: 0.97 - ETA: 29:16 - loss: 0.0914 - acc: 0.97 - ETA: 29:13 - loss: 0.0913 - acc: 0.97 - ETA: 29:10 - loss: 0.0911 - acc: 0.97 - ETA: 29:06 - loss: 0.0906 - acc: 0.97 - ETA: 29:03 - loss: 0.0905 - acc: 0.97 - ETA: 29:00 - loss: 0.0908 - acc: 0.97 - ETA: 28:56 - loss: 0.0910 - acc: 0.97 - ETA: 28:52 - loss: 0.0914 - acc: 0.97 - ETA: 28:49 - loss: 0.0910 - acc: 0.97 - ETA: 28:45 - loss: 0.0906 - acc: 0.97 - ETA: 28:42 - loss: 0.0902 - acc: 0.97 - ETA: 28:38 - loss: 0.0898 - acc: 0.97 - ETA: 28:35 - loss: 0.0899 - acc: 0.97 - ETA: 28:31 - loss: 0.0895 - acc: 0.97 - ETA: 28:28 - loss: 0.0892 - acc: 0.97 - ETA: 28:25 - loss: 0.0889 - acc: 0.97 - ETA: 28:22 - loss: 0.0887 - acc: 0.97 - ETA: 28:18 - loss: 0.0884 - acc: 0.97 - ETA: 28:15 - loss: 0.0891 - acc: 0.97 - ETA: 28:11 - loss: 0.0887 - acc: 0.97 - ETA: 28:08 - loss: 0.0885 - acc: 0.97 - ETA: 28:04 - loss: 0.0882 - acc: 0.97 - ETA: 28:01 - loss: 0.0879 - acc: 0.97 - ETA: 27:57 - loss: 0.0887 - acc: 0.97 - ETA: 27:54 - loss: 0.0883 - acc: 0.97 - ETA: 27:51 - loss: 0.0881 - acc: 0.97 - ETA: 27:47 - loss: 0.0878 - acc: 0.97 - ETA: 27:44 - loss: 0.0882 - acc: 0.97 - ETA: 27:41 - loss: 0.0884 - acc: 0.97 - ETA: 27:37 - loss: 0.0887 - acc: 0.97 - ETA: 27:34 - loss: 0.0885 - acc: 0.97 - ETA: 27:31 - loss: 0.0886 - acc: 0.97 - ETA: 27:28 - loss: 0.0883 - acc: 0.97 - ETA: 27:24 - loss: 0.0884 - acc: 0.97 - ETA: 27:21 - loss: 0.0888 - acc: 0.97 - ETA: 27:18 - loss: 0.0886 - acc: 0.97 - ETA: 27:15 - loss: 0.0888 - acc: 0.97 - ETA: 27:11 - loss: 0.0891 - acc: 0.97 - ETA: 27:08 - loss: 0.0887 - acc: 0.97 - ETA: 27:05 - loss: 0.0889 - acc: 0.97 - ETA: 27:02 - loss: 0.0885 - acc: 0.97 - ETA: 26:58 - loss: 0.0886 - acc: 0.97 - ETA: 26:55 - loss: 0.0885 - acc: 0.97 - ETA: 26:52 - loss: 0.0883 - acc: 0.97 - ETA: 26:49 - loss: 0.0881 - acc: 0.97 - ETA: 26:45 - loss: 0.0882 - acc: 0.97 - ETA: 26:43 - loss: 0.0880 - acc: 0.97 - ETA: 26:39 - loss: 0.0878 - acc: 0.97 - ETA: 26:36 - loss: 0.0882 - acc: 0.97 - ETA: 26:33 - loss: 0.0880 - acc: 0.97 - ETA: 26:30 - loss: 0.0880 - acc: 0.97 - ETA: 26:26 - loss: 0.0878 - acc: 0.97 - ETA: 26:23 - loss: 0.0881 - acc: 0.97 - ETA: 26:20 - loss: 0.0880 - acc: 0.97 - ETA: 26:17 - loss: 0.0883 - acc: 0.97 - ETA: 26:14 - loss: 0.0881 - acc: 0.97 - ETA: 26:10 - loss: 0.0878 - acc: 0.97 - ETA: 26:07 - loss: 0.0878 - acc: 0.97 - ETA: 26:04 - loss: 0.0875 - acc: 0.97 - ETA: 26:01 - loss: 0.0875 - acc: 0.97 - ETA: 25:58 - loss: 0.0872 - acc: 0.97 - ETA: 25:54 - loss: 0.0871 - acc: 0.97 - ETA: 25:51 - loss: 0.0869 - acc: 0.97 - ETA: 25:48 - loss: 0.0868 - acc: 0.97 - ETA: 25:45 - loss: 0.0867 - acc: 0.97 - ETA: 25:41 - loss: 0.0868 - acc: 0.97 - ETA: 25:38 - loss: 0.0865 - acc: 0.97 - ETA: 25:35 - loss: 0.0867 - acc: 0.97 - ETA: 25:32 - loss: 0.0864 - acc: 0.97 - ETA: 25:29 - loss: 0.0861 - acc: 0.97 - ETA: 25:26 - loss: 0.0864 - acc: 0.9717\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407/704 [================>.............] - ETA: 25:22 - loss: 0.0864 - acc: 0.97 - ETA: 25:19 - loss: 0.0864 - acc: 0.97 - ETA: 25:16 - loss: 0.0861 - acc: 0.97 - ETA: 25:13 - loss: 0.0858 - acc: 0.97 - ETA: 25:09 - loss: 0.0855 - acc: 0.97 - ETA: 25:06 - loss: 0.0857 - acc: 0.97 - ETA: 25:03 - loss: 0.0856 - acc: 0.97 - ETA: 25:00 - loss: 0.0853 - acc: 0.97 - ETA: 24:57 - loss: 0.0851 - acc: 0.97 - ETA: 24:54 - loss: 0.0849 - acc: 0.97 - ETA: 24:50 - loss: 0.0848 - acc: 0.97 - ETA: 24:47 - loss: 0.0847 - acc: 0.97 - ETA: 24:44 - loss: 0.0844 - acc: 0.97 - ETA: 24:41 - loss: 0.0842 - acc: 0.97 - ETA: 24:37 - loss: 0.0841 - acc: 0.97 - ETA: 24:34 - loss: 0.0843 - acc: 0.97 - ETA: 24:30 - loss: 0.0842 - acc: 0.97 - ETA: 24:27 - loss: 0.0842 - acc: 0.97 - ETA: 24:24 - loss: 0.0842 - acc: 0.97 - ETA: 24:21 - loss: 0.0841 - acc: 0.97 - ETA: 24:17 - loss: 0.0842 - acc: 0.97 - ETA: 24:14 - loss: 0.0840 - acc: 0.97 - ETA: 24:11 - loss: 0.0843 - acc: 0.97 - ETA: 24:08 - loss: 0.0840 - acc: 0.97 - ETA: 24:05 - loss: 0.0840 - acc: 0.97 - ETA: 24:01 - loss: 0.0841 - acc: 0.97 - ETA: 23:58 - loss: 0.0839 - acc: 0.97 - ETA: 23:55 - loss: 0.0837 - acc: 0.97 - ETA: 23:52 - loss: 0.0844 - acc: 0.97 - ETA: 23:49 - loss: 0.0842 - acc: 0.97 - ETA: 23:46 - loss: 0.0841 - acc: 0.97 - ETA: 23:42 - loss: 0.0840 - acc: 0.97 - ETA: 23:39 - loss: 0.0838 - acc: 0.97 - ETA: 23:36 - loss: 0.0836 - acc: 0.97 - ETA: 23:33 - loss: 0.0835 - acc: 0.97 - ETA: 23:30 - loss: 0.0832 - acc: 0.97 - ETA: 23:26 - loss: 0.0834 - acc: 0.97 - ETA: 23:24 - loss: 0.0832 - acc: 0.97 - ETA: 23:20 - loss: 0.0830 - acc: 0.97 - ETA: 23:17 - loss: 0.0830 - acc: 0.97 - ETA: 23:14 - loss: 0.0829 - acc: 0.97 - ETA: 23:11 - loss: 0.0827 - acc: 0.97 - ETA: 23:08 - loss: 0.0826 - acc: 0.97 - ETA: 23:04 - loss: 0.0823 - acc: 0.97 - ETA: 23:01 - loss: 0.0822 - acc: 0.97 - ETA: 22:58 - loss: 0.0820 - acc: 0.97 - ETA: 22:55 - loss: 0.0822 - acc: 0.97 - ETA: 22:51 - loss: 0.0824 - acc: 0.97 - ETA: 22:48 - loss: 0.0824 - acc: 0.97 - ETA: 22:45 - loss: 0.0832 - acc: 0.97 - ETA: 22:42 - loss: 0.0831 - acc: 0.97 - ETA: 22:39 - loss: 0.0829 - acc: 0.97 - ETA: 22:36 - loss: 0.0829 - acc: 0.97 - ETA: 22:32 - loss: 0.0831 - acc: 0.97 - ETA: 22:29 - loss: 0.0831 - acc: 0.97 - ETA: 22:26 - loss: 0.0829 - acc: 0.97 - ETA: 22:23 - loss: 0.0829 - acc: 0.97 - ETA: 22:20 - loss: 0.0831 - acc: 0.97 - ETA: 22:17 - loss: 0.0831 - acc: 0.97 - ETA: 22:14 - loss: 0.0832 - acc: 0.97 - ETA: 22:11 - loss: 0.0831 - acc: 0.97 - ETA: 22:08 - loss: 0.0831 - acc: 0.97 - ETA: 22:04 - loss: 0.0832 - acc: 0.97 - ETA: 22:01 - loss: 0.0832 - acc: 0.97 - ETA: 21:58 - loss: 0.0832 - acc: 0.97 - ETA: 21:55 - loss: 0.0832 - acc: 0.97 - ETA: 21:52 - loss: 0.0831 - acc: 0.97 - ETA: 21:48 - loss: 0.0831 - acc: 0.97 - ETA: 21:46 - loss: 0.0829 - acc: 0.97 - ETA: 21:42 - loss: 0.0829 - acc: 0.97 - ETA: 21:39 - loss: 0.0830 - acc: 0.97 - ETA: 21:36 - loss: 0.0829 - acc: 0.97 - ETA: 21:33 - loss: 0.0827 - acc: 0.97 - ETA: 21:30 - loss: 0.0826 - acc: 0.97 - ETA: 21:27 - loss: 0.0825 - acc: 0.97 - ETA: 21:23 - loss: 0.0826 - acc: 0.97 - ETA: 21:20 - loss: 0.0827 - acc: 0.97 - ETA: 21:17 - loss: 0.0826 - acc: 0.97 - ETA: 21:14 - loss: 0.0824 - acc: 0.97 - ETA: 21:11 - loss: 0.0823 - acc: 0.97 - ETA: 21:08 - loss: 0.0825 - acc: 0.97 - ETA: 21:05 - loss: 0.0824 - acc: 0.97 - ETA: 21:02 - loss: 0.0822 - acc: 0.97 - ETA: 20:59 - loss: 0.0823 - acc: 0.97 - ETA: 20:56 - loss: 0.0822 - acc: 0.97 - ETA: 20:53 - loss: 0.0820 - acc: 0.97 - ETA: 20:50 - loss: 0.0821 - acc: 0.97 - ETA: 20:46 - loss: 0.0821 - acc: 0.97 - ETA: 20:43 - loss: 0.0820 - acc: 0.97 - ETA: 20:40 - loss: 0.0818 - acc: 0.97 - ETA: 20:37 - loss: 0.0818 - acc: 0.97 - ETA: 20:34 - loss: 0.0816 - acc: 0.97 - ETA: 20:31 - loss: 0.0816 - acc: 0.97 - ETA: 20:28 - loss: 0.0815 - acc: 0.97 - ETA: 20:24 - loss: 0.0814 - acc: 0.97 - ETA: 20:21 - loss: 0.0814 - acc: 0.97 - ETA: 20:18 - loss: 0.0815 - acc: 0.97 - ETA: 20:15 - loss: 0.0815 - acc: 0.97 - ETA: 20:12 - loss: 0.0815 - acc: 0.97 - ETA: 20:09 - loss: 0.0818 - acc: 0.97 - ETA: 20:06 - loss: 0.0816 - acc: 0.97 - ETA: 20:03 - loss: 0.0817 - acc: 0.97 - ETA: 20:00 - loss: 0.0819 - acc: 0.97 - ETA: 19:57 - loss: 0.0818 - acc: 0.97 - ETA: 19:53 - loss: 0.0818 - acc: 0.97 - ETA: 19:50 - loss: 0.0816 - acc: 0.97 - ETA: 19:47 - loss: 0.0816 - acc: 0.97 - ETA: 19:44 - loss: 0.0815 - acc: 0.97 - ETA: 19:41 - loss: 0.0814 - acc: 0.97 - ETA: 19:38 - loss: 0.0812 - acc: 0.97 - ETA: 19:35 - loss: 0.0812 - acc: 0.97 - ETA: 19:32 - loss: 0.0816 - acc: 0.97 - ETA: 19:29 - loss: 0.0816 - acc: 0.97 - ETA: 19:25 - loss: 0.0814 - acc: 0.97 - ETA: 19:22 - loss: 0.0814 - acc: 0.97 - ETA: 19:19 - loss: 0.0816 - acc: 0.97 - ETA: 19:16 - loss: 0.0818 - acc: 0.97 - ETA: 19:13 - loss: 0.0820 - acc: 0.97 - ETA: 19:10 - loss: 0.0819 - acc: 0.97 - ETA: 19:06 - loss: 0.0818 - acc: 0.97 - ETA: 19:03 - loss: 0.0816 - acc: 0.97 - ETA: 19:00 - loss: 0.0815 - acc: 0.97 - ETA: 18:57 - loss: 0.0817 - acc: 0.97 - ETA: 18:54 - loss: 0.0815 - acc: 0.97 - ETA: 18:51 - loss: 0.0813 - acc: 0.97 - ETA: 18:48 - loss: 0.0811 - acc: 0.97 - ETA: 18:45 - loss: 0.0810 - acc: 0.97 - ETA: 18:42 - loss: 0.0808 - acc: 0.97 - ETA: 18:39 - loss: 0.0808 - acc: 0.97 - ETA: 18:36 - loss: 0.0807 - acc: 0.97 - ETA: 18:33 - loss: 0.0806 - acc: 0.97 - ETA: 18:30 - loss: 0.0807 - acc: 0.97 - ETA: 18:27 - loss: 0.0805 - acc: 0.97 - ETA: 18:24 - loss: 0.0805 - acc: 0.97 - ETA: 18:21 - loss: 0.0803 - acc: 0.97 - ETA: 18:18 - loss: 0.0803 - acc: 0.97 - ETA: 18:15 - loss: 0.0806 - acc: 0.97 - ETA: 18:12 - loss: 0.0806 - acc: 0.97 - ETA: 18:09 - loss: 0.0808 - acc: 0.97 - ETA: 18:06 - loss: 0.0808 - acc: 0.97 - ETA: 18:03 - loss: 0.0807 - acc: 0.97 - ETA: 18:00 - loss: 0.0809 - acc: 0.97 - ETA: 17:57 - loss: 0.0810 - acc: 0.97 - ETA: 17:54 - loss: 0.0809 - acc: 0.97 - ETA: 17:51 - loss: 0.0810 - acc: 0.97 - ETA: 17:48 - loss: 0.0809 - acc: 0.97 - ETA: 17:45 - loss: 0.0808 - acc: 0.97 - ETA: 17:42 - loss: 0.0807 - acc: 0.97 - ETA: 17:39 - loss: 0.0805 - acc: 0.97 - ETA: 17:36 - loss: 0.0804 - acc: 0.97 - ETA: 17:33 - loss: 0.0802 - acc: 0.97 - ETA: 17:30 - loss: 0.0801 - acc: 0.97 - ETA: 17:27 - loss: 0.0801 - acc: 0.97 - ETA: 17:24 - loss: 0.0800 - acc: 0.97 - ETA: 17:21 - loss: 0.0799 - acc: 0.97 - ETA: 17:18 - loss: 0.0800 - acc: 0.97 - ETA: 17:15 - loss: 0.0800 - acc: 0.97 - ETA: 17:12 - loss: 0.0799 - acc: 0.97 - ETA: 17:09 - loss: 0.0800 - acc: 0.97 - ETA: 17:06 - loss: 0.0800 - acc: 0.97 - ETA: 17:03 - loss: 0.0799 - acc: 0.97 - ETA: 17:00 - loss: 0.0801 - acc: 0.97 - ETA: 16:57 - loss: 0.0802 - acc: 0.97 - ETA: 16:54 - loss: 0.0802 - acc: 0.97 - ETA: 16:51 - loss: 0.0801 - acc: 0.97 - ETA: 16:48 - loss: 0.0804 - acc: 0.97 - ETA: 16:45 - loss: 0.0804 - acc: 0.97 - ETA: 16:41 - loss: 0.0803 - acc: 0.97 - ETA: 16:38 - loss: 0.0804 - acc: 0.97 - ETA: 16:35 - loss: 0.0805 - acc: 0.97 - ETA: 16:32 - loss: 0.0803 - acc: 0.97 - ETA: 16:29 - loss: 0.0807 - acc: 0.97 - ETA: 16:26 - loss: 0.0806 - acc: 0.97 - ETA: 16:23 - loss: 0.0805 - acc: 0.97 - ETA: 16:20 - loss: 0.0804 - acc: 0.97 - ETA: 16:17 - loss: 0.0808 - acc: 0.97 - ETA: 16:14 - loss: 0.0808 - acc: 0.97 - ETA: 16:11 - loss: 0.0808 - acc: 0.97 - ETA: 16:08 - loss: 0.0809 - acc: 0.97 - ETA: 16:05 - loss: 0.0807 - acc: 0.97 - ETA: 16:02 - loss: 0.0809 - acc: 0.97 - ETA: 15:59 - loss: 0.0808 - acc: 0.97 - ETA: 15:56 - loss: 0.0806 - acc: 0.97 - ETA: 15:53 - loss: 0.0805 - acc: 0.97 - ETA: 15:50 - loss: 0.0804 - acc: 0.97 - ETA: 15:47 - loss: 0.0803 - acc: 0.97 - ETA: 15:44 - loss: 0.0803 - acc: 0.97 - ETA: 15:41 - loss: 0.0803 - acc: 0.97 - ETA: 15:38 - loss: 0.0802 - acc: 0.97 - ETA: 15:35 - loss: 0.0802 - acc: 0.97 - ETA: 15:32 - loss: 0.0801 - acc: 0.97 - ETA: 15:29 - loss: 0.0802 - acc: 0.97 - ETA: 15:26 - loss: 0.0801 - acc: 0.97 - ETA: 15:23 - loss: 0.0800 - acc: 0.97 - ETA: 15:20 - loss: 0.0800 - acc: 0.97 - ETA: 15:17 - loss: 0.0799 - acc: 0.97 - ETA: 15:14 - loss: 0.0798 - acc: 0.97 - ETA: 15:11 - loss: 0.0797 - acc: 0.97 - ETA: 15:08 - loss: 0.0797 - acc: 0.97 - ETA: 15:05 - loss: 0.0796 - acc: 0.97 - ETA: 15:02 - loss: 0.0797 - acc: 0.97 - ETA: 14:59 - loss: 0.0796 - acc: 0.97 - ETA: 14:56 - loss: 0.0795 - acc: 0.97 - ETA: 14:53 - loss: 0.0796 - acc: 0.9748"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611/704 [=========================>....] - ETA: 14:50 - loss: 0.0796 - acc: 0.97 - ETA: 14:47 - loss: 0.0795 - acc: 0.97 - ETA: 14:44 - loss: 0.0795 - acc: 0.97 - ETA: 14:41 - loss: 0.0794 - acc: 0.97 - ETA: 14:38 - loss: 0.0793 - acc: 0.97 - ETA: 14:35 - loss: 0.0793 - acc: 0.97 - ETA: 14:32 - loss: 0.0793 - acc: 0.97 - ETA: 14:29 - loss: 0.0791 - acc: 0.97 - ETA: 14:26 - loss: 0.0794 - acc: 0.97 - ETA: 14:23 - loss: 0.0793 - acc: 0.97 - ETA: 14:20 - loss: 0.0793 - acc: 0.97 - ETA: 14:17 - loss: 0.0793 - acc: 0.97 - ETA: 14:14 - loss: 0.0793 - acc: 0.97 - ETA: 14:11 - loss: 0.0793 - acc: 0.97 - ETA: 14:08 - loss: 0.0794 - acc: 0.97 - ETA: 14:05 - loss: 0.0793 - acc: 0.97 - ETA: 14:02 - loss: 0.0791 - acc: 0.97 - ETA: 13:59 - loss: 0.0790 - acc: 0.97 - ETA: 13:55 - loss: 0.0790 - acc: 0.97 - ETA: 13:53 - loss: 0.0789 - acc: 0.97 - ETA: 13:50 - loss: 0.0789 - acc: 0.97 - ETA: 13:47 - loss: 0.0788 - acc: 0.97 - ETA: 13:44 - loss: 0.0789 - acc: 0.97 - ETA: 13:40 - loss: 0.0788 - acc: 0.97 - ETA: 13:37 - loss: 0.0787 - acc: 0.97 - ETA: 13:34 - loss: 0.0787 - acc: 0.97 - ETA: 13:31 - loss: 0.0786 - acc: 0.97 - ETA: 13:28 - loss: 0.0786 - acc: 0.97 - ETA: 13:25 - loss: 0.0785 - acc: 0.97 - ETA: 13:22 - loss: 0.0786 - acc: 0.97 - ETA: 13:19 - loss: 0.0786 - acc: 0.97 - ETA: 13:16 - loss: 0.0787 - acc: 0.97 - ETA: 13:13 - loss: 0.0787 - acc: 0.97 - ETA: 13:10 - loss: 0.0786 - acc: 0.97 - ETA: 13:07 - loss: 0.0786 - acc: 0.97 - ETA: 13:04 - loss: 0.0785 - acc: 0.97 - ETA: 13:01 - loss: 0.0786 - acc: 0.97 - ETA: 12:58 - loss: 0.0784 - acc: 0.97 - ETA: 12:55 - loss: 0.0785 - acc: 0.97 - ETA: 12:52 - loss: 0.0784 - acc: 0.97 - ETA: 12:49 - loss: 0.0784 - acc: 0.97 - ETA: 12:46 - loss: 0.0783 - acc: 0.97 - ETA: 12:43 - loss: 0.0782 - acc: 0.97 - ETA: 12:40 - loss: 0.0781 - acc: 0.97 - ETA: 12:37 - loss: 0.0785 - acc: 0.97 - ETA: 12:34 - loss: 0.0784 - acc: 0.97 - ETA: 12:31 - loss: 0.0783 - acc: 0.97 - ETA: 12:28 - loss: 0.0786 - acc: 0.97 - ETA: 12:25 - loss: 0.0785 - acc: 0.97 - ETA: 12:22 - loss: 0.0785 - acc: 0.97 - ETA: 12:19 - loss: 0.0784 - acc: 0.97 - ETA: 12:16 - loss: 0.0785 - acc: 0.97 - ETA: 12:13 - loss: 0.0784 - acc: 0.97 - ETA: 12:10 - loss: 0.0784 - acc: 0.97 - ETA: 12:07 - loss: 0.0784 - acc: 0.97 - ETA: 12:04 - loss: 0.0784 - acc: 0.97 - ETA: 12:01 - loss: 0.0783 - acc: 0.97 - ETA: 11:58 - loss: 0.0782 - acc: 0.97 - ETA: 11:55 - loss: 0.0783 - acc: 0.97 - ETA: 11:52 - loss: 0.0783 - acc: 0.97 - ETA: 11:49 - loss: 0.0782 - acc: 0.97 - ETA: 11:45 - loss: 0.0784 - acc: 0.97 - ETA: 11:42 - loss: 0.0783 - acc: 0.97 - ETA: 11:39 - loss: 0.0782 - acc: 0.97 - ETA: 11:36 - loss: 0.0782 - acc: 0.97 - ETA: 11:33 - loss: 0.0781 - acc: 0.97 - ETA: 11:30 - loss: 0.0782 - acc: 0.97 - ETA: 11:27 - loss: 0.0781 - acc: 0.97 - ETA: 11:24 - loss: 0.0780 - acc: 0.97 - ETA: 11:21 - loss: 0.0779 - acc: 0.97 - ETA: 11:18 - loss: 0.0778 - acc: 0.97 - ETA: 11:15 - loss: 0.0778 - acc: 0.97 - ETA: 11:12 - loss: 0.0777 - acc: 0.97 - ETA: 11:09 - loss: 0.0776 - acc: 0.97 - ETA: 11:06 - loss: 0.0775 - acc: 0.97 - ETA: 11:03 - loss: 0.0775 - acc: 0.97 - ETA: 11:00 - loss: 0.0773 - acc: 0.97 - ETA: 10:57 - loss: 0.0772 - acc: 0.97 - ETA: 10:54 - loss: 0.0772 - acc: 0.97 - ETA: 10:51 - loss: 0.0771 - acc: 0.97 - ETA: 10:48 - loss: 0.0770 - acc: 0.97 - ETA: 10:45 - loss: 0.0771 - acc: 0.97 - ETA: 10:42 - loss: 0.0770 - acc: 0.97 - ETA: 10:39 - loss: 0.0769 - acc: 0.97 - ETA: 10:36 - loss: 0.0769 - acc: 0.97 - ETA: 10:33 - loss: 0.0768 - acc: 0.97 - ETA: 10:30 - loss: 0.0767 - acc: 0.97 - ETA: 10:27 - loss: 0.0767 - acc: 0.97 - ETA: 10:24 - loss: 0.0766 - acc: 0.97 - ETA: 10:21 - loss: 0.0769 - acc: 0.97 - ETA: 10:18 - loss: 0.0769 - acc: 0.97 - ETA: 10:15 - loss: 0.0769 - acc: 0.97 - ETA: 10:12 - loss: 0.0770 - acc: 0.97 - ETA: 10:09 - loss: 0.0770 - acc: 0.97 - ETA: 10:06 - loss: 0.0769 - acc: 0.97 - ETA: 10:03 - loss: 0.0768 - acc: 0.97 - ETA: 10:00 - loss: 0.0767 - acc: 0.97 - ETA: 9:57 - loss: 0.0769 - acc: 0.9756 - ETA: 9:54 - loss: 0.0768 - acc: 0.975 - ETA: 9:51 - loss: 0.0767 - acc: 0.975 - ETA: 9:48 - loss: 0.0766 - acc: 0.975 - ETA: 9:45 - loss: 0.0766 - acc: 0.975 - ETA: 9:42 - loss: 0.0765 - acc: 0.975 - ETA: 9:39 - loss: 0.0765 - acc: 0.975 - ETA: 9:36 - loss: 0.0765 - acc: 0.975 - ETA: 9:33 - loss: 0.0764 - acc: 0.975 - ETA: 9:30 - loss: 0.0764 - acc: 0.975 - ETA: 9:27 - loss: 0.0763 - acc: 0.975 - ETA: 9:24 - loss: 0.0763 - acc: 0.975 - ETA: 9:21 - loss: 0.0763 - acc: 0.975 - ETA: 9:18 - loss: 0.0764 - acc: 0.975 - ETA: 9:15 - loss: 0.0763 - acc: 0.975 - ETA: 9:12 - loss: 0.0763 - acc: 0.975 - ETA: 9:09 - loss: 0.0765 - acc: 0.975 - ETA: 9:06 - loss: 0.0764 - acc: 0.975 - ETA: 9:03 - loss: 0.0764 - acc: 0.975 - ETA: 9:00 - loss: 0.0764 - acc: 0.975 - ETA: 8:57 - loss: 0.0765 - acc: 0.975 - ETA: 8:54 - loss: 0.0768 - acc: 0.975 - ETA: 8:51 - loss: 0.0767 - acc: 0.975 - ETA: 8:47 - loss: 0.0767 - acc: 0.975 - ETA: 8:45 - loss: 0.0767 - acc: 0.975 - ETA: 8:42 - loss: 0.0766 - acc: 0.975 - ETA: 8:39 - loss: 0.0767 - acc: 0.975 - ETA: 8:36 - loss: 0.0766 - acc: 0.975 - ETA: 8:33 - loss: 0.0765 - acc: 0.975 - ETA: 8:29 - loss: 0.0765 - acc: 0.975 - ETA: 8:26 - loss: 0.0765 - acc: 0.975 - ETA: 8:23 - loss: 0.0766 - acc: 0.975 - ETA: 8:20 - loss: 0.0765 - acc: 0.975 - ETA: 8:17 - loss: 0.0765 - acc: 0.975 - ETA: 8:14 - loss: 0.0765 - acc: 0.975 - ETA: 8:11 - loss: 0.0765 - acc: 0.975 - ETA: 8:08 - loss: 0.0765 - acc: 0.975 - ETA: 8:05 - loss: 0.0765 - acc: 0.975 - ETA: 8:02 - loss: 0.0766 - acc: 0.975 - ETA: 7:59 - loss: 0.0766 - acc: 0.975 - ETA: 7:56 - loss: 0.0766 - acc: 0.975 - ETA: 7:53 - loss: 0.0765 - acc: 0.975 - ETA: 7:50 - loss: 0.0764 - acc: 0.975 - ETA: 7:47 - loss: 0.0766 - acc: 0.975 - ETA: 7:44 - loss: 0.0765 - acc: 0.975 - ETA: 7:41 - loss: 0.0765 - acc: 0.975 - ETA: 7:38 - loss: 0.0765 - acc: 0.975 - ETA: 7:35 - loss: 0.0765 - acc: 0.975 - ETA: 7:32 - loss: 0.0765 - acc: 0.975 - ETA: 7:29 - loss: 0.0764 - acc: 0.975 - ETA: 7:26 - loss: 0.0763 - acc: 0.976 - ETA: 7:23 - loss: 0.0763 - acc: 0.976 - ETA: 7:21 - loss: 0.0763 - acc: 0.975 - ETA: 7:18 - loss: 0.0762 - acc: 0.976 - ETA: 7:15 - loss: 0.0762 - acc: 0.976 - ETA: 7:11 - loss: 0.0761 - acc: 0.976 - ETA: 7:08 - loss: 0.0760 - acc: 0.976 - ETA: 7:05 - loss: 0.0759 - acc: 0.976 - ETA: 7:02 - loss: 0.0759 - acc: 0.976 - ETA: 6:59 - loss: 0.0758 - acc: 0.976 - ETA: 6:56 - loss: 0.0759 - acc: 0.976 - ETA: 6:53 - loss: 0.0758 - acc: 0.976 - ETA: 6:50 - loss: 0.0757 - acc: 0.976 - ETA: 6:47 - loss: 0.0757 - acc: 0.976 - ETA: 6:44 - loss: 0.0758 - acc: 0.976 - ETA: 6:41 - loss: 0.0757 - acc: 0.976 - ETA: 6:38 - loss: 0.0758 - acc: 0.976 - ETA: 6:35 - loss: 0.0759 - acc: 0.976 - ETA: 6:32 - loss: 0.0759 - acc: 0.976 - ETA: 6:29 - loss: 0.0758 - acc: 0.976 - ETA: 6:26 - loss: 0.0758 - acc: 0.976 - ETA: 6:23 - loss: 0.0757 - acc: 0.976 - ETA: 6:20 - loss: 0.0758 - acc: 0.976 - ETA: 6:17 - loss: 0.0757 - acc: 0.976 - ETA: 6:14 - loss: 0.0756 - acc: 0.976 - ETA: 6:12 - loss: 0.0756 - acc: 0.976 - ETA: 6:09 - loss: 0.0756 - acc: 0.976 - ETA: 6:06 - loss: 0.0755 - acc: 0.976 - ETA: 6:03 - loss: 0.0755 - acc: 0.976 - ETA: 6:00 - loss: 0.0754 - acc: 0.976 - ETA: 5:57 - loss: 0.0755 - acc: 0.976 - ETA: 5:54 - loss: 0.0754 - acc: 0.976 - ETA: 5:51 - loss: 0.0755 - acc: 0.976 - ETA: 5:48 - loss: 0.0754 - acc: 0.976 - ETA: 5:45 - loss: 0.0753 - acc: 0.976 - ETA: 5:42 - loss: 0.0752 - acc: 0.976 - ETA: 5:39 - loss: 0.0752 - acc: 0.976 - ETA: 5:36 - loss: 0.0752 - acc: 0.976 - ETA: 5:33 - loss: 0.0751 - acc: 0.976 - ETA: 5:30 - loss: 0.0750 - acc: 0.976 - ETA: 5:27 - loss: 0.0749 - acc: 0.976 - ETA: 5:24 - loss: 0.0749 - acc: 0.976 - ETA: 5:21 - loss: 0.0749 - acc: 0.976 - ETA: 5:18 - loss: 0.0750 - acc: 0.976 - ETA: 5:15 - loss: 0.0750 - acc: 0.976 - ETA: 5:12 - loss: 0.0749 - acc: 0.976 - ETA: 5:09 - loss: 0.0750 - acc: 0.976 - ETA: 5:06 - loss: 0.0749 - acc: 0.976 - ETA: 5:03 - loss: 0.0748 - acc: 0.976 - ETA: 5:00 - loss: 0.0747 - acc: 0.976 - ETA: 4:57 - loss: 0.0750 - acc: 0.976 - ETA: 4:54 - loss: 0.0749 - acc: 0.976 - ETA: 4:51 - loss: 0.0751 - acc: 0.976 - ETA: 4:48 - loss: 0.0750 - acc: 0.976 - ETA: 4:45 - loss: 0.0749 - acc: 0.976 - ETA: 4:42 - loss: 0.0749 - acc: 0.976 - ETA: 4:39 - loss: 0.0749 - acc: 0.9765"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704/704 [==============================] - ETA: 4:36 - loss: 0.0748 - acc: 0.976 - ETA: 4:33 - loss: 0.0749 - acc: 0.976 - ETA: 4:30 - loss: 0.0748 - acc: 0.976 - ETA: 4:27 - loss: 0.0747 - acc: 0.976 - ETA: 4:24 - loss: 0.0747 - acc: 0.976 - ETA: 4:21 - loss: 0.0747 - acc: 0.976 - ETA: 4:18 - loss: 0.0747 - acc: 0.976 - ETA: 4:15 - loss: 0.0746 - acc: 0.976 - ETA: 4:12 - loss: 0.0747 - acc: 0.976 - ETA: 4:09 - loss: 0.0747 - acc: 0.976 - ETA: 4:06 - loss: 0.0749 - acc: 0.976 - ETA: 4:03 - loss: 0.0749 - acc: 0.976 - ETA: 4:00 - loss: 0.0748 - acc: 0.976 - ETA: 3:57 - loss: 0.0748 - acc: 0.976 - ETA: 3:54 - loss: 0.0748 - acc: 0.976 - ETA: 3:51 - loss: 0.0748 - acc: 0.976 - ETA: 3:48 - loss: 0.0747 - acc: 0.976 - ETA: 3:45 - loss: 0.0749 - acc: 0.976 - ETA: 3:42 - loss: 0.0750 - acc: 0.976 - ETA: 3:39 - loss: 0.0751 - acc: 0.976 - ETA: 3:36 - loss: 0.0751 - acc: 0.976 - ETA: 3:33 - loss: 0.0751 - acc: 0.976 - ETA: 3:30 - loss: 0.0753 - acc: 0.976 - ETA: 3:27 - loss: 0.0753 - acc: 0.976 - ETA: 3:24 - loss: 0.0752 - acc: 0.976 - ETA: 3:21 - loss: 0.0756 - acc: 0.976 - ETA: 3:18 - loss: 0.0755 - acc: 0.976 - ETA: 3:15 - loss: 0.0755 - acc: 0.976 - ETA: 3:12 - loss: 0.0755 - acc: 0.976 - ETA: 3:09 - loss: 0.0755 - acc: 0.976 - ETA: 3:06 - loss: 0.0755 - acc: 0.976 - ETA: 3:03 - loss: 0.0754 - acc: 0.976 - ETA: 3:00 - loss: 0.0754 - acc: 0.976 - ETA: 2:57 - loss: 0.0754 - acc: 0.976 - ETA: 2:54 - loss: 0.0753 - acc: 0.976 - ETA: 2:51 - loss: 0.0752 - acc: 0.976 - ETA: 2:48 - loss: 0.0752 - acc: 0.976 - ETA: 2:45 - loss: 0.0751 - acc: 0.976 - ETA: 2:42 - loss: 0.0751 - acc: 0.976 - ETA: 2:39 - loss: 0.0751 - acc: 0.976 - ETA: 2:36 - loss: 0.0750 - acc: 0.976 - ETA: 2:33 - loss: 0.0750 - acc: 0.976 - ETA: 2:30 - loss: 0.0750 - acc: 0.976 - ETA: 2:27 - loss: 0.0749 - acc: 0.976 - ETA: 2:24 - loss: 0.0748 - acc: 0.976 - ETA: 2:21 - loss: 0.0747 - acc: 0.976 - ETA: 2:18 - loss: 0.0746 - acc: 0.976 - ETA: 2:15 - loss: 0.0747 - acc: 0.976 - ETA: 2:12 - loss: 0.0748 - acc: 0.976 - ETA: 2:09 - loss: 0.0748 - acc: 0.976 - ETA: 2:06 - loss: 0.0747 - acc: 0.976 - ETA: 2:03 - loss: 0.0748 - acc: 0.976 - ETA: 2:00 - loss: 0.0747 - acc: 0.976 - ETA: 1:57 - loss: 0.0747 - acc: 0.976 - ETA: 1:54 - loss: 0.0746 - acc: 0.976 - ETA: 1:51 - loss: 0.0747 - acc: 0.976 - ETA: 1:48 - loss: 0.0749 - acc: 0.976 - ETA: 1:45 - loss: 0.0749 - acc: 0.976 - ETA: 1:42 - loss: 0.0748 - acc: 0.976 - ETA: 1:39 - loss: 0.0748 - acc: 0.976 - ETA: 1:36 - loss: 0.0749 - acc: 0.976 - ETA: 1:33 - loss: 0.0748 - acc: 0.976 - ETA: 1:30 - loss: 0.0748 - acc: 0.976 - ETA: 1:27 - loss: 0.0749 - acc: 0.976 - ETA: 1:24 - loss: 0.0750 - acc: 0.976 - ETA: 1:21 - loss: 0.0750 - acc: 0.976 - ETA: 1:18 - loss: 0.0749 - acc: 0.976 - ETA: 1:15 - loss: 0.0749 - acc: 0.976 - ETA: 1:12 - loss: 0.0749 - acc: 0.976 - ETA: 1:09 - loss: 0.0749 - acc: 0.976 - ETA: 1:06 - loss: 0.0750 - acc: 0.976 - ETA: 1:03 - loss: 0.0750 - acc: 0.976 - ETA: 1:00 - loss: 0.0749 - acc: 0.976 - ETA: 57s - loss: 0.0750 - acc: 0.976 - ETA: 54s - loss: 0.0749 - acc: 0.97 - ETA: 51s - loss: 0.0749 - acc: 0.97 - ETA: 48s - loss: 0.0748 - acc: 0.97 - ETA: 45s - loss: 0.0748 - acc: 0.97 - ETA: 42s - loss: 0.0747 - acc: 0.97 - ETA: 39s - loss: 0.0747 - acc: 0.97 - ETA: 36s - loss: 0.0747 - acc: 0.97 - ETA: 33s - loss: 0.0746 - acc: 0.97 - ETA: 30s - loss: 0.0746 - acc: 0.97 - ETA: 27s - loss: 0.0746 - acc: 0.97 - ETA: 24s - loss: 0.0746 - acc: 0.97 - ETA: 21s - loss: 0.0745 - acc: 0.97 - ETA: 18s - loss: 0.0745 - acc: 0.97 - ETA: 15s - loss: 0.0744 - acc: 0.97 - ETA: 12s - loss: 0.0744 - acc: 0.97 - ETA: 9s - loss: 0.0744 - acc: 0.9771 - ETA: 6s - loss: 0.0743 - acc: 0.977 - ETA: 3s - loss: 0.0742 - acc: 0.977 - 2325s 3s/step - loss: 0.0742 - acc: 0.9771 - val_loss: 0.0663 - val_acc: 0.9801\n",
      "Model training time: 38.8 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:86: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61446\n",
      "160136\n",
      "Epoch 1/1\n",
      "  3/704 [..............................] - ETA: 1:07:01 - loss: 0.0391 - acc: 0.99 - ETA: 1:00:06 - loss: 0.0515 - acc: 0.98 - ETA: 57:19 - loss: 0.0514 - acc: 0.9840  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-4673a1cb6631>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_samples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         epochs=N_EPOCHS)\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfile_handles\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfh\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile_handles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m                 \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    683\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m                 \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\tf-gpu\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 638\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\tf-gpu\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\tf-gpu\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\tf-gpu\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "columns = ['is_tissue','slide_path','is_tumor','is_all_tumor','tile_loc']\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 1\n",
    "\n",
    "for i in range(len(slide_4_test)):\n",
    "    \n",
    "    # [1] dataset , 2 pos, 2 neg, mean ratio = 3:1\n",
    "    four_samples = pd.DataFrame(columns = columns)\n",
    "    four_image_path = list()\n",
    "    four_mask_path = list()    \n",
    "    for j in range(4):\n",
    "        image_path = image_paths[slide_4_test[i][j]][1:] # 이 부분은 data 읽을때 고치자 ( [1:] 빼야함)\n",
    "        mask_path = tumor_mask_paths[slide_4_test[i][j]][1:] # 이 부분은 data 읽을때 고치자\n",
    "        samples = find_patches_from_slide(image_path, mask_path)\n",
    "        \n",
    "        four_samples = four_samples.append(samples)   \n",
    "        four_image_path.append(image_path)\n",
    "        four_mask_path.append(mask_path)\n",
    "    NUM_SAMPLES = len(four_samples)\n",
    "    if NUM_SAMPLES > 50000:\n",
    "        NUM_SAMPLES = 50000\n",
    "    \n",
    "    samples = four_samples.sample(NUM_SAMPLES, random_state=42)\n",
    "    samples.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    tumor_samples = four_samples[four_samples.is_tumor == True]\n",
    "    print(len(tumor_samples))\n",
    "    non_tumor_samples = four_samples[four_samples.is_tumor == False]\n",
    "    print(len(non_tumor_samples))\n",
    "    non_tumor_samples_3_ratio = non_tumor_samples.sample(len(tumor_samples) * 3, random_state = 42,replace=True)\n",
    "    \n",
    "    all_sample = tumor_samples.append(non_tumor_samples_3_ratio)\n",
    "\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "    for train_index, test_index in split.split(samples, samples[\"is_tumor\"]):\n",
    "            train_samples = samples.loc[train_index]\n",
    "            validation_samples = samples.loc[test_index]\n",
    "    \n",
    "    train_generator = gen_imgs(four_image_path,four_mask_path,train_samples, BATCH_SIZE)\n",
    "    validation_generator = gen_imgs(four_image_path,four_mask_path,validation_samples, BATCH_SIZE)\n",
    "    \n",
    "    train_start_time = datetime.now()\n",
    "    history = model.fit_generator(train_generator, np.ceil(len(train_samples) / BATCH_SIZE),\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=np.ceil(len(validation_samples) / BATCH_SIZE),\n",
    "        epochs=N_EPOCHS)\n",
    "    if file_handles != []:\n",
    "        for fh in file_handles:\n",
    "            fh.close()\n",
    "    file_handles=[]\n",
    "    #del train_generator\n",
    "    #del validation_generator\n",
    "    train_end_time = datetime.now()\n",
    "    print(\"Model training time: %.1f minutes\" % ((train_end_time - train_start_time).seconds / 60,))\n",
    "    model.save('i_1.h5')\n",
    "    # split\n",
    "    # data gen : all_image_path, all_mask_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('i_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches in slide: 105213\n",
      "Wall time: 4min 44s\n",
      "5000 gen time: 4.7 minutes\n",
      "Model test time: 3.1 minutes\n",
      "0.99646385332144\n"
     ]
    }
   ],
   "source": [
    "ipath = all_image_path[0]\n",
    "tpath = all_mask_path[0]\n",
    "\n",
    "all_tissue_samples = find_patches_from_slide(ipath,tpath)\n",
    "print('Total patches in slide: %d' % len(all_tissue_samples)) \n",
    "all_tissue_samples.iloc[:10]\n",
    "all_tissue_samples.is_tumor.value_counts() \n",
    "test_start_time = datetime.now()\n",
    "sample_gen = gen_imgs(all_image_path,all_mask_path,all_tissue_samples, 5000, shuffle=True)\n",
    "%time example_X, example_y  = next(sample_gen)\n",
    "test_end_time = datetime.now()\n",
    "print(\"5000 gen time: %.1f minutes\" % ((test_end_time - test_start_time).seconds / 60,))\n",
    "\n",
    "start_x = PATCH_SIZE//4\n",
    "start_y = PATCH_SIZE//4\n",
    "pred_size = PATCH_SIZE//2\n",
    "\n",
    "test_start_time = datetime.now()\n",
    "preds = []\n",
    "labels = []\n",
    "for i in range(5000):\n",
    "    prediction = predict_from_model(example_X[i],model)\n",
    "    pred_X = np.zeros((pred_size,pred_size))\n",
    "    y = example_y[i].argmax\n",
    "    for x in range(start_x,start_x+pred_size):\n",
    "        for y in range(start_y, start_y+pred_size):\n",
    "            pred_X[x-start_x][y-start_y] = prediction[x][y]\n",
    "            \n",
    "    pred_s = pd.Series(pred_X.flatten())\n",
    "    max_p = np.max(pred_s)\n",
    "    \n",
    "    y = np.max(example_y[i].argmax(axis=2))\n",
    "    preds.append(max_p)\n",
    "    labels.append(y)\n",
    "test_end_time = datetime.now()\n",
    "print(\"Model test time: %.1f minutes\" % ((test_end_time - test_start_time).seconds / 60,))    \n",
    "fpr, tpr, thresholds = metrics.roc_curve(labels,preds,pos_label=1)\n",
    "print(metrics.auc(fpr,tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches in slide: 105213\n",
      "Wall time: 5min 10s\n",
      "5000 gen time: 5.2 minutes\n",
      "Model test time: 1.8 minutes\n",
      "0.9954325313632394\n"
     ]
    }
   ],
   "source": [
    "ipath = all_image_path[0]\n",
    "tpath = all_mask_path[0]\n",
    "\n",
    "all_tissue_samples = find_patches_from_slide(ipath,tpath)\n",
    "print('Total patches in slide: %d' % len(all_tissue_samples)) \n",
    "all_tissue_samples.iloc[:10]\n",
    "all_tissue_samples.is_tumor.value_counts() \n",
    "test_start_time = datetime.now()\n",
    "sample_gen = gen_imgs(all_image_path,all_mask_path,all_tissue_samples, 5000, shuffle=True)\n",
    "%time example_X, example_y  = next(sample_gen)\n",
    "test_end_time = datetime.now()\n",
    "print(\"5000 gen time: %.1f minutes\" % ((test_end_time - test_start_time).seconds / 60,))\n",
    "\n",
    "start_x = PATCH_SIZE//4\n",
    "start_y = PATCH_SIZE//4\n",
    "pred_size = PATCH_SIZE//2\n",
    "\n",
    "test_start_time = datetime.now()\n",
    "preds = []\n",
    "labels = []\n",
    "for i in range(5000):\n",
    "    prediction = predict_from_model(example_X[i],model)\n",
    "    pred_X = np.zeros((pred_size,pred_size))\n",
    "    y = example_y[i].argmax\n",
    "    pred_s = pd.Series(prediction.flatten())\n",
    "    max_p = np.max(pred_s)\n",
    "    \n",
    "    y = np.max(example_y[i].argmax(axis=2))\n",
    "    preds.append(max_p)\n",
    "    labels.append(y)\n",
    "test_end_time = datetime.now()\n",
    "print(\"Model test time: %.1f minutes\" % ((test_end_time - test_start_time).seconds / 60,))    \n",
    "fpr, tpr, thresholds = metrics.roc_curve(labels,preds,pos_label=1)\n",
    "print(metrics.auc(fpr,tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches in slide: 105213\n",
      "Wall time: 4min 56s\n",
      "5000 gen time: 5.0 minutes\n",
      "Model test time: 4.6 minutes\n",
      "0.9954990298853795\n"
     ]
    }
   ],
   "source": [
    "ipath = all_image_path[0]\n",
    "tpath = all_mask_path[0]\n",
    "\n",
    "all_tissue_samples = find_patches_from_slide(ipath,tpath)\n",
    "print('Total patches in slide: %d' % len(all_tissue_samples)) \n",
    "all_tissue_samples.iloc[:10]\n",
    "all_tissue_samples.is_tumor.value_counts() \n",
    "test_start_time = datetime.now()\n",
    "sample_gen = gen_imgs(all_image_path,all_mask_path,all_tissue_samples, 5000, shuffle=True)\n",
    "%time example_X, example_y  = next(sample_gen)\n",
    "test_end_time = datetime.now()\n",
    "print(\"5000 gen time: %.1f minutes\" % ((test_end_time - test_start_time).seconds / 60,))\n",
    "\n",
    "start_x = 32\n",
    "start_y = 32\n",
    "pred_size = 192\n",
    "\n",
    "test_start_time = datetime.now()\n",
    "preds = []\n",
    "labels = []\n",
    "for i in range(5000):\n",
    "    prediction = predict_from_model(example_X[i],model)\n",
    "    pred_X = np.zeros((pred_size,pred_size))\n",
    "    y = example_y[i].argmax\n",
    "    for x in range(start_x,start_x+pred_size):\n",
    "        for y in range(start_y, start_y+pred_size):\n",
    "            pred_X[x-start_x][y-start_y] = prediction[x][y]\n",
    "            \n",
    "    pred_s = pd.Series(pred_X.flatten())\n",
    "    max_p = np.max(pred_s)\n",
    "    \n",
    "    y = np.max(example_y[i].argmax(axis=2))\n",
    "    preds.append(max_p)\n",
    "    labels.append(y)\n",
    "test_end_time = datetime.now()\n",
    "print(\"Model test time: %.1f minutes\" % ((test_end_time - test_start_time).seconds / 60,))    \n",
    "fpr, tpr, thresholds = metrics.roc_curve(labels,preds,pos_label=1)\n",
    "print(metrics.auc(fpr,tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004315432452131063"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_pred_x = np.max(preds)\n",
    "max_pred_x\n",
    "\n",
    "min_pred_x = np.min(preds)\n",
    "min_pred_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model test time: 0.0 minutes\n"
     ]
    }
   ],
   "source": [
    "test_start_time = datetime.now()\n",
    "slide = openslide.open_slide(ipath)\n",
    "tiles = DeepZoomGenerator(slide,tile_size=256,overlap=0, limit_bounds=False) \n",
    "test_end_time = datetime.now()\n",
    "print(\"Model test time: %.1f minutes\" % ((test_end_time - test_start_time).seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test_data_path_2():\n",
    "    path_dir = './'\n",
    "    file_list = os.listdir(path_dir)\n",
    "    paths = []\n",
    "    for pt in file_list:\n",
    "        if 'ipynb' in pt:\n",
    "            paths.append(pt)\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = read_test_data_path_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " '1_Data_Load.ipynb',\n",
       " 'data_augmentation_keras_test.ipynb',\n",
       " 'Inceptionv3_test.ipynb',\n",
       " 'Inception_V3_test-Copy1.ipynb',\n",
       " 'Inception_V3_test.ipynb',\n",
       " 'i_m_1.ipynb',\n",
       " 'model1.ipynb',\n",
       " 'model_inception_1.ipynb',\n",
       " 'model_simple_1.ipynb',\n",
       " 'model_unet_1.ipynb',\n",
       " 'OpenslideTest.ipynb',\n",
       " 'openslide_test_2.ipynb',\n",
       " 'sampleImg_sampleModel.ipynb',\n",
       " 'SampleModel_functional.ipynb',\n",
       " 'sample_model.ipynb',\n",
       " 'Sample_Model_0129.ipynb',\n",
       " 'Simple2_Unet.ipynb',\n",
       " 'Simple_Unet.ipynb',\n",
       " 'Simple_Unet1.ipynb',\n",
       " 's_m_1.ipynb',\n",
       " 's_m_2.ipynb',\n",
       " 'Test.ipynb',\n",
       " 'Test2.ipynb',\n",
       " 'Untitled.ipynb',\n",
       " 'Untitled1.ipynb',\n",
       " 'Untitled2.ipynb']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path_dir = './'\n",
    "file_list = os.listdir(path_dir)\n",
    "file_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
