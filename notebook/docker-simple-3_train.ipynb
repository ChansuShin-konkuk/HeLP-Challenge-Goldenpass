{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import os.path as osp\n",
    "import openslide\n",
    "from pathlib import Path\n",
    "# https://devstorylog.blogspot.com/2018/05/anaconda-python-vscode.html\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "from skimage.filters import threshold_otsu\n",
    "from openslide.deepzoom import DeepZoomGenerator\n",
    "import cv2\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# network\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Lambda, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.models import load_model\n",
    "\n",
    "# Unet\n",
    "import numpy as np \n",
    "import os\n",
    "\n",
    "import skimage.transform as trans\n",
    "#import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras\n",
    "\n",
    "# train\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from datetime import datetime\n",
    "\n",
    "# evaluate\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "from PIL import Image\n",
    "#from xml.etree.ElementTree import ElementTree, Element, SubElement\n",
    "from io import BytesIO\n",
    "import skimage.io as io\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from sklearn import metrics\n",
    "from keras.preprocessing.image import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 256\n",
    "IS_TRAIN = True\n",
    "def find_patches_from_slide(slide_path, truth_path, patch_size=PATCH_SIZE,filter_non_tissue=True,filter_only_all_tumor=True):\n",
    "    \n",
    "    slide_contains_tumor = 'pos' in slide_path\n",
    "    \n",
    "    ############### read_region을 위한 start, level, size를 구함 #######################\n",
    "    BOUNDS_OFFSET_PROPS = (openslide.PROPERTY_NAME_BOUNDS_X, openslide.PROPERTY_NAME_BOUNDS_Y)\n",
    "    BOUNDS_SIZE_PROPS = (openslide.PROPERTY_NAME_BOUNDS_WIDTH, openslide.PROPERTY_NAME_BOUNDS_HEIGHT)\n",
    "\n",
    "\n",
    "    if slide_contains_tumor:\n",
    "        with openslide.open_slide(slide_path) as slide:\n",
    "            start = (int(slide.properties.get('openslide.bounds-x',0)),int(slide.properties.get('openslide.bounds-y',0)))\n",
    "            level = np.log2(patch_size) \n",
    "            level = int(level)\n",
    "            \n",
    "            size_scale = tuple(int(slide.properties.get(prop, l0_lim)) / l0_lim\n",
    "                            for prop, l0_lim in zip(BOUNDS_SIZE_PROPS,\n",
    "                            slide.dimensions))\n",
    "            _l_dimensions = tuple(tuple(int(math.ceil(l_lim * scale))\n",
    "                            for l_lim, scale in zip(l_size, size_scale))\n",
    "                            for l_size in slide.level_dimensions)\n",
    "            size = _l_dimensions[level]\n",
    "            \n",
    "            \n",
    "            with openslide.open_slide(truth_path) as truth:\n",
    "                print('truth dimensions: ',truth.dimensions)\n",
    "                z_dimensions=[]\n",
    "                z_size = truth.dimensions\n",
    "                z_dimensions.append(z_size)\n",
    "                while z_size[0] > 1 or z_size[1] > 1:\n",
    "                    \n",
    "                    z_size = tuple(max(1, int(math.ceil(z / 2))) for z in z_size)\n",
    "                    z_dimensions.append(z_size)\n",
    "                print('truth_4_dimension_size:',z_dimensions[4]) # level-4\n",
    "            size = z_dimensions[level-4]\n",
    "            slide4 = slide.read_region(start,level,size)\n",
    "            print('slide4_size',slide4.size)\n",
    "    else :\n",
    "        with openslide.open_slide(slide_path) as slide:\n",
    "            start = (0,0)\n",
    "            level = np.log2(patch_size) \n",
    "            level = int(level)\n",
    "            \n",
    "            size_scale = (1,1)\n",
    "            _l_dimensions = tuple(tuple(int(math.ceil(l_lim * scale))\n",
    "                            for l_lim, scale in zip(l_size, size_scale))\n",
    "                            for l_size in slide.level_dimensions)\n",
    "            size = _l_dimensions[level]\n",
    "            \n",
    "            slide4 = slide.read_region(start,level,size) \n",
    "    ####################################################################################\n",
    "    \n",
    "    \n",
    "    # is_tissue 부분 \n",
    "    slide4_grey = np.array(slide4.convert('L'))\n",
    "    binary = slide4_grey > 0  # black이면 0임\n",
    "    \n",
    "    # 검은색 제외하고 흰색영역(배경이라고 여겨지는)에 대해서도 작업해주어야함.\n",
    "    slide4_not_black = slide4_grey[slide4_grey>0]\n",
    "    thresh = threshold_otsu(slide4_not_black)\n",
    "    \n",
    "    I, J = slide4_grey.shape\n",
    "    for i in range(I):\n",
    "        for j in range(J):\n",
    "            if slide4_grey[i,j] > thresh :\n",
    "                binary[i,j] = False\n",
    "    patches = pd.DataFrame(pd.DataFrame(binary).stack())\n",
    "    patches['is_tissue'] = patches[0]\n",
    "    patches.drop(0, axis=1,inplace =True)\n",
    "    patches.loc[:,'slide_path'] = slide_path\n",
    "    \n",
    "\n",
    "    if slide_contains_tumor:\n",
    "        with openslide.open_slide(truth_path) as truth:\n",
    "            thumbnail_truth = truth.get_thumbnail(size) \n",
    "        \n",
    "        patches_y = pd.DataFrame(pd.DataFrame(np.array(thumbnail_truth.convert(\"L\"))).stack())\n",
    "        patches_y['is_tumor'] = patches_y[0] > 0\n",
    "        \n",
    "        # mask된 영역이 애매할 수도 있으므로\n",
    "        patches_y['is_all_tumor'] = patches_y[0] == 255\n",
    "        patches_y.drop(0, axis=1, inplace=True)\n",
    "        samples = pd.concat([patches, patches_y], axis=1) #len(samples)\n",
    "    else:\n",
    "        samples = patches\n",
    "        #dfmi.loc[:,('one','second')] = value\n",
    "        samples.loc[:,'is_tumor'] = False\n",
    "        samples.loc[:,'is_all_tumor'] = False\n",
    "    \n",
    "    if filter_non_tissue:\n",
    "        samples = samples[samples.is_tissue == True] # remove patches with no tissue #samples = samples[samples.is_tissue == True]\n",
    "    \n",
    "    if filter_only_all_tumor :\n",
    "        samples['tile_loc'] = list(samples.index)\n",
    "        all_tissue_samples1 = samples[samples.is_tumor==False]\n",
    "        all_tissue_samples1 = all_tissue_samples1.append(samples[samples.is_all_tumor==True])\n",
    "        \n",
    "        all_tissue_samples1.reset_index(inplace=True, drop=True)\n",
    "    else :\n",
    "        return samples\n",
    "    \n",
    "    return all_tissue_samples1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2 # not_tumor, tumor\n",
    "file_handles=[]\n",
    "\n",
    "def gen_imgs(all_image_path, all_mask_path, samples, batch_size, patch_size = PATCH_SIZE, shuffle=True):\n",
    "   \n",
    "    num_samples = len(samples)\n",
    "    # 특정 몇개의 slide만 open 해서 쓰기\n",
    "    # 4개씩 묶었으니까 \n",
    "  \n",
    "    slide_path0 = all_image_path[0]\n",
    "    slide_path1 = all_image_path[1]\n",
    "    slide_path2 = all_image_path[2]\n",
    "    slide_path3 = all_image_path[3]\n",
    "\n",
    "    # slide 0~3 까지 미리 열어두기\n",
    "    slide0 = openslide.open_slide(slide_path0)\n",
    "    slide1 = openslide.open_slide(slide_path1)\n",
    "    slide2 = openslide.open_slide(slide_path2)\n",
    "    slide3 = openslide.open_slide(slide_path3)\n",
    "    file_handles.append(slide0)\n",
    "    file_handles.append(slide1)\n",
    "    file_handles.append(slide2)\n",
    "    file_handles.append(slide3)\n",
    "    # with openslide.open_slide(slide_path) as slide\n",
    "    tiles0 = DeepZoomGenerator(slide0,tile_size=patch_size, overlap=0, limit_bounds=False) \n",
    "    tiles1 = DeepZoomGenerator(slide1,tile_size=patch_size, overlap=0, limit_bounds=False)\n",
    "    tiles2 = DeepZoomGenerator(slide2,tile_size=patch_size, overlap=0, limit_bounds=False)\n",
    "    tiles3 = DeepZoomGenerator(slide3,tile_size=patch_size, overlap=0, limit_bounds=False)\n",
    "    \n",
    "\n",
    "    if 'pos' in slide_path0:\n",
    "        start_x0 = int(slide0.properties.get('openslide.bounds-x',0))\n",
    "        start_y0 = int(slide0.properties.get('openslide.bounds-y',0))\n",
    "        start_x0 = start_x0 / patch_size\n",
    "        start_y0 = start_y0 / patch_size\n",
    "        \n",
    "        truth0 = openslide.open_slide(all_mask_path[0])\n",
    "        truth_tiles0 = DeepZoomGenerator(truth0, tile_size=16, overlap=0, limit_bounds=False) \n",
    "        file_handles.append(truth0)\n",
    "        \n",
    "    else : \n",
    "        start_x0 = 0\n",
    "        start_y0 = 0\n",
    "    \n",
    "    if 'pos' in slide_path1:\n",
    "        start_x1 = int(slide1.properties.get('openslide.bounds-x',0))\n",
    "        start_y1 = int(slide1.properties.get('openslide.bounds-y',0))\n",
    "        start_x1 = start_x1 / patch_size\n",
    "        start_y1 = start_y1 / patch_size\n",
    "        \n",
    "        \n",
    "        truth1 = openslide.open_slide(all_mask_path[1])\n",
    "        truth_tiles1 = DeepZoomGenerator(truth1, tile_size=16, overlap=0, limit_bounds=False) \n",
    "        file_handles.append(truth1)\n",
    "        \n",
    "    else : \n",
    "        start_x1 = 0\n",
    "        start_y1 = 0\n",
    "    \n",
    "    if 'pos' in slide_path2:\n",
    "        start_x2 = int(slide2.properties.get('openslide.bounds-x',0))\n",
    "        start_y2 = int(slide2.properties.get('openslide.bounds-y',0))\n",
    "        start_x2 = start_x2 / patch_size\n",
    "        start_y2 = start_y2 / patch_size\n",
    "        \n",
    "        truth2 = openslide.open_slide(all_mask_path[2])\n",
    "        truth_tiles2 = DeepZoomGenerator(truth2, tile_size=16, overlap=0, limit_bounds=False) \n",
    "        file_handles.append(truth2)\n",
    "        \n",
    "    else : \n",
    "        start_x2 = 0\n",
    "        start_y2 = 0\n",
    "        \n",
    "    if 'pos' in slide_path3:\n",
    "        start_x3 = int(slide3.properties.get('openslide.bounds-x',0))\n",
    "        start_y3 = int(slide3.properties.get('openslide.bounds-y',0))\n",
    "        start_x3 = start_x3 / patch_size\n",
    "        start_y3 = start_y3 / patch_size\n",
    "        \n",
    "        truth3 = openslide.open_slide(all_mask_path[3])\n",
    "        truth_tiles3 = DeepZoomGenerator(truth3, tile_size=16, overlap=0, limit_bounds=False) \n",
    "        file_handles.append(truth3)\n",
    "    else : \n",
    "        start_x3 = 0\n",
    "        start_y3 = 0\n",
    "    \n",
    "\n",
    "    \n",
    "    for epoc in range(5): # Loop forever so the generator never terminates\n",
    "        if shuffle:\n",
    "            samples = samples.sample(frac=1) # shuffle samples\n",
    "\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples.iloc[offset:offset+batch_size]\n",
    "            images = []\n",
    "            masks = []\n",
    "            for _, batch_sample in batch_samples.iterrows(): # 배치마다 deep zoom 하네 약간 비효율적\n",
    "                \n",
    "                # 여기서 하나씩 4개 체크해서 해당되는 부분으로 가야지. for 4번 돌리면서 가야한다.\n",
    "                mask_size_up = np.zeros((patch_size,patch_size))\n",
    "                a,b=mask_size_up.shape\n",
    "                if batch_sample.slide_path == slide_path0:\n",
    "                    x, y = batch_sample.tile_loc[::-1]\n",
    "                    x += start_x0\n",
    "                    y += start_y0\n",
    "                    img = tiles0.get_tile(tiles0.level_count-1, (x,y))\n",
    "                    if 'pos' in slide_path0:\n",
    "                        mask = truth_tiles0.get_tile(truth_tiles0.level_count-1, batch_sample.tile_loc[::-1])\n",
    "                        mask = (cv2.cvtColor(np.array(mask), cv2.COLOR_RGB2GRAY) > 0).astype(int)\n",
    "                            # mask_size_up , 16 to 256\n",
    "                        k, l = mask.shape\n",
    "                        for i in range(k):\n",
    "                            for j in range(l):\n",
    "                                for o in range(16):\n",
    "                                    for p in range(16):\n",
    "                                        mask_size_up[i*16+o,j*16+p] = mask[i][j]\n",
    "\n",
    "                        \n",
    "                        # for i in range(a):\n",
    "                        #     for j in range(b) :\n",
    "                        #         k = i//16\n",
    "                        #         l = j//16\n",
    "                        #         mask_size_up[i,j] = mask[k,l]\n",
    "                    \n",
    "                elif batch_sample.slide_path == slide_path1:\n",
    "                    x, y = batch_sample.tile_loc[::-1]\n",
    "                    x += start_x1\n",
    "                    y += start_y1\n",
    "                    img = tiles1.get_tile(tiles1.level_count-1, (x,y))\n",
    "                    if 'pos' in slide_path1:\n",
    "                        mask = truth_tiles1.get_tile(truth_tiles1.level_count-1, batch_sample.tile_loc[::-1])\n",
    "                        mask = (cv2.cvtColor(np.array(mask), cv2.COLOR_RGB2GRAY) > 0).astype(int)\n",
    "                            # mask_size_up , 16 to 256\n",
    "                        k, l = mask.shape\n",
    "                        for i in range(k):\n",
    "                            for j in range(l):\n",
    "                                for o in range(16):\n",
    "                                    for p in range(16):\n",
    "                                        mask_size_up[i*16+o,j*16+p] = mask[i][j]\n",
    "\n",
    "                \n",
    "                elif batch_sample.slide_path == slide_path2:\n",
    "                    x, y = batch_sample.tile_loc[::-1]\n",
    "                    x += start_x2\n",
    "                    y += start_y2\n",
    "                    img = tiles2.get_tile(tiles2.level_count-1, (x,y))\n",
    "                    if 'pos' in slide_path2:\n",
    "                        mask = truth_tiles2.get_tile(truth_tiles2.level_count-1, batch_sample.tile_loc[::-1])\n",
    "                        mask = (cv2.cvtColor(np.array(mask), cv2.COLOR_RGB2GRAY) > 0).astype(int)\n",
    "                            # mask_size_up , 16 to 256\n",
    "                        k, l = mask.shape\n",
    "                        for i in range(k):\n",
    "                            for j in range(l):\n",
    "                                for o in range(16):\n",
    "                                    for p in range(16):\n",
    "                                        mask_size_up[i*16+o,j*16+p] = mask[i][j]\n",
    "\n",
    "                \n",
    "                else:\n",
    "                    x, y = batch_sample.tile_loc[::-1]\n",
    "                    x += start_x3\n",
    "                    y += start_y3\n",
    "                    img = tiles3.get_tile(tiles3.level_count-1, (x,y))\n",
    "                    if 'pos' in slide_path3:\n",
    "                        mask = truth_tiles3.get_tile(truth_tiles3.level_count-1, batch_sample.tile_loc[::-1])\n",
    "                        mask = (cv2.cvtColor(np.array(mask), cv2.COLOR_RGB2GRAY) > 0).astype(int)\n",
    "                            # mask_size_up , 16 to 256\n",
    "                        k, l = mask.shape\n",
    "                        for i in range(k):\n",
    "                            for j in range(l):\n",
    "                                for o in range(16):\n",
    "                                    for p in range(16):\n",
    "                                        mask_size_up[i*16+o,j*16+p] = mask[i][j]\n",
    "\n",
    "                \n",
    "                # 여기에 조건문 넣자.\n",
    "                if img.size != (patch_size,patch_size):\n",
    "                    print('this tisuue shape is not ',(patch_size,patch_size))\n",
    "                    img = Image.new('RGB', (patch_size,patch_size))\n",
    "                    mask_size_up = np.zeros((patch_size,patch_size))\n",
    "\n",
    "                images.append(np.array(img))\n",
    "                masks.append(mask_size_up)\n",
    "\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(masks)\n",
    "            #print('x_train_shape :', X_train.shape)\n",
    "            y_train = to_categorical(y_train, num_classes=2).reshape(y_train.shape[0], patch_size, patch_size, 2) \n",
    "            #print('y_train_shape : ',y_train.shape)\n",
    "            \n",
    "            #X_train, y_train = datagen().flow(X_train,y = y_train,batch_size = batch_size)\n",
    "            X_train, y_train = next(ImageDataGenerator(\n",
    "                rotation_range=90,\n",
    "                horizontal_flip=True,\n",
    "                vertical_flip=True,\n",
    "                brightness_range =(0.25,1.)).flow(X_train,y=y_train,batch_size=batch_size))\n",
    "            #print(X_train.shape)\n",
    "            #print(y_train.shape)\n",
    "            yield X_train, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_model(pretrained_weights = None):\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(256, 256, 3)))\n",
    "    model.add(Convolution2D(100, (3, 3), strides=(2, 2), activation='elu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Convolution2D(200, (3, 3), strides=(2, 2), activation='elu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Convolution2D(300, (3, 3), activation='elu', padding='same'))\n",
    "    model.add(Convolution2D(300, (3, 3), activation='elu',  padding='same'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Convolution2D(2, (1, 1))) # this is called upscore layer for some reason?\n",
    "    model.add(Conv2DTranspose(2, (31, 31), strides=(16, 16), activation='softmax', padding='same'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    if(pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_from_model(patches, model):\n",
    "    \"\"\"Predict which pixels are tumor.\n",
    "    \n",
    "    input: patch: `batch_size`x256x256x3, rgb image\n",
    "    input: model: keras model\n",
    "    output: prediction: 256x256x1, per-pixel tumor probability\n",
    "    \"\"\"\n",
    "    predictions = model.predict(patches)\n",
    "    predictions = predictions[:, :, :, 1]\n",
    "    return predictions\n",
    "def predict_from_model(patch, model):\n",
    "    \"\"\"Predict which pixels are tumor.\n",
    "    \n",
    "    input: patch: 256x256x3, rgb image\n",
    "    input: model: keras model\n",
    "    output: prediction: 256x256x1, per-pixel tumor probability\n",
    "    \"\"\"\n",
    "    \n",
    "    prediction = model.predict(patch.reshape(1, 256, 256, 3))\n",
    "    prediction = prediction[:, :, :, 1].reshape(256, 256)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "\n",
    "model = simple_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data Path Load\n",
    "def read_data_path():\n",
    "    image_paths = []\n",
    "    with open('train.txt','r') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip('\\n')\n",
    "            image_paths.append(line)\n",
    "    #print('image_path # : ',len(image_paths))\n",
    "\n",
    "    tumor_mask_paths = []\n",
    "\n",
    "    with open('train_mask.txt','r') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip('\\n')\n",
    "            tumor_mask_paths.append(line)\n",
    "    #print('mask_patch # : ',len(tumor_mask_paths))\n",
    "    \n",
    "    return image_paths, tumor_mask_paths\n",
    "\n",
    "def read_test_data_path():\n",
    "    image_paths = []\n",
    "    with open('test.txt','r') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip('\\n')\n",
    "            image_paths.append(line)\n",
    "    #print('image_path # : ',len(image_paths))\n",
    "    \n",
    "    return image_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths, tumor_mask_paths = read_data_path()\n",
    "\n",
    "slide_4_list_1 = [[102,104,29,44],[144,55,30,18],[54,65,21,36],[139,82,1,49],[105,151,15,2],[75,100,41,9],[156,113,32,37]]\n",
    "slide_4_list_2 = [[109,58,14,28],[101,69,11,43],[94,74,3,20],[64,140,17,16],[92,154,8,26],[99,60,0,33],[86,146,25,19],[68,112,38,51],\n",
    "                 [71,136,31,4],[59,91,12,6]]\n",
    "slide_4_list_3 = [[143,132,124,85],[95,120,81,77],[97,96,110,83],[152,128,149,155],[153,111,57,138],[134,135,114,76],\n",
    "                  [123,90,121,61],[147,148,119,142],[66,137,63,80],[70,79,115,133],[129,141,127,145]]\n",
    "slide_4_test = [[55,55,0,0]]\n",
    "\n",
    "columns = ['is_tissue','slide_path','is_tumor','is_all_tumor','tile_loc']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th training\n",
      "\n",
      "55\n",
      "55\n",
      "0\n",
      "0\n",
      "truth dimensions:  (5316, 10007)\n",
      "truth_4_dimension_size: (333, 626)\n",
      "slide4_size (333, 626)\n",
      "truth dimensions:  (5316, 10007)\n",
      "truth_4_dimension_size: (333, 626)\n",
      "slide4_size (333, 626)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cjh/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 31/563 [>.............................] - ETA: 12:05 - loss: 0.6298 - acc: 0.6482"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-9b9ea8e0e976>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_samples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         epochs=N_EPOCHS)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mtrain_end_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 5\n",
    "\n",
    "for i in range(len(slide_4_test)):\n",
    "    # [1] dataset , 2 pos, 2 neg, mean ratio = 3:1\n",
    "    print(i,'th training\\n')\n",
    "    print(slide_4_test[i][0])\n",
    "    print(slide_4_test[i][1]) \n",
    "    print(slide_4_test[i][2])\n",
    "    print(slide_4_test[i][3])\n",
    "\n",
    "    four_samples = pd.DataFrame(columns = columns)\n",
    "    four_image_path = list()\n",
    "    four_mask_path = list()    \n",
    "    for j in range(4):\n",
    "        image_path = image_paths[slide_4_test[i][j]] # 이 부분은 data 읽을때 고치자 ( [1:] 빼야함)\n",
    "        mask_path = tumor_mask_paths[slide_4_test[i][j]] # 이 부분은 data 읽을때 고치자\n",
    "        samples = find_patches_from_slide('.'+image_path, '.'+mask_path)\n",
    "        \n",
    "        four_samples = four_samples.append(samples)   \n",
    "        four_image_path.append('.'+image_path)\n",
    "        four_mask_path.append('.'+mask_path)\n",
    "        \n",
    "    # train sample size\n",
    "    NUM_SAMPLES = len(four_samples)\n",
    "    if NUM_SAMPLES > 20000:\n",
    "        NUM_SAMPLES = 20000\n",
    "    \n",
    "    samples = four_samples.sample(NUM_SAMPLES, random_state=42)\n",
    "    samples.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "    for train_index, test_index in split.split(samples, samples[\"is_tumor\"]):\n",
    "            train_samples = samples.loc[train_index]\n",
    "            validation_samples = samples.loc[test_index]\n",
    "\n",
    "    train_generator = gen_imgs(four_image_path,four_mask_path,train_samples, BATCH_SIZE)\n",
    "    validation_generator = gen_imgs(four_image_path,four_mask_path,validation_samples, BATCH_SIZE)\n",
    "    \n",
    "    train_start_time = datetime.now()\n",
    "    history = model.fit_generator(train_generator, np.ceil(len(train_samples) / BATCH_SIZE),\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=np.ceil(len(validation_samples) / BATCH_SIZE),\n",
    "        epochs=N_EPOCHS)\n",
    "\n",
    "    train_end_time = datetime.now()\n",
    "    print(\"Model training time: %.1f minutes\" % ((train_end_time - train_start_time).seconds / 60,))\n",
    "    # split\n",
    "    # data gen : all_image_path, all_mask_path\n",
    "\n",
    "\n",
    "path = '/data/model'\n",
    "model.save(path+'/u_1.h5')\n",
    "print('model save completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
