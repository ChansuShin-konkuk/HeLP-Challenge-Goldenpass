{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer Detection Model U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import Module\n",
    "\n",
    "- [OpenSlide](https://openslide.org/api/python/#module-openslide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import openslide\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "from skimage.filters import threshold_otsu\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from openslide.deepzoom import DeepZoomGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Patch Gen DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_patches_from_slide(slide_path, \n",
    "                            truth_path, \n",
    "                            patch_size=256, \n",
    "                            filter_non_tissue=True,\n",
    "                            filter_only_all_tumor=True):\n",
    "    '''Returns a DataFrame of all patches in slide\n",
    "        Args:\n",
    "            - slide_path: path of slide\n",
    "            - truth_path: path of truth(mask)\n",
    "            - patch_size: patch size for samples\n",
    "            - filter_non_tissue: remove samples no tissue detected\n",
    "        Returns:\n",
    "            - all_tissue_samples: patch samples from slide'''\n",
    "    # 해당 데이터가 양성인지 판단\n",
    "    slide_contains_tumor = 'pos' in slide_path\n",
    "\n",
    "    # read_region을 위한 start, level, size 계산\n",
    "    bounds_offset_props = (openslide.PROPERTY_NAME_BOUNDS_X, openslide.PROPERTY_NAME_BOUNDS_Y)\n",
    "    bounds_size_props = (openslide.PROPERTY_NAME_BOUNDS_WIDTH, openslide.PROPERTY_NAME_BOUNDS_HEIGHT)\n",
    "\n",
    "    with openslide.open_slide(slide_path) as slide:\n",
    "        start = (0, 0)\n",
    "        size_scale = (1, 1)\n",
    "        level = int(np.log2(patch_size))\n",
    "        l_dimensions = [(int(np.ceil(dim_x * size_scale[0])), int(np.ceil(dim_y * size_scale[1])))\n",
    "                        for dim_x, dim_y in slide.level_dimensions]\n",
    "        size = l_dimensions[level]\n",
    "        \n",
    "        if slide_contains_tumor: \n",
    "            start = (int(slide.properties.get(bounds_offset_props[0], 0)), \n",
    "                     int(slide.properties.get(bounds_offset_props[1], 0)))\n",
    "            size_scale = tuple(int(slide.properties.get(prop, dim)) / dim \n",
    "                               for prop, dim in zip(bounds_size_props, slide.dimensions))\n",
    "            \n",
    "            with openslide.open_slide(truth_path) as truth:\n",
    "                z_dimensions = []\n",
    "                z_size = truth.dimensions\n",
    "                z_dimensions.append(z_size)\n",
    "                while z_size[0] > 1 or z_size[1] > 1:\n",
    "                    z_size = tuple(max(1, int(np.ceil(z/2))) for z in z_size)\n",
    "                    z_dimensions.append(z_size)\n",
    "            size = z_dimensions[level-4]\n",
    "\n",
    "        slide4 = slide.read_region(start, level, size)\n",
    "        \n",
    "    # is_tissue 부분\n",
    "    slide4_grey = np.array(slide4.convert('L'))\n",
    "\n",
    "    # background에 대한 작업\n",
    "    slide4_not_black = slide4_grey[slide4_grey > 0]\n",
    "    # thresh = threshold_otsu(slide4_not_black)\n",
    "    ret, th = cv2.threshold(slide4_not_black, 0, 255, \n",
    "                            cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "    binary = slide4_grey > 0  # black == 0\n",
    "    h, w = slide4_grey.shape\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            if slide4_grey[i, j] > ret:\n",
    "                binary[i, j] = False\n",
    "\n",
    "    # patch_df\n",
    "    patches = pd.DataFrame(pd.DataFrame(binary).stack(), columns=['is_tissue'])\n",
    "    patches.loc[:, 'slide_path'] = slide_path\n",
    "    \n",
    "    # is_tumor 부분\n",
    "    if slide_contains_tumor:\n",
    "        with openslide.open_slide(truth_path) as truth:\n",
    "            thumbnail_truth = truth.get_thumbnail(size)\n",
    "\n",
    "        # truth pathes_df\n",
    "        patches_y = pd.DataFrame(\n",
    "                pd.DataFrame(np.array(thumbnail_truth.convert('L'))).stack())\n",
    "        patches_y['is_tumor'] = patches_y[0] > 0\n",
    "\n",
    "        # mask된 영역이 애매한 경우\n",
    "        patches_y['is_all_tumor'] = patches_y[0] == 255\n",
    "        patches_y.drop(0, axis=1, inplace=True)\n",
    "        samples = pd.concat([patches, patches_y], axis=1)\n",
    "    else: \n",
    "        samples = patches\n",
    "        samples.loc[:, 'is_tumor'] = False\n",
    "        samples.loc[:, 'is_all_tumor'] = False\n",
    "\n",
    "    if filter_non_tissue:  # tissue인것만 가져오기\n",
    "        samples = samples[samples['is_tissue'] == True]\n",
    "        \n",
    "    if filter_only_all_tumor:  # 어떤 의미?\n",
    "        samples['tile_loc'] = list(samples.index)\n",
    "        all_tissue_samples = samples[samples['is_tumor'] == False]\n",
    "        all_tissue_samples = all_tissue_samples.append(samples[samples['is_all_tumor'] == True])\n",
    "        all_tissue_samples.reset_index(inplace=True, drop=True)\n",
    "    else:\n",
    "        return samples\n",
    "    \n",
    "    return all_tissue_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.38 s, sys: 72 ms, total: 1.45 s\n",
      "Wall time: 1.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "slide_path = '../data/train/pos/16-S-042893_A1.mrxs'  # slide\n",
    "truth_path = '../data/train/pos/Mask_16-S-042893_A1.png'  # mask\n",
    "\n",
    "all_tissue_samples = find_patches_from_slide(slide_path, truth_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    74490\n",
       "True     30723\n",
       "Name: is_tumor, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tissue_samples['is_tumor'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train data Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(samples,\n",
    "              slide_paths,\n",
    "              truth_paths,\n",
    "              batch_size,\n",
    "              patch_size=256,\n",
    "              shuffle=True):\n",
    "    '''The generator for DataSet\n",
    "        Args:\n",
    "            - samples: DataFrame of samples\n",
    "            - slide_paths: paths of all slides \n",
    "            - truth_paths: paths of all truth(masks)\n",
    "            - batch_size: mini-batch size\n",
    "            - patch_size: patch size for samples\n",
    "            - shuffle: bool, if True shuffle samples\n",
    "        Returns(yield):\n",
    "            - train_x: train dataset → [batch_size, patch_size, patch_size, 3]\n",
    "            - train_y: train labelset → [batch_size, patch_size, patch_size, 2]'''\n",
    "    \n",
    "    # 4개씩 묶은 slide path\n",
    "    slide0 = openslide.open_slide(slide_paths[0])\n",
    "    slide1 = openslide.open_slide(slide_paths[1])\n",
    "    slide2 = openslide.open_slide(slide_paths[2])\n",
    "    slide3 = openslide.open_slide(slide_paths[3])\n",
    "\n",
    "    # tiles\n",
    "    tiles0 = DeepZoomGenerator(slide0, tile_size=patch_size, overlap=0, limit_bounds=False)\n",
    "    tiles1 = DeepZoomGenerator(slide1, tile_size=patch_size, overlap=0, limit_bounds=False)\n",
    "    tiles2 = DeepZoomGenerator(slide2, tile_size=patch_size, overlap=0, limit_bounds=False)\n",
    "    tiles3 = DeepZoomGenerator(slide3, tile_size=patch_size, overlap=0, limit_bounds=False)\n",
    "\n",
    "    start_x0, start_y0 = 0, 0\n",
    "    start_x1, start_y1 = 0, 0\n",
    "    start_x2, start_y2 = 0, 0\n",
    "    start_x3, start_y3 = 0, 0\n",
    "    if 'pos' in slide_paths[0]:\n",
    "        start_x0 = int(slide0.properties.get('openslide.bounds-x', 0)) / patch_size\n",
    "        start_y0 = int(slide0.properties.get('openslide.bounds-y', 0)) / patch_size\n",
    "        truth0 = openslide.open_slide(truth_paths[0])\n",
    "        truth_tiles0 = DeepZoomGenerator(truth0, tile_size=16,overlap=0, limit_bounds=False)\n",
    "    \n",
    "    if 'pos' in slide_paths[1]: \n",
    "        start_x1 = int(slide1.properties.get('openslide.bounds-x', 0)) / patch_size\n",
    "        start_y1 = int(slide1.properties.get('openslide.bounds-y', 0)) / patch_size\n",
    "        truth1 = openslide.open_slide(truth_paths[1])\n",
    "        truth_tiles1 = DeepZoomGenerator(truth1, tile_size=16,overlap=0, limit_bounds=False)\n",
    "        \n",
    "    if 'pos' in slide_paths[2]:\n",
    "        start_x2 = int(slide2.properties.get('openslide.bounds-x', 0)) / patch_size\n",
    "        start_y2 = int(slide2.properties.get('openslide.bounds-y', 0)) / patch_size\n",
    "        truth2 = openslide.open_slide(truth_paths[2])\n",
    "        truth_tiles2 = DeepZoomGenerator(truth2, tile_size=16, overlap=0, limit_bounds=False)\n",
    "        \n",
    "    if 'pos' in slide_paths[3]:\n",
    "        start_x3 = int(slide3.properties.get('openslide.bounds-x', 0)) / patch_size\n",
    "        start_y3 = int(slide3.properties.get('openslide.bounds-y', 0)) / patch_size\n",
    "        truth3 = openslide.open_slide(truth_paths[3])\n",
    "        truth_tiles3 = DeepZoomGenerator(truth3, tile_size=16, overlap=0, limit_bounds=False)\n",
    "        \n",
    "    num_samples = len(samples)\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            samples = samples.sample(frac=1)  # shuffling\n",
    "\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples.iloc[offset:offset+batch_size]\n",
    "\n",
    "            batch_tiles, batch_masks = [], []\n",
    "            for slide_path, (y, x) in zip(batch_samples['slide_path'].values, \n",
    "                                          batch_samples['tile_loc'].values):\n",
    "                \n",
    "                mask_tile_zoom = np.zeros((patch_size,patch_size))\n",
    "                if slide_path == slide_paths[0]:\n",
    "                    img = tiles0.get_tile(tiles0.level_count-1, (x+start_x0, y+start_y0))\n",
    "                    if 'pos' in slide_path:\n",
    "                        mask_tile = truth_tiles0.get_tile(truth_tiles0.level_count-1, (x, y))\n",
    "                        mask_tile = (cv2.cvtColor(np.array(mask_tile), cv2.COLOR_RGB2GRAY) > 0).astype(int)\n",
    "                        # mask_size_up , 16 to 256\n",
    "                        k, l = mask_tile.shape\n",
    "                        for i in range(k):\n",
    "                            for j in range(l):\n",
    "                                for o in range(16):\n",
    "                                    for p in range(16):\n",
    "                                        mask_tile_zoom[i*16+o,j*16+p] = mask_tile[i][j]\n",
    "                        \n",
    "                elif slide_path == slide_paths[1]:\n",
    "                    img = tiles1.get_tile(tiles1.level_count-1, (x+start_x1, y+start_y1))\n",
    "                    if 'pos' in slide_path:\n",
    "                        mask_tile = truth_tiles1.get_tile(truth_tiles1.level_count-1, (x, y))\n",
    "                        mask_tile = (cv2.cvtColor(np.array(mask_tile), cv2.COLOR_RGB2GRAY) > 0).astype(int)\n",
    "                        # mask_size_up , 16 to 256\n",
    "                        k, l = mask_tile.shape\n",
    "                        for i in range(k):\n",
    "                            for j in range(l):\n",
    "                                for o in range(16):\n",
    "                                    for p in range(16):\n",
    "                                        mask_tile_zoom[i*16+o,j*16+p] = mask_tile[i][j]\n",
    "                \n",
    "                elif slide_path == slide_paths[2]:\n",
    "                    img = tiles2.get_tile(tiles2.level_count-1, (x+start_x2, y+start_y2))\n",
    "                    if 'pos' in slide_path:\n",
    "                        mask_tile = truth_tiles2.get_tile(truth_tiles2.level_count-1, (x, y))\n",
    "                        mask_tile = (cv2.cvtColor(np.array(mask_tile), cv2.COLOR_RGB2GRAY) > 0).astype(int)\n",
    "                        # mask_size_up , 16 to 256\n",
    "                        k, l = mask_tile.shape\n",
    "                        for i in range(k):\n",
    "                            for j in range(l):\n",
    "                                for o in range(16):\n",
    "                                    for p in range(16):\n",
    "                                        mask_tile_zoom[i*16+o,j*16+p] = mask_tile[i][j]\n",
    "\n",
    "                elif slide_path == slide_paths[3]:\n",
    "                    img = tiles3.get_tile(tiles3.level_count-1, (x+start_x3, y+start_y3))\n",
    "                    if 'pos' in slide_path:\n",
    "                        mask_tile = truth_tiles3.get_tile(truth_tiles3.level_count-1, (x, y))\n",
    "                        mask_tile = (cv2.cvtColor(np.array(mask_tile), cv2.COLOR_RGB2GRAY) > 0).astype(int)\n",
    "                        # mask_size_up , 16 to 256\n",
    "                        k, l = mask_tile.shape\n",
    "                        for i in range(k):\n",
    "                            for j in range(l):\n",
    "                                for o in range(16):\n",
    "                                    for p in range(16):\n",
    "                                        mask_tile_zoom[i*16+o,j*16+p] = mask_tile[i][j]\n",
    "\n",
    "                \n",
    "                if img.size != (patch_size, patch_size):\n",
    "                    img = Image.new('RGB', (patch_size, patch_size))\n",
    "                    mask_tile_zoom = np.zeros((patch_size, patch_size))\n",
    "                    \n",
    "                batch_tiles.append(np.array(img))\n",
    "                batch_masks.append(mask_tile_zoom)\n",
    "                \n",
    "            # train_x & train_y\n",
    "            train_x = np.array(batch_tiles)\n",
    "            train_y = to_categorical(np.array(batch_masks), num_classes=2)\n",
    "            \n",
    "            # data augmentation\n",
    "#             train_x, train_y = next(\n",
    "#                 ImageDataGenerator(rotation_range=90,\n",
    "#                                    horizontal_flip=True,\n",
    "#                                    vertical_flip=True,\n",
    "#                                    brightness_range=(0.25, 1.)).flow(train_x, y=train_y, batch_size=batch_size))\n",
    "            yield train_x, train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "def create_model(patch_size=256, pre_trained_path=False):\n",
    "    # Build U-Net model\n",
    "    inputs = layers.Input(shape=(patch_size, patch_size, 3), dtype='float32', name='inputs')\n",
    "    inputs_norm = layers.Lambda(lambda x: x/255. - .5)(inputs)\n",
    "\n",
    "    # Conv layers\n",
    "    conv1 = layers.Conv2D(16, 3, padding='same', \n",
    "                          activation='relu', kernel_initializer='he_normal')(inputs_norm)\n",
    "    conv1 = layers.Dropout(0.1)(conv1)\n",
    "    conv1 = layers.Conv2D(16, 3, padding='same', \n",
    "                          activation='relu', kernel_initializer='he_normal')(conv1)\n",
    "    pool1 = layers.MaxPooling2D()(conv1)\n",
    "\n",
    "    conv2 = layers.Conv2D(32, 3, padding='same', \n",
    "                          activation='relu', kernel_initializer='he_normal')(pool1)\n",
    "    conv2 = layers.Dropout(0.1)(conv2)\n",
    "    conv2 = layers.Conv2D(32, 3, padding='same', \n",
    "                          activation='relu', kernel_initializer='he_normal')(conv2)\n",
    "    pool2 = layers.MaxPooling2D()(conv2)\n",
    "\n",
    "    conv3 = layers.Conv2D(64, 3, padding='same', \n",
    "                          activation='relu', kernel_initializer='he_normal')(pool2)\n",
    "    conv3 = layers.Dropout(0.2)(conv3)\n",
    "    conv3 = layers.Conv2D(64, 3, padding='same', \n",
    "                          activation='relu', kernel_initializer='he_normal')(conv3)\n",
    "    pool3 = layers.MaxPooling2D()(conv3)\n",
    "\n",
    "    conv4 = layers.Conv2D(128, 3, padding='same', \n",
    "                          activation='relu', kernel_initializer='he_normal')(pool3)\n",
    "    conv4 = layers.Dropout(0.2)(conv4)\n",
    "    conv4 = layers.Conv2D(128, 3, padding='same', \n",
    "                          activation='relu', kernel_initializer='he_normal')(conv4)\n",
    "    pool4 = layers.MaxPooling2D()(conv4)\n",
    "\n",
    "    conv5 = layers.Conv2D(256, 3, padding='same', \n",
    "                          activation='relu', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = layers.Dropout(0.3)(conv5)\n",
    "    conv5 = layers.Conv2D(256, 3, padding='same', \n",
    "                          activation='relu', kernel_initializer='he_normal')(conv5)\n",
    "\n",
    "    # Up-Conv layers\n",
    "    up_conv6 = layers.Conv2DTranspose(128, 2, strides=2, padding='same')(conv5)\n",
    "    up_conv6 = layers.concatenate([up_conv6, conv4])\n",
    "    conv6 = layers.Conv2D(128, 3, padding='same', \n",
    "                          activation='relu', kernel_initializer='he_normal')(up_conv6)\n",
    "    conv6 = layers.Dropout(0.2)(conv6)\n",
    "    conv6 = layers.Conv2D(128, 3, padding='same', \n",
    "                          activation='relu', kernel_initializer='he_normal')(conv6)\n",
    "\n",
    "    up_conv7 = layers.Conv2DTranspose(64, 2, strides=2, padding='same')(conv6)\n",
    "    up_conv7 = layers.concatenate([up_conv7, conv3])\n",
    "    conv7 = layers.Conv2D(64, 3, padding='same', \n",
    "                          activation='relu', kernel_initializer='he_normal')(up_conv7)\n",
    "    conv7 = layers.Dropout(0.2)(conv7)\n",
    "    conv7 = layers.Conv2D(64, 3, padding='same', \n",
    "                          activation='relu', kernel_initializer='he_normal')(conv7)\n",
    "\n",
    "    up_conv8 = layers.Conv2DTranspose(32, 2, strides=2, padding='same')(conv7)\n",
    "    up_conv8 = layers.concatenate([up_conv8, conv2])\n",
    "    conv8 = layers.Conv2D(32, 3, padding='same', \n",
    "                          activation='relu', kernel_initializer='he_normal')(up_conv8)\n",
    "    conv8 = layers.Dropout(0.1)(conv8)\n",
    "    conv8 = layers.Conv2D(32, 3, padding='same', \n",
    "                          activation='relu', kernel_initializer='he_normal')(conv8)\n",
    "\n",
    "    up_conv9 = layers.Conv2DTranspose(16, 2, strides=2, padding='same')(conv8)\n",
    "    up_conv9 = layers.concatenate([up_conv9, conv1])\n",
    "    conv9 = layers.Conv2D(16, 3, padding='same', \n",
    "                          activation='relu', kernel_initializer='he_normal')(up_conv9)\n",
    "    conv9 = layers.Dropout(0.1)(conv9)\n",
    "    conv9 = layers.Conv2D(16, 3, padding='same', \n",
    "                          activation='relu', kernel_initializer='he_normal')(conv9)\n",
    "\n",
    "    outputs = layers.Conv2D(2, 1, activation='softmax', \n",
    "                           kernel_initializer='he_normal')(conv9)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    if pre_trained_path:\n",
    "        model = models.load_model(pre_trained_path)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# def simple_model(pretrained_weights = None):\n",
    "#     model = Sequential()\n",
    "#     model.add(layers.Lambda(lambda x: x / 255.0 - 0.5, input_shape=(256, 256, 3)))\n",
    "#     model.add(layers.Convolution2D(100, (3, 3), strides=(2, 2), activation='elu', padding='same'))\n",
    "#     model.add(layers.MaxPooling2D())\n",
    "#     model.add(layers.Convolution2D(200, (3, 3), strides=(2, 2), activation='elu', padding='same'))\n",
    "#     model.add(layers.MaxPooling2D())\n",
    "#     model.add(layers.Convolution2D(300, (3, 3), activation='elu', padding='same'))\n",
    "#     model.add(layers.Convolution2D(300, (3, 3), activation='elu',  padding='same'))\n",
    "#     model.add(layers.Dropout(0.2))\n",
    "#     model.add(layers.Convolution2D(2, (1, 1))) # this is called upscore layer for some reason?\n",
    "#     model.add(layers.Conv2DTranspose(2, (31, 31), strides=(16, 16), activation='softmax', padding='same'))\n",
    "\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "#     if(pretrained_weights):\n",
    "#         model.load_weights(pretrained_weights)\n",
    "        \n",
    "#     return model\n",
    "\n",
    "# model = simple_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_path():\n",
    "    slide_paths, mask_paths = {}, {}\n",
    "    with open('./train.txt', 'r') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            path = line.rstrip('\\n')\n",
    "            slide_paths[idx] = path\n",
    "    \n",
    "    with open('./train_mask.txt', 'r') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            path = line.rstrip('\\n')\n",
    "            mask_paths[idx] = path\n",
    "            \n",
    "    return slide_paths, mask_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide_paths, mask_paths = get_data_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide_4_list_1 = [[102,104,29,44],[144,55,30,18],[54,65,21,36],[139,82,1,49],[105,151,15,2],[75,100,41,9],[156,113,32,37]]\n",
    "slide_4_list_2 = [[109,58,14,28],[101,69,11,43],[94,74,3,20],[64,140,17,16],[92,154,8,26],[99,60,0,33],[86,146,25,19],[68,112,38,51],\n",
    "                 [71,136,31,4],[59,91,12,6]]\n",
    "slide_4_list_3 = [[143,132,124,85],[95,120,81,77],[97,96,110,83],[152,128,149,155],[153,111,57,138],[134,135,114,76],\n",
    "                  [123,90,121,61],[147,148,119,142],[66,137,63,80],[70,79,115,133],[129,141,127,145]]\n",
    "slide_4_test = [[55,55, 0, 0]]\n",
    "\n",
    "columns = ['is_tissue','slide_path','is_tumor','is_all_tumor','tile_loc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cjh/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:89: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 122s 974ms/step - loss: 13.3230 - acc: 0.7853 - val_loss: 9.0734 - val_acc: 0.9046\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 115s 917ms/step - loss: 7.8958 - acc: 0.9140 - val_loss: 6.7301 - val_acc: 0.9121\n",
      "Epoch 3/10\n",
      " 28/125 [=====>........................] - ETA: 47s - loss: 7.1492 - acc: 0.9181"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    145\u001b[0m       \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    823\u001b[0m           \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m           \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;31m# Make sure to rethrow the first exception in the queue, if any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 32\n",
    "n_epochs = 10\n",
    "for slides in slide_4_test:\n",
    "    sample_group_df = pd.DataFrame(\n",
    "            columns=['is_tissue','slide_path','is_tumor','is_all_tumor','tile_loc'])\n",
    "    \n",
    "    group_slide_path, group_mask_path = [], []\n",
    "    for idx in slides:\n",
    "        slide_path, truth_path = slide_paths[idx], mask_paths[idx]\n",
    "        samples = find_patches_from_slide('.'+slide_path, '.'+truth_path)\n",
    "        sample_group_df = sample_group_df.append(samples)\n",
    "        group_slide_path.append('.'+slide_path)\n",
    "        group_mask_path.append('.'+truth_path)\n",
    "        \n",
    "    num_samples = len(sample_group_df)\n",
    "    if num_samples > 5000:\n",
    "        num_samples = 5000\n",
    "    \n",
    "    samples = sample_group_df.sample(num_samples, random_state=42)\n",
    "    samples.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    for train_index, test_index in split.split(samples, samples[\"is_tumor\"]):\n",
    "            train_samples = samples.loc[train_index]\n",
    "            validation_samples = samples.loc[test_index]\n",
    "            \n",
    "    train_gen = generator(train_samples, group_slide_path, group_mask_path, batch_size)\n",
    "    val_gen = generator(validation_samples, group_slide_path, group_mask_path, batch_size)\n",
    "    \n",
    "    model.fit_generator(train_gen, \n",
    "                        steps_per_epoch=np.ceil(len(train_samples)/batch_size),\n",
    "                        epochs=n_epochs,\n",
    "                        validation_data=val_gen,\n",
    "                        validation_steps=np.ceil(len(validation_samples)/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_x, val_y = next(val_gen)\n",
    "\n",
    "# f, axes = plt.subplots(4, 8, figsize=(20, 4));\n",
    "# ax = axes.flatten()\n",
    "# for i in range(0, val_x.shape[0]):\n",
    "#     _ = ax[i].imshow(val_x[i]);\n",
    "#     _ = ax[i].axis('off');\n",
    "# f.suptitle('Batch of Patches 32x256x256x3');\n",
    "    \n",
    "# f, axes = plt.subplots(4, 8, figsize=(20, 4));\n",
    "# ax = axes.flatten()    \n",
    "# for i in range(0, val_x.shape[0]):\n",
    "#     _ = ax[i].imshow(val_y[i].argmax(axis=2), cmap='gray', vmin=0, vmax=1);\n",
    "#     _ = ax[i].axis('off');\n",
    "# f.suptitle('Batch of Truth Masks 32x256x256x1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
