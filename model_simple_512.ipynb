{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "import openslide\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "# 이부분 python 에서는 뺴주기\n",
    "\n",
    "from skimage.filters import threshold_otsu\n",
    "from openslide.deepzoom import DeepZoomGenerator\n",
    "import cv2\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# network\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Lambda, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.models import load_model\n",
    "\n",
    "# Unet\n",
    "import numpy as np \n",
    "import os\n",
    "\n",
    "import skimage.transform as trans\n",
    "#import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras\n",
    "\n",
    "\n",
    "# train\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from datetime import datetime\n",
    "\n",
    "# evaluate\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "from PIL import Image\n",
    "from xml.etree.ElementTree import ElementTree, Element, SubElement\n",
    "from io import BytesIO\n",
    "import skimage.io as io\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing.image import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 512\n",
    "IS_TRAIN = True\n",
    "def find_patches_from_slide(slide_path, truth_path, patch_size=PATCH_SIZE,filter_non_tissue=True,filter_only_all_tumor=True):\n",
    "    \n",
    "    slide_contains_tumor = 'pos' in slide_path\n",
    "    \n",
    "    ############### read_region을 위한 start, level, size를 구함 #######################\n",
    "    BOUNDS_OFFSET_PROPS = (openslide.PROPERTY_NAME_BOUNDS_X, openslide.PROPERTY_NAME_BOUNDS_Y)\n",
    "    BOUNDS_SIZE_PROPS = (openslide.PROPERTY_NAME_BOUNDS_WIDTH, openslide.PROPERTY_NAME_BOUNDS_HEIGHT)\n",
    "\n",
    "\n",
    "    if slide_contains_tumor:\n",
    "        with openslide.open_slide(slide_path) as slide:\n",
    "            start = (int(slide.properties.get('openslide.bounds-x',0)),int(slide.properties.get('openslide.bounds-y',0)))\n",
    "            level = np.log2(patch_size) \n",
    "            level = int(level)\n",
    "            \n",
    "            size_scale = tuple(int(slide.properties.get(prop, l0_lim)) / l0_lim\n",
    "                            for prop, l0_lim in zip(BOUNDS_SIZE_PROPS,\n",
    "                            slide.dimensions))\n",
    "            _l_dimensions = tuple(tuple(int(math.ceil(l_lim * scale))\n",
    "                            for l_lim, scale in zip(l_size, size_scale))\n",
    "                            for l_size in slide.level_dimensions)\n",
    "            size = _l_dimensions[level]\n",
    "            \n",
    "            \n",
    "            with openslide.open_slide(truth_path) as truth:\n",
    "                print('truth dimensions: ',truth.dimensions)\n",
    "                z_dimensions=[]\n",
    "                z_size = truth.dimensions\n",
    "                z_dimensions.append(z_size)\n",
    "                while z_size[0] > 1 or z_size[1] > 1:\n",
    "                    \n",
    "                    z_size = tuple(max(1, int(math.ceil(z / 2))) for z in z_size)\n",
    "                    z_dimensions.append(z_size)\n",
    "                print('truth_4_dimension_size:',z_dimensions[4]) # level-4\n",
    "            size = z_dimensions[level-4]\n",
    "            slide4 = slide.read_region(start,level,size)\n",
    "            print('slide4_size',slide4.size)\n",
    "    else :\n",
    "        with openslide.open_slide(slide_path) as slide:\n",
    "            start = (0,0)\n",
    "            level = np.log2(patch_size) \n",
    "            level = int(level)\n",
    "            \n",
    "            size_scale = (1,1)\n",
    "            _l_dimensions = tuple(tuple(int(math.ceil(l_lim * scale))\n",
    "                            for l_lim, scale in zip(l_size, size_scale))\n",
    "                            for l_size in slide.level_dimensions)\n",
    "            size = _l_dimensions[level]\n",
    "            \n",
    "            slide4 = slide.read_region(start,level,size) \n",
    "    ####################################################################################\n",
    "    \n",
    "    \n",
    "    # is_tissue 부분 \n",
    "    slide4_grey = np.array(slide4.convert('L'))\n",
    "    binary = slide4_grey > 0  # black이면 0임\n",
    "    \n",
    "    # 검은색 제외하고 흰색영역(배경이라고 여겨지는)에 대해서도 작업해주어야함.\n",
    "    slide4_not_black = slide4_grey[slide4_grey>0]\n",
    "    thresh = threshold_otsu(slide4_not_black)\n",
    "    \n",
    "    I, J = slide4_grey.shape\n",
    "    for i in range(I):\n",
    "        for j in range(J):\n",
    "            if slide4_grey[i,j] > thresh :\n",
    "                binary[i,j] = False\n",
    "    patches = pd.DataFrame(pd.DataFrame(binary).stack())\n",
    "    patches['is_tissue'] = patches[0]\n",
    "    patches.drop(0, axis=1,inplace =True)\n",
    "    patches.loc[:,'slide_path'] = slide_path\n",
    "    \n",
    "\n",
    "    if slide_contains_tumor:\n",
    "        with openslide.open_slide(truth_path) as truth:\n",
    "            thumbnail_truth = truth.get_thumbnail(size) \n",
    "        \n",
    "        patches_y = pd.DataFrame(pd.DataFrame(np.array(thumbnail_truth.convert(\"L\"))).stack())\n",
    "        # !!\n",
    "        patches_y['is_tumor'] = patches_y[0] > 0\n",
    "        \n",
    "        # mask된 영역이 애매할 수도 있으므로\n",
    "        # !!\n",
    "        patches_y['is_all_tumor'] = patches_y[0] == 255\n",
    "        patches_y.drop(0, axis=1, inplace=True)\n",
    "        samples = pd.concat([patches, patches_y], axis=1) #len(samples)\n",
    "    else:\n",
    "        samples = patches\n",
    "        #dfmi.loc[:,('one','second')] = value\n",
    "        samples.loc[:,'is_tumor'] = False\n",
    "        samples.loc[:,'is_all_tumor'] = False\n",
    "    \n",
    "    if filter_non_tissue:\n",
    "        samples = samples[samples.is_tissue == True] # remove patches with no tissue #samples = samples[samples.is_tissue == True]\n",
    "    \n",
    "    if filter_only_all_tumor :\n",
    "        samples['tile_loc'] = list(samples.index)\n",
    "        all_tissue_samples1 = samples[samples.is_tumor==False]\n",
    "        all_tissue_samples1 = all_tissue_samples1.append(samples[samples.is_all_tumor==True])\n",
    "        \n",
    "        all_tissue_samples1.reset_index(inplace=True, drop=True)\n",
    "    else :\n",
    "        return samples\n",
    "    \n",
    "    return all_tissue_samples1\n",
    "\n",
    "\n",
    "NUM_CLASSES = 2 # not_tumor, tumor\n",
    "\n",
    "file_handles=[]\n",
    "def gen_imgs(all_image_path, all_mask_path, samples, batch_size, patch_size = PATCH_SIZE, shuffle=True):\n",
    "   \n",
    "    num_samples = len(samples)\n",
    "    # 특정 몇개의 slide만 open 해서 쓰기\n",
    "    # 4개씩 묶었으니까 \n",
    "  \n",
    "    slide_path0 = all_image_path[0]\n",
    "    slide_path1 = all_image_path[1]\n",
    "    slide_path2 = all_image_path[2]\n",
    "    slide_path3 = all_image_path[3]\n",
    "    \n",
    "    \n",
    "    # slide 0~3 까지 미리 열어두기\n",
    "    slide0 = openslide.open_slide(slide_path0)\n",
    "    slide1 = openslide.open_slide(slide_path1)\n",
    "    slide2 = openslide.open_slide(slide_path2)\n",
    "    slide3 = openslide.open_slide(slide_path3)\n",
    "    file_handles.append(slide0)\n",
    "    file_handles.append(slide1)\n",
    "    file_handles.append(slide2)\n",
    "    file_handles.append(slide3)\n",
    "    \n",
    "    # with openslide.open_slide(slide_path) as slide\n",
    "    tiles0 = DeepZoomGenerator(slide0,tile_size=patch_size, overlap=0, limit_bounds=False) \n",
    "    tiles1 = DeepZoomGenerator(slide1,tile_size=patch_size, overlap=0, limit_bounds=False)\n",
    "    tiles2 = DeepZoomGenerator(slide2,tile_size=patch_size, overlap=0, limit_bounds=False)\n",
    "    tiles3 = DeepZoomGenerator(slide3,tile_size=patch_size, overlap=0, limit_bounds=False)\n",
    "    \n",
    "    \n",
    "    if 'pos' in slide_path0:\n",
    "        start_x0 = int(slide0.properties.get('openslide.bounds-x',0))\n",
    "        start_y0 = int(slide0.properties.get('openslide.bounds-y',0))\n",
    "        start_x0 = start_x0 / patch_size\n",
    "        start_y0 = start_y0 / patch_size\n",
    "        \n",
    "        truth0 = openslide.open_slide(all_mask_path[0])\n",
    "        truth_tiles0 = DeepZoomGenerator(truth0, tile_size=32, overlap=0, limit_bounds=False) \n",
    "        \n",
    "    else : \n",
    "        start_x0 = 0\n",
    "        start_y0 = 0\n",
    "    \n",
    "    if 'pos' in slide_path1:\n",
    "        start_x1 = int(slide1.properties.get('openslide.bounds-x',0))\n",
    "        start_y1 = int(slide1.properties.get('openslide.bounds-y',0))\n",
    "        start_x1 = start_x1 / patch_size\n",
    "        start_y1 = start_y1 / patch_size\n",
    "        \n",
    "        truth1 = openslide.open_slide(all_mask_path[1])\n",
    "        truth_tiles1 = DeepZoomGenerator(truth1, tile_size=32, overlap=0, limit_bounds=False) \n",
    "        \n",
    "    else : \n",
    "        start_x1 = 0\n",
    "        start_y1 = 0\n",
    "    \n",
    "    if 'pos' in slide_path2:\n",
    "        start_x2 = int(slide2.properties.get('openslide.bounds-x',0))\n",
    "        start_y2 = int(slide2.properties.get('openslide.bounds-y',0))\n",
    "        start_x2 = start_x2 / patch_size\n",
    "        start_y2 = start_y2 / patch_size\n",
    "        \n",
    "        truth2 = openslide.open_slide(all_mask_path[2])\n",
    "        truth_tiles2 = DeepZoomGenerator(truth2, tile_size=32, overlap=0, limit_bounds=False) \n",
    "        \n",
    "    else : \n",
    "        start_x2 = 0\n",
    "        start_y2 = 0\n",
    "        \n",
    "    if 'pos' in slide_path3:\n",
    "        start_x3 = int(slide3.properties.get('openslide.bounds-x',0))\n",
    "        start_y3 = int(slide3.properties.get('openslide.bounds-y',0))\n",
    "        start_x3 = start_x3 / patch_size\n",
    "        start_y3 = start_y3 / patch_size\n",
    "        \n",
    "        truth3 = openslide.open_slide(all_mask_path[3])\n",
    "        truth_tiles3 = DeepZoomGenerator(truth3, tile_size=32, overlap=0, limit_bounds=False) \n",
    "        \n",
    "    else : \n",
    "        start_x3 = 0\n",
    "        start_y3 = 0\n",
    "    \n",
    "\n",
    "    \n",
    "    for epo in range(5): # Loop forever so the generator never terminates\n",
    "        if shuffle:\n",
    "            samples = samples.sample(frac=1) # shuffle samples\n",
    "\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples.iloc[offset:offset+batch_size]\n",
    "            images = []\n",
    "            masks = []\n",
    "            for _, batch_sample in batch_samples.iterrows(): # 배치마다 deep zoom 하네 약간 비효율적\n",
    "                \n",
    "                # 여기서 하나씩 4개 체크해서 해당되는 부분으로 가야지. for 4번 돌리면서 가야한다.\n",
    "                mask_size_up = np.zeros((patch_size,patch_size))\n",
    "                a,b=mask_size_up.shape\n",
    "                \n",
    "                if batch_sample.slide_path == slide_path0:\n",
    "                    x, y = batch_sample.tile_loc[::-1]\n",
    "                    x += start_x0\n",
    "                    y += start_y0\n",
    "                    img = tiles0.get_tile(tiles0.level_count-1, (x,y))\n",
    "                    if 'pos' in slide_path0:\n",
    "                        mask = truth_tiles0.get_tile(truth_tiles0.level_count-1, batch_sample.tile_loc[::-1])\n",
    "                        mask = (cv2.cvtColor(np.array(mask), cv2.COLOR_RGB2GRAY) > 0).astype(int)\n",
    "                            # mask_size_up , 32 to ,512\n",
    "                        for i in range(a):\n",
    "                            for j in range(b) :\n",
    "                                k = i//16\n",
    "                                l = j//16\n",
    "                                mask_size_up[i,j] = mask[k,l]\n",
    "                    \n",
    "                elif batch_sample.slide_path == slide_path1:\n",
    "                    x, y = batch_sample.tile_loc[::-1]\n",
    "                    x += start_x1\n",
    "                    y += start_y1\n",
    "                    img = tiles1.get_tile(tiles1.level_count-1, (x,y))\n",
    "                    if 'pos' in slide_path1:\n",
    "                        mask = truth_tiles1.get_tile(truth_tiles1.level_count-1, batch_sample.tile_loc[::-1])\n",
    "                        mask = (cv2.cvtColor(np.array(mask), cv2.COLOR_RGB2GRAY) > 0).astype(int)\n",
    "                            # mask_size_up , 16 to 512\n",
    "                        for i in range(a):\n",
    "                            for j in range(b) :\n",
    "                                k = i//16\n",
    "                                l = j//16\n",
    "                                mask_size_up[i,j] = mask[k,l]\n",
    "                \n",
    "                elif batch_sample.slide_path == slide_path2:\n",
    "                    x, y = batch_sample.tile_loc[::-1]\n",
    "                    x += start_x2\n",
    "                    y += start_y2\n",
    "                    img = tiles2.get_tile(tiles2.level_count-1, (x,y))\n",
    "                    if 'pos' in slide_path2:\n",
    "                        mask = truth_tiles2.get_tile(truth_tiles2.level_count-1, batch_sample.tile_loc[::-1])\n",
    "                        mask = (cv2.cvtColor(np.array(mask), cv2.COLOR_RGB2GRAY) > 0).astype(int)\n",
    "                            # mask_size_up , 16 to 256\n",
    "                        for i in range(a):\n",
    "                            for j in range(b) :\n",
    "                                k = i//16\n",
    "                                l = j//16\n",
    "                                mask_size_up[i,j] = mask[k,l]\n",
    "                \n",
    "                else:\n",
    "                    x, y = batch_sample.tile_loc[::-1]\n",
    "                    x += start_x3\n",
    "                    y += start_y3\n",
    "                    img = tiles3.get_tile(tiles3.level_count-1, (x,y))\n",
    "                    if 'pos' in slide_path3:\n",
    "                        mask = truth_tiles3.get_tile(truth_tiles3.level_count-1, batch_sample.tile_loc[::-1])\n",
    "                        mask = (cv2.cvtColor(np.array(mask), cv2.COLOR_RGB2GRAY) > 0).astype(int)\n",
    "                            # mask_size_up , 16 to 256\n",
    "                        for i in range(a):\n",
    "                            for j in range(b) :\n",
    "                                k = i//16\n",
    "                                l = j//16\n",
    "                                mask_size_up[i,j] = mask[k,l]\n",
    "                \n",
    "                    \n",
    "\n",
    "                images.append(np.array(img))\n",
    "                masks.append(mask_size_up)\n",
    "\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(masks)\n",
    "            #print('x_train_shape :', X_train.shape)\n",
    "            \n",
    "            y_train = to_categorical(y_train, num_classes=2).reshape(y_train.shape[0], patch_size, patch_size, 2) \n",
    "            #print('y_train_shape : ',y_train.shape)\n",
    "            \n",
    "            #X_train, y_train = datagen().flow(X_train,y = y_train,batch_size = batch_size)\n",
    "            X_train, y_train = next(ImageDataGenerator(\n",
    "                rotation_range=45,\n",
    "                horizontal_flip=True,\n",
    "                vertical_flip=True,\n",
    "                brightness_range =(0.4,1.)).flow(X_train,y=y_train,batch_size=batch_size))\n",
    "            #print(X_train.shape)\n",
    "            #print(y_train.shape)\n",
    "            yield X_train, y_train\n",
    "            \n",
    "def predict_batch_from_model(patches, model):\n",
    "    \"\"\"Predict which pixels are tumor.\n",
    "    \n",
    "    input: patch: `batch_size`x256x256x3, rgb image\n",
    "    input: model: keras model\n",
    "    output: prediction: 256x256x1, per-pixel tumor probability\n",
    "    \"\"\"\n",
    "    predictions = model.predict(patches)\n",
    "    predictions = predictions[:, :, :, 1]\n",
    "    return predictions\n",
    "def predict_from_model(patch, model):\n",
    "    \"\"\"Predict which pixels are tumor.\n",
    "    \n",
    "    input: patch: 256x256x3, rgb image\n",
    "    input: model: keras model\n",
    "    output: prediction: 256x256x1, per-pixel tumor probability\n",
    "    \"\"\"\n",
    "    \n",
    "    prediction = model.predict(patch.reshape(1, PATCH_SIZE, PATCH_SIZE, 3))\n",
    "    prediction = prediction[:, :, :, 1].reshape(PATCH_SIZE, PATCH_SIZE)\n",
    "    return prediction\n",
    "\n",
    "def predict_from_model_n(patch, model):\n",
    "    \"\"\"Predict which pixels are tumor.\n",
    "    \n",
    "    input: patch: 256x256x3, rgb image\n",
    "    input: model: keras model\n",
    "    output: prediction: 256x256x1, per-pixel tumor probability\n",
    "    \"\"\"\n",
    "    \n",
    "    prediction = model.predict(patch.reshape(1, PATCH_SIZE, PATCH_SIZE, 3))\n",
    "    prediction = prediction[:, :, :, 0].reshape(PATCH_SIZE, PATCH_SIZE)\n",
    "    return prediction\n",
    "\n",
    "def simple_model(pretrained_weights = None):\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(PATCH_SIZE, PATCH_SIZE, 3)))\n",
    "    model.add(Convolution2D(100, (3, 3), strides=(2, 2), activation='elu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Convolution2D(200, (3, 3), strides=(2, 2), activation='elu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Convolution2D(300, (3, 3), activation='elu', padding='same'))\n",
    "    model.add(Convolution2D(300, (3, 3), activation='elu',  padding='same'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Convolution2D(2, (1, 1))) # this is called upscore layer for some reason?\n",
    "    model.add(Conv2DTranspose(2, (31, 31), strides=(16, 16), activation='softmax', padding='same'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    if(pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_path # :  157\n",
      "mask_patch # :  157\n"
     ]
    }
   ],
   "source": [
    "def read_data_path():\n",
    "    image_paths = []\n",
    "    with open('train.txt','r') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip('\\n')\n",
    "            image_paths.append(line)\n",
    "    #print('image_path # : ',len(image_paths))\n",
    "\n",
    "    tumor_mask_paths = []\n",
    "\n",
    "    with open('train_mask.txt','r') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip('\\n')\n",
    "            tumor_mask_paths.append(line)\n",
    "    #print('mask_patch # : ',len(tumor_mask_paths))\n",
    "    \n",
    "    return image_paths, tumor_mask_paths\n",
    "\n",
    "def read_test_data_path():\n",
    "    image_paths = []\n",
    "    with open('test.txt','r') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip('\\n')\n",
    "            image_paths.append(line)\n",
    "    #print('image_path # : ',len(image_paths))\n",
    "    \n",
    "    return image_paths\n",
    "\n",
    "test_image_paths = read_test_data_path()\n",
    "image_paths, tumor_mask_paths = read_data_path()\n",
    "image_paths = []\n",
    "with open('train.txt','r') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip('\\n')\n",
    "        image_paths.append(line)\n",
    "print('image_path # : ',len(image_paths))\n",
    "\n",
    "tumor_mask_paths = []\n",
    "with open('train_mask.txt','r') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip('\\n')\n",
    "        tumor_mask_paths.append(line)\n",
    "print('mask_patch # : ',len(tumor_mask_paths))\n",
    "\n",
    "slide_4_list_1 = [[102,104,29,44],[144,55,30,18],[125,56,35,40],[54,65,21,36],[139,82,1,49],[73,108,7,23],[107,117,24,52],[106,103,27,13]\n",
    "               ,[105,151,15,2],[75,100,41,9],[156,113,32,37],[150,88,39,10],[84,122,5,50],[93,118,53,47],[87,78,45,34],[116,98,48,46],\n",
    "                [72,131,22,42]]\n",
    "slide_4_list_2 = [[109,58,14,28],[101,69,11,43],[94,74,3,20],[64,140,17,16],[92,154,8,26],[99,60,0,33],[86,146,25,19],[68,112,38,51],\n",
    "                 [71,136,31,4],[59,91,12,6]]\n",
    "slide_4_list_3 = [[143,132,124,85],[95,120,81,77],[97,96,110,83],[152,128,149,155],[153,111,57,138],[134,135,114,76],\n",
    "                  [123,90,121,61],[147,148,119,142],[66,137,63,80],[70,79,115,133],[129,141,127,145]]\n",
    "slide_4_test = [[55,55,0,0],[55,55,0,0]]\n",
    "\n",
    "all_image_path = []\n",
    "all_mask_path = []\n",
    "for j in range(4):\n",
    "    image_path = image_paths[slide_4_test[0][j]][1:] # 이 부분은 data 읽을때 고치자 ( [1:] 빼야함)\n",
    "    mask_path = tumor_mask_paths[slide_4_test[0][j]][1:] # 이 부분은 data 읽을때 고치자\n",
    "    all_image_path.append(image_path)\n",
    "    all_mask_path.append(mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simple_model(pretrained_weights='s_512.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth dimensions:  (5316, 10007)\n",
      "truth_4_dimension_size: (333, 626)\n",
      "slide4_size (167, 313)\n",
      "truth dimensions:  (5316, 10007)\n",
      "truth_4_dimension_size: (333, 626)\n",
      "slide4_size (167, 313)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14538\n",
      "39456\n",
      "Epoch 1/5\n",
      "  2/282 [..............................] - ETA: 49:14 - loss: 2.1324 - acc: 0.62 - ETA: 45:46 - loss: 1.7225 - acc: 0.6405"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.190649). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4/282 [..............................] - ETA: 44:10 - loss: 1.3583 - acc: 0.67 - ETA: 44:03 - loss: 1.1564 - acc: 0.7030"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.193651). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5/282 [..............................] - ETA: 43:51 - loss: 1.0286 - acc: 0.7368"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.300241). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6/282 [..............................] - ETA: 43:57 - loss: 0.9546 - acc: 0.7481"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.301746). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9/282 [..............................] - ETA: 43:21 - loss: 0.9083 - acc: 0.74 - ETA: 43:18 - loss: 0.8687 - acc: 0.74 - ETA: 43:14 - loss: 0.8414 - acc: 0.7449"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.303252). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12/282 [>.............................] - ETA: 43:02 - loss: 0.8171 - acc: 0.74 - ETA: 42:52 - loss: 0.7970 - acc: 0.74 - ETA: 41:30 - loss: 0.7845 - acc: 0.7382"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.326636). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14/282 [>.............................] - ETA: 40:24 - loss: 0.7682 - acc: 0.74 - ETA: 39:21 - loss: 0.7567 - acc: 0.7398"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.194144). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218/282 [======================>.......] - ETA: 38:23 - loss: 0.7470 - acc: 0.73 - ETA: 37:34 - loss: 0.7320 - acc: 0.74 - ETA: 36:46 - loss: 0.7225 - acc: 0.74 - ETA: 36:05 - loss: 0.7083 - acc: 0.75 - ETA: 35:30 - loss: 0.7062 - acc: 0.74 - ETA: 34:55 - loss: 0.7041 - acc: 0.74 - ETA: 34:24 - loss: 0.7019 - acc: 0.74 - ETA: 33:55 - loss: 0.6982 - acc: 0.73 - ETA: 33:30 - loss: 0.6989 - acc: 0.73 - ETA: 33:00 - loss: 0.6890 - acc: 0.73 - ETA: 32:38 - loss: 0.6820 - acc: 0.74 - ETA: 32:15 - loss: 0.6793 - acc: 0.73 - ETA: 31:54 - loss: 0.6770 - acc: 0.73 - ETA: 31:34 - loss: 0.6721 - acc: 0.73 - ETA: 31:15 - loss: 0.6682 - acc: 0.73 - ETA: 30:58 - loss: 0.6656 - acc: 0.73 - ETA: 30:42 - loss: 0.6611 - acc: 0.73 - ETA: 30:24 - loss: 0.6573 - acc: 0.73 - ETA: 30:08 - loss: 0.6546 - acc: 0.73 - ETA: 29:53 - loss: 0.6539 - acc: 0.73 - ETA: 29:38 - loss: 0.6534 - acc: 0.73 - ETA: 29:23 - loss: 0.6495 - acc: 0.73 - ETA: 29:10 - loss: 0.6441 - acc: 0.74 - ETA: 28:57 - loss: 0.6408 - acc: 0.74 - ETA: 28:44 - loss: 0.6375 - acc: 0.74 - ETA: 28:31 - loss: 0.6351 - acc: 0.74 - ETA: 28:20 - loss: 0.6336 - acc: 0.74 - ETA: 28:08 - loss: 0.6315 - acc: 0.74 - ETA: 27:56 - loss: 0.6302 - acc: 0.74 - ETA: 27:47 - loss: 0.6262 - acc: 0.74 - ETA: 27:37 - loss: 0.6289 - acc: 0.74 - ETA: 27:25 - loss: 0.6249 - acc: 0.74 - ETA: 27:15 - loss: 0.6234 - acc: 0.74 - ETA: 27:03 - loss: 0.6213 - acc: 0.74 - ETA: 26:53 - loss: 0.6222 - acc: 0.74 - ETA: 26:41 - loss: 0.6201 - acc: 0.74 - ETA: 26:32 - loss: 0.6243 - acc: 0.73 - ETA: 26:22 - loss: 0.6205 - acc: 0.73 - ETA: 26:11 - loss: 0.6229 - acc: 0.73 - ETA: 26:02 - loss: 0.6202 - acc: 0.73 - ETA: 25:52 - loss: 0.6198 - acc: 0.73 - ETA: 25:42 - loss: 0.6183 - acc: 0.73 - ETA: 25:33 - loss: 0.6181 - acc: 0.73 - ETA: 25:24 - loss: 0.6197 - acc: 0.73 - ETA: 25:14 - loss: 0.6205 - acc: 0.73 - ETA: 25:05 - loss: 0.6189 - acc: 0.73 - ETA: 24:56 - loss: 0.6186 - acc: 0.73 - ETA: 24:48 - loss: 0.6177 - acc: 0.73 - ETA: 24:39 - loss: 0.6173 - acc: 0.73 - ETA: 24:30 - loss: 0.6162 - acc: 0.73 - ETA: 24:22 - loss: 0.6162 - acc: 0.73 - ETA: 24:12 - loss: 0.6153 - acc: 0.73 - ETA: 24:03 - loss: 0.6141 - acc: 0.73 - ETA: 23:55 - loss: 0.6126 - acc: 0.73 - ETA: 23:47 - loss: 0.6133 - acc: 0.73 - ETA: 23:38 - loss: 0.6120 - acc: 0.73 - ETA: 23:29 - loss: 0.6100 - acc: 0.73 - ETA: 23:20 - loss: 0.6072 - acc: 0.73 - ETA: 23:13 - loss: 0.6061 - acc: 0.73 - ETA: 23:05 - loss: 0.6057 - acc: 0.73 - ETA: 22:58 - loss: 0.6060 - acc: 0.73 - ETA: 22:49 - loss: 0.6054 - acc: 0.73 - ETA: 22:41 - loss: 0.6035 - acc: 0.73 - ETA: 22:34 - loss: 0.6035 - acc: 0.73 - ETA: 22:25 - loss: 0.6028 - acc: 0.73 - ETA: 22:17 - loss: 0.6025 - acc: 0.73 - ETA: 22:10 - loss: 0.6021 - acc: 0.73 - ETA: 22:02 - loss: 0.6011 - acc: 0.73 - ETA: 21:54 - loss: 0.6007 - acc: 0.73 - ETA: 21:47 - loss: 0.5998 - acc: 0.73 - ETA: 21:39 - loss: 0.5993 - acc: 0.73 - ETA: 21:31 - loss: 0.5986 - acc: 0.72 - ETA: 21:24 - loss: 0.5983 - acc: 0.72 - ETA: 21:17 - loss: 0.5949 - acc: 0.73 - ETA: 21:10 - loss: 0.5925 - acc: 0.73 - ETA: 21:02 - loss: 0.5912 - acc: 0.73 - ETA: 20:55 - loss: 0.5890 - acc: 0.73 - ETA: 20:48 - loss: 0.5859 - acc: 0.73 - ETA: 20:40 - loss: 0.5845 - acc: 0.73 - ETA: 20:32 - loss: 0.5824 - acc: 0.73 - ETA: 20:25 - loss: 0.5816 - acc: 0.73 - ETA: 20:18 - loss: 0.5793 - acc: 0.73 - ETA: 20:11 - loss: 0.5787 - acc: 0.73 - ETA: 20:04 - loss: 0.5770 - acc: 0.73 - ETA: 19:57 - loss: 0.5754 - acc: 0.73 - ETA: 19:49 - loss: 0.5734 - acc: 0.73 - ETA: 19:42 - loss: 0.5706 - acc: 0.74 - ETA: 19:35 - loss: 0.5690 - acc: 0.74 - ETA: 19:27 - loss: 0.5706 - acc: 0.74 - ETA: 19:20 - loss: 0.5691 - acc: 0.74 - ETA: 19:13 - loss: 0.5679 - acc: 0.74 - ETA: 19:07 - loss: 0.5669 - acc: 0.74 - ETA: 18:59 - loss: 0.5651 - acc: 0.74 - ETA: 18:52 - loss: 0.5631 - acc: 0.74 - ETA: 18:45 - loss: 0.5656 - acc: 0.74 - ETA: 18:38 - loss: 0.5627 - acc: 0.74 - ETA: 18:31 - loss: 0.5620 - acc: 0.74 - ETA: 18:23 - loss: 0.5605 - acc: 0.74 - ETA: 18:16 - loss: 0.5587 - acc: 0.74 - ETA: 18:10 - loss: 0.5570 - acc: 0.74 - ETA: 18:03 - loss: 0.5554 - acc: 0.74 - ETA: 17:55 - loss: 0.5545 - acc: 0.74 - ETA: 17:49 - loss: 0.5524 - acc: 0.75 - ETA: 17:42 - loss: 0.5517 - acc: 0.75 - ETA: 17:35 - loss: 0.5503 - acc: 0.75 - ETA: 17:27 - loss: 0.5490 - acc: 0.75 - ETA: 17:21 - loss: 0.5474 - acc: 0.75 - ETA: 17:14 - loss: 0.5459 - acc: 0.75 - ETA: 17:07 - loss: 0.5454 - acc: 0.75 - ETA: 17:00 - loss: 0.5437 - acc: 0.75 - ETA: 16:53 - loss: 0.5427 - acc: 0.75 - ETA: 16:46 - loss: 0.5423 - acc: 0.75 - ETA: 16:39 - loss: 0.5418 - acc: 0.75 - ETA: 16:33 - loss: 0.5407 - acc: 0.75 - ETA: 16:26 - loss: 0.5401 - acc: 0.75 - ETA: 16:20 - loss: 0.5385 - acc: 0.75 - ETA: 16:13 - loss: 0.5374 - acc: 0.75 - ETA: 16:06 - loss: 0.5363 - acc: 0.75 - ETA: 15:59 - loss: 0.5360 - acc: 0.75 - ETA: 15:52 - loss: 0.5349 - acc: 0.75 - ETA: 15:45 - loss: 0.5350 - acc: 0.75 - ETA: 15:38 - loss: 0.5335 - acc: 0.75 - ETA: 15:32 - loss: 0.5326 - acc: 0.75 - ETA: 15:25 - loss: 0.5318 - acc: 0.75 - ETA: 15:18 - loss: 0.5314 - acc: 0.75 - ETA: 15:12 - loss: 0.5300 - acc: 0.76 - ETA: 15:05 - loss: 0.5293 - acc: 0.76 - ETA: 14:58 - loss: 0.5281 - acc: 0.76 - ETA: 14:52 - loss: 0.5271 - acc: 0.76 - ETA: 14:45 - loss: 0.5264 - acc: 0.76 - ETA: 14:38 - loss: 0.5241 - acc: 0.76 - ETA: 14:31 - loss: 0.5232 - acc: 0.76 - ETA: 14:25 - loss: 0.5217 - acc: 0.76 - ETA: 14:18 - loss: 0.5216 - acc: 0.76 - ETA: 14:12 - loss: 0.5199 - acc: 0.76 - ETA: 14:05 - loss: 0.5197 - acc: 0.76 - ETA: 13:58 - loss: 0.5190 - acc: 0.76 - ETA: 13:52 - loss: 0.5175 - acc: 0.76 - ETA: 13:45 - loss: 0.5161 - acc: 0.76 - ETA: 13:39 - loss: 0.5165 - acc: 0.76 - ETA: 13:32 - loss: 0.5159 - acc: 0.76 - ETA: 13:25 - loss: 0.5155 - acc: 0.76 - ETA: 13:19 - loss: 0.5148 - acc: 0.76 - ETA: 13:12 - loss: 0.5140 - acc: 0.76 - ETA: 13:06 - loss: 0.5144 - acc: 0.76 - ETA: 12:59 - loss: 0.5132 - acc: 0.76 - ETA: 12:52 - loss: 0.5126 - acc: 0.76 - ETA: 12:46 - loss: 0.5120 - acc: 0.76 - ETA: 12:40 - loss: 0.5107 - acc: 0.77 - ETA: 12:33 - loss: 0.5104 - acc: 0.77 - ETA: 12:26 - loss: 0.5104 - acc: 0.77 - ETA: 12:20 - loss: 0.5094 - acc: 0.77 - ETA: 12:13 - loss: 0.5084 - acc: 0.77 - ETA: 12:07 - loss: 0.5078 - acc: 0.77 - ETA: 12:00 - loss: 0.5061 - acc: 0.77 - ETA: 11:54 - loss: 0.5049 - acc: 0.77 - ETA: 11:47 - loss: 0.5045 - acc: 0.77 - ETA: 11:40 - loss: 0.5038 - acc: 0.77 - ETA: 11:33 - loss: 0.5037 - acc: 0.77 - ETA: 11:27 - loss: 0.5025 - acc: 0.77 - ETA: 11:20 - loss: 0.5017 - acc: 0.77 - ETA: 11:13 - loss: 0.5012 - acc: 0.77 - ETA: 11:07 - loss: 0.5006 - acc: 0.77 - ETA: 11:00 - loss: 0.4995 - acc: 0.77 - ETA: 10:54 - loss: 0.4989 - acc: 0.77 - ETA: 10:47 - loss: 0.4987 - acc: 0.77 - ETA: 10:41 - loss: 0.4976 - acc: 0.77 - ETA: 10:34 - loss: 0.4965 - acc: 0.77 - ETA: 10:28 - loss: 0.4956 - acc: 0.77 - ETA: 10:21 - loss: 0.4949 - acc: 0.77 - ETA: 10:15 - loss: 0.4949 - acc: 0.77 - ETA: 10:08 - loss: 0.4944 - acc: 0.77 - ETA: 10:02 - loss: 0.4939 - acc: 0.77 - ETA: 9:56 - loss: 0.4936 - acc: 0.7786 - ETA: 9:49 - loss: 0.4931 - acc: 0.778 - ETA: 9:42 - loss: 0.4922 - acc: 0.779 - ETA: 9:36 - loss: 0.4921 - acc: 0.779 - ETA: 9:29 - loss: 0.4923 - acc: 0.779 - ETA: 9:23 - loss: 0.4921 - acc: 0.779 - ETA: 9:16 - loss: 0.4915 - acc: 0.779 - ETA: 9:10 - loss: 0.4908 - acc: 0.779 - ETA: 9:03 - loss: 0.4903 - acc: 0.779 - ETA: 8:57 - loss: 0.4898 - acc: 0.780 - ETA: 8:50 - loss: 0.4889 - acc: 0.780 - ETA: 8:44 - loss: 0.4887 - acc: 0.780 - ETA: 8:38 - loss: 0.4879 - acc: 0.781 - ETA: 8:31 - loss: 0.4874 - acc: 0.781 - ETA: 8:25 - loss: 0.4865 - acc: 0.781 - ETA: 8:19 - loss: 0.4859 - acc: 0.782 - ETA: 8:12 - loss: 0.4850 - acc: 0.782 - ETA: 8:06 - loss: 0.4842 - acc: 0.783 - ETA: 7:59 - loss: 0.4841 - acc: 0.783 - ETA: 7:53 - loss: 0.4833 - acc: 0.783 - ETA: 7:47 - loss: 0.4824 - acc: 0.784 - ETA: 7:40 - loss: 0.4810 - acc: 0.784 - ETA: 7:34 - loss: 0.4799 - acc: 0.785 - ETA: 7:27 - loss: 0.4804 - acc: 0.785 - ETA: 7:21 - loss: 0.4799 - acc: 0.785 - ETA: 7:15 - loss: 0.4786 - acc: 0.786 - ETA: 7:08 - loss: 0.4783 - acc: 0.786 - ETA: 7:02 - loss: 0.4776 - acc: 0.786 - ETA: 6:56 - loss: 0.4772 - acc: 0.786 - ETA: 6:49 - loss: 0.4766 - acc: 0.787 - ETA: 6:43 - loss: 0.4755 - acc: 0.7877"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - ETA: 6:36 - loss: 0.4759 - acc: 0.787 - ETA: 6:30 - loss: 0.4751 - acc: 0.787 - ETA: 6:24 - loss: 0.4742 - acc: 0.788 - ETA: 6:17 - loss: 0.4740 - acc: 0.788 - ETA: 6:11 - loss: 0.4737 - acc: 0.788 - ETA: 6:05 - loss: 0.4736 - acc: 0.788 - ETA: 5:58 - loss: 0.4736 - acc: 0.788 - ETA: 5:52 - loss: 0.4737 - acc: 0.788 - ETA: 5:46 - loss: 0.4738 - acc: 0.788 - ETA: 5:39 - loss: 0.4729 - acc: 0.789 - ETA: 5:33 - loss: 0.4727 - acc: 0.789 - ETA: 5:27 - loss: 0.4722 - acc: 0.789 - ETA: 5:20 - loss: 0.4720 - acc: 0.789 - ETA: 5:14 - loss: 0.4718 - acc: 0.789 - ETA: 5:08 - loss: 0.4716 - acc: 0.789 - ETA: 5:01 - loss: 0.4709 - acc: 0.790 - ETA: 4:55 - loss: 0.4704 - acc: 0.790 - ETA: 4:49 - loss: 0.4695 - acc: 0.791 - ETA: 4:42 - loss: 0.4688 - acc: 0.791 - ETA: 4:36 - loss: 0.4683 - acc: 0.791 - ETA: 4:30 - loss: 0.4691 - acc: 0.791 - ETA: 4:23 - loss: 0.4683 - acc: 0.791 - ETA: 4:17 - loss: 0.4678 - acc: 0.791 - ETA: 4:11 - loss: 0.4676 - acc: 0.791 - ETA: 4:04 - loss: 0.4669 - acc: 0.792 - ETA: 3:58 - loss: 0.4665 - acc: 0.792 - ETA: 3:52 - loss: 0.4657 - acc: 0.792 - ETA: 3:45 - loss: 0.4652 - acc: 0.793 - ETA: 3:39 - loss: 0.4649 - acc: 0.793 - ETA: 3:33 - loss: 0.4642 - acc: 0.793 - ETA: 3:26 - loss: 0.4636 - acc: 0.793 - ETA: 3:20 - loss: 0.4628 - acc: 0.794 - ETA: 3:14 - loss: 0.4625 - acc: 0.794 - ETA: 3:08 - loss: 0.4623 - acc: 0.794 - ETA: 3:01 - loss: 0.4616 - acc: 0.795 - ETA: 2:55 - loss: 0.4613 - acc: 0.795 - ETA: 2:49 - loss: 0.4606 - acc: 0.795 - ETA: 2:42 - loss: 0.4603 - acc: 0.795 - ETA: 2:36 - loss: 0.4600 - acc: 0.795 - ETA: 2:30 - loss: 0.4601 - acc: 0.795 - ETA: 2:23 - loss: 0.4597 - acc: 0.795 - ETA: 2:17 - loss: 0.4596 - acc: 0.795 - ETA: 2:11 - loss: 0.4591 - acc: 0.796 - ETA: 2:05 - loss: 0.4586 - acc: 0.796 - ETA: 1:58 - loss: 0.4580 - acc: 0.796 - ETA: 1:52 - loss: 0.4576 - acc: 0.796 - ETA: 1:46 - loss: 0.4574 - acc: 0.797 - ETA: 1:40 - loss: 0.4571 - acc: 0.797 - ETA: 1:33 - loss: 0.4566 - acc: 0.797 - ETA: 1:27 - loss: 0.4560 - acc: 0.797 - ETA: 1:21 - loss: 0.4554 - acc: 0.798 - ETA: 1:15 - loss: 0.4550 - acc: 0.798 - ETA: 1:08 - loss: 0.4545 - acc: 0.798 - ETA: 1:02 - loss: 0.4537 - acc: 0.799 - ETA: 56s - loss: 0.4536 - acc: 0.799 - ETA: 49s - loss: 0.4536 - acc: 0.79 - ETA: 43s - loss: 0.4532 - acc: 0.79 - ETA: 37s - loss: 0.4531 - acc: 0.79 - ETA: 31s - loss: 0.4529 - acc: 0.79 - ETA: 24s - loss: 0.4529 - acc: 0.79 - ETA: 18s - loss: 0.4525 - acc: 0.79 - ETA: 12s - loss: 0.4517 - acc: 0.80 - ETA: 6s - loss: 0.4512 - acc: 0.8004 - 1908s 7s/step - loss: 0.4516 - acc: 0.8003 - val_loss: 0.3221 - val_acc: 0.8720\n",
      "Epoch 2/5\n",
      " 14/282 [>.............................] - ETA: 55s - loss: 0.3787 - acc: 0.81 - ETA: 1:00 - loss: 0.3412 - acc: 0.851 - ETA: 1:02 - loss: 0.3172 - acc: 0.869 - ETA: 1:03 - loss: 0.3192 - acc: 0.873 - ETA: 1:05 - loss: 0.3250 - acc: 0.869 - ETA: 1:19 - loss: 0.3174 - acc: 0.874 - ETA: 1:16 - loss: 0.3141 - acc: 0.874 - ETA: 1:15 - loss: 0.3144 - acc: 0.875 - ETA: 1:24 - loss: 0.3190 - acc: 0.873 - ETA: 1:27 - loss: 0.3096 - acc: 0.877 - ETA: 1:29 - loss: 0.3127 - acc: 0.876 - ETA: 3:29 - loss: 0.3204 - acc: 0.874 - ETA: 6:23 - loss: 0.3168 - acc: 0.875 - ETA: 8:42 - loss: 0.3158 - acc: 0.8751"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.228811). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16/282 [>.............................] - ETA: 10:57 - loss: 0.3102 - acc: 0.87 - ETA: 12:43 - loss: 0.3112 - acc: 0.8772"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.280954). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17/282 [>.............................] - ETA: 14:13 - loss: 0.3164 - acc: 0.8745"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.338881). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19/282 [=>............................] - ETA: 15:27 - loss: 0.3151 - acc: 0.87 - ETA: 16:40 - loss: 0.3188 - acc: 0.8719"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.368808). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20/282 [=>............................] - ETA: 17:43 - loss: 0.3179 - acc: 0.8718"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.376802). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 24/282 [=>............................] - ETA: 18:37 - loss: 0.3200 - acc: 0.87 - ETA: 19:15 - loss: 0.3175 - acc: 0.87 - ETA: 19:28 - loss: 0.3171 - acc: 0.87 - ETA: 19:38 - loss: 0.3184 - acc: 0.8697"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.346875). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25/282 [=>............................] - ETA: 19:47 - loss: 0.3227 - acc: 0.8676"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.285831). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229/282 [=======================>......] - ETA: 19:53 - loss: 0.3197 - acc: 0.86 - ETA: 19:59 - loss: 0.3153 - acc: 0.87 - ETA: 20:06 - loss: 0.3129 - acc: 0.87 - ETA: 20:13 - loss: 0.3132 - acc: 0.87 - ETA: 20:17 - loss: 0.3098 - acc: 0.87 - ETA: 20:20 - loss: 0.3084 - acc: 0.87 - ETA: 20:21 - loss: 0.3088 - acc: 0.87 - ETA: 20:23 - loss: 0.3108 - acc: 0.87 - ETA: 20:26 - loss: 0.3102 - acc: 0.87 - ETA: 20:28 - loss: 0.3108 - acc: 0.87 - ETA: 20:30 - loss: 0.3101 - acc: 0.87 - ETA: 20:32 - loss: 0.3115 - acc: 0.87 - ETA: 20:32 - loss: 0.3104 - acc: 0.87 - ETA: 20:34 - loss: 0.3110 - acc: 0.87 - ETA: 20:35 - loss: 0.3122 - acc: 0.87 - ETA: 20:35 - loss: 0.3128 - acc: 0.87 - ETA: 20:35 - loss: 0.3147 - acc: 0.87 - ETA: 20:34 - loss: 0.3151 - acc: 0.87 - ETA: 20:34 - loss: 0.3134 - acc: 0.87 - ETA: 20:34 - loss: 0.3116 - acc: 0.87 - ETA: 20:31 - loss: 0.3108 - acc: 0.87 - ETA: 20:30 - loss: 0.3141 - acc: 0.87 - ETA: 20:28 - loss: 0.3126 - acc: 0.87 - ETA: 20:26 - loss: 0.3119 - acc: 0.87 - ETA: 20:25 - loss: 0.3124 - acc: 0.87 - ETA: 20:23 - loss: 0.3103 - acc: 0.87 - ETA: 20:21 - loss: 0.3108 - acc: 0.87 - ETA: 20:19 - loss: 0.3092 - acc: 0.87 - ETA: 20:15 - loss: 0.3085 - acc: 0.87 - ETA: 20:13 - loss: 0.3089 - acc: 0.87 - ETA: 20:07 - loss: 0.3122 - acc: 0.87 - ETA: 20:04 - loss: 0.3100 - acc: 0.87 - ETA: 20:02 - loss: 0.3140 - acc: 0.87 - ETA: 19:59 - loss: 0.3126 - acc: 0.87 - ETA: 19:56 - loss: 0.3122 - acc: 0.87 - ETA: 19:54 - loss: 0.3111 - acc: 0.87 - ETA: 19:50 - loss: 0.3100 - acc: 0.87 - ETA: 19:47 - loss: 0.3111 - acc: 0.87 - ETA: 19:44 - loss: 0.3110 - acc: 0.87 - ETA: 19:40 - loss: 0.3121 - acc: 0.87 - ETA: 19:37 - loss: 0.3133 - acc: 0.87 - ETA: 19:33 - loss: 0.3134 - acc: 0.87 - ETA: 19:29 - loss: 0.3140 - acc: 0.87 - ETA: 19:25 - loss: 0.3131 - acc: 0.87 - ETA: 19:20 - loss: 0.3138 - acc: 0.87 - ETA: 19:18 - loss: 0.3133 - acc: 0.87 - ETA: 19:14 - loss: 0.3137 - acc: 0.87 - ETA: 19:10 - loss: 0.3127 - acc: 0.87 - ETA: 19:05 - loss: 0.3129 - acc: 0.87 - ETA: 19:01 - loss: 0.3157 - acc: 0.87 - ETA: 18:57 - loss: 0.3153 - acc: 0.87 - ETA: 18:53 - loss: 0.3149 - acc: 0.87 - ETA: 18:48 - loss: 0.3130 - acc: 0.87 - ETA: 18:44 - loss: 0.3130 - acc: 0.87 - ETA: 18:40 - loss: 0.3129 - acc: 0.87 - ETA: 18:36 - loss: 0.3127 - acc: 0.87 - ETA: 18:31 - loss: 0.3119 - acc: 0.87 - ETA: 18:26 - loss: 0.3106 - acc: 0.87 - ETA: 18:21 - loss: 0.3123 - acc: 0.87 - ETA: 18:17 - loss: 0.3144 - acc: 0.87 - ETA: 18:12 - loss: 0.3133 - acc: 0.87 - ETA: 18:07 - loss: 0.3135 - acc: 0.87 - ETA: 18:03 - loss: 0.3145 - acc: 0.87 - ETA: 17:58 - loss: 0.3141 - acc: 0.87 - ETA: 17:53 - loss: 0.3132 - acc: 0.87 - ETA: 17:48 - loss: 0.3129 - acc: 0.87 - ETA: 17:43 - loss: 0.3131 - acc: 0.87 - ETA: 17:38 - loss: 0.3155 - acc: 0.87 - ETA: 17:33 - loss: 0.3161 - acc: 0.87 - ETA: 17:29 - loss: 0.3154 - acc: 0.87 - ETA: 17:23 - loss: 0.3159 - acc: 0.87 - ETA: 17:18 - loss: 0.3156 - acc: 0.87 - ETA: 17:13 - loss: 0.3167 - acc: 0.87 - ETA: 17:08 - loss: 0.3179 - acc: 0.86 - ETA: 17:03 - loss: 0.3174 - acc: 0.86 - ETA: 16:58 - loss: 0.3166 - acc: 0.87 - ETA: 16:54 - loss: 0.3162 - acc: 0.87 - ETA: 16:49 - loss: 0.3159 - acc: 0.87 - ETA: 16:44 - loss: 0.3158 - acc: 0.87 - ETA: 16:39 - loss: 0.3152 - acc: 0.87 - ETA: 16:34 - loss: 0.3152 - acc: 0.87 - ETA: 16:29 - loss: 0.3148 - acc: 0.87 - ETA: 16:24 - loss: 0.3152 - acc: 0.87 - ETA: 16:19 - loss: 0.3156 - acc: 0.87 - ETA: 16:13 - loss: 0.3161 - acc: 0.86 - ETA: 16:08 - loss: 0.3156 - acc: 0.86 - ETA: 16:03 - loss: 0.3146 - acc: 0.87 - ETA: 15:57 - loss: 0.3165 - acc: 0.87 - ETA: 15:52 - loss: 0.3158 - acc: 0.87 - ETA: 15:46 - loss: 0.3170 - acc: 0.86 - ETA: 15:41 - loss: 0.3157 - acc: 0.87 - ETA: 15:36 - loss: 0.3162 - acc: 0.87 - ETA: 15:30 - loss: 0.3159 - acc: 0.87 - ETA: 15:25 - loss: 0.3162 - acc: 0.86 - ETA: 15:20 - loss: 0.3164 - acc: 0.86 - ETA: 15:14 - loss: 0.3170 - acc: 0.86 - ETA: 15:09 - loss: 0.3169 - acc: 0.86 - ETA: 15:03 - loss: 0.3189 - acc: 0.86 - ETA: 14:58 - loss: 0.3182 - acc: 0.86 - ETA: 14:52 - loss: 0.3176 - acc: 0.86 - ETA: 14:47 - loss: 0.3173 - acc: 0.86 - ETA: 14:42 - loss: 0.3170 - acc: 0.87 - ETA: 14:36 - loss: 0.3163 - acc: 0.87 - ETA: 14:31 - loss: 0.3163 - acc: 0.87 - ETA: 14:25 - loss: 0.3160 - acc: 0.87 - ETA: 14:20 - loss: 0.3154 - acc: 0.87 - ETA: 14:14 - loss: 0.3149 - acc: 0.87 - ETA: 14:10 - loss: 0.3158 - acc: 0.87 - ETA: 14:04 - loss: 0.3165 - acc: 0.87 - ETA: 13:59 - loss: 0.3171 - acc: 0.87 - ETA: 13:53 - loss: 0.3183 - acc: 0.86 - ETA: 13:48 - loss: 0.3180 - acc: 0.86 - ETA: 13:42 - loss: 0.3175 - acc: 0.87 - ETA: 13:36 - loss: 0.3170 - acc: 0.87 - ETA: 13:31 - loss: 0.3162 - acc: 0.87 - ETA: 13:26 - loss: 0.3154 - acc: 0.87 - ETA: 13:20 - loss: 0.3162 - acc: 0.87 - ETA: 13:15 - loss: 0.3167 - acc: 0.87 - ETA: 13:09 - loss: 0.3169 - acc: 0.87 - ETA: 13:04 - loss: 0.3161 - acc: 0.87 - ETA: 12:58 - loss: 0.3158 - acc: 0.87 - ETA: 12:53 - loss: 0.3160 - acc: 0.87 - ETA: 12:47 - loss: 0.3161 - acc: 0.87 - ETA: 12:41 - loss: 0.3156 - acc: 0.87 - ETA: 12:36 - loss: 0.3151 - acc: 0.87 - ETA: 12:30 - loss: 0.3150 - acc: 0.87 - ETA: 12:25 - loss: 0.3151 - acc: 0.87 - ETA: 12:19 - loss: 0.3151 - acc: 0.87 - ETA: 12:13 - loss: 0.3158 - acc: 0.87 - ETA: 12:08 - loss: 0.3152 - acc: 0.87 - ETA: 12:02 - loss: 0.3148 - acc: 0.87 - ETA: 11:57 - loss: 0.3145 - acc: 0.87 - ETA: 11:51 - loss: 0.3142 - acc: 0.87 - ETA: 11:46 - loss: 0.3146 - acc: 0.87 - ETA: 11:40 - loss: 0.3144 - acc: 0.87 - ETA: 11:35 - loss: 0.3141 - acc: 0.87 - ETA: 11:29 - loss: 0.3144 - acc: 0.87 - ETA: 11:24 - loss: 0.3135 - acc: 0.87 - ETA: 11:18 - loss: 0.3142 - acc: 0.87 - ETA: 11:12 - loss: 0.3140 - acc: 0.87 - ETA: 11:07 - loss: 0.3137 - acc: 0.87 - ETA: 11:01 - loss: 0.3132 - acc: 0.87 - ETA: 10:55 - loss: 0.3133 - acc: 0.87 - ETA: 10:50 - loss: 0.3129 - acc: 0.87 - ETA: 10:44 - loss: 0.3127 - acc: 0.87 - ETA: 10:38 - loss: 0.3125 - acc: 0.87 - ETA: 10:32 - loss: 0.3120 - acc: 0.87 - ETA: 10:26 - loss: 0.3122 - acc: 0.87 - ETA: 10:21 - loss: 0.3141 - acc: 0.87 - ETA: 10:15 - loss: 0.3142 - acc: 0.87 - ETA: 10:09 - loss: 0.3134 - acc: 0.87 - ETA: 10:04 - loss: 0.3128 - acc: 0.87 - ETA: 9:58 - loss: 0.3128 - acc: 0.8721 - ETA: 9:52 - loss: 0.3127 - acc: 0.872 - ETA: 9:47 - loss: 0.3118 - acc: 0.872 - ETA: 9:41 - loss: 0.3119 - acc: 0.872 - ETA: 9:35 - loss: 0.3120 - acc: 0.872 - ETA: 9:29 - loss: 0.3119 - acc: 0.872 - ETA: 9:24 - loss: 0.3120 - acc: 0.872 - ETA: 9:18 - loss: 0.3120 - acc: 0.872 - ETA: 9:12 - loss: 0.3119 - acc: 0.872 - ETA: 9:06 - loss: 0.3116 - acc: 0.872 - ETA: 9:01 - loss: 0.3117 - acc: 0.872 - ETA: 8:55 - loss: 0.3113 - acc: 0.872 - ETA: 8:50 - loss: 0.3113 - acc: 0.872 - ETA: 8:44 - loss: 0.3112 - acc: 0.872 - ETA: 8:38 - loss: 0.3112 - acc: 0.872 - ETA: 8:33 - loss: 0.3107 - acc: 0.873 - ETA: 8:27 - loss: 0.3107 - acc: 0.873 - ETA: 8:21 - loss: 0.3108 - acc: 0.873 - ETA: 8:15 - loss: 0.3109 - acc: 0.873 - ETA: 8:10 - loss: 0.3104 - acc: 0.873 - ETA: 8:04 - loss: 0.3102 - acc: 0.873 - ETA: 7:58 - loss: 0.3096 - acc: 0.873 - ETA: 7:52 - loss: 0.3096 - acc: 0.873 - ETA: 7:47 - loss: 0.3097 - acc: 0.873 - ETA: 7:41 - loss: 0.3095 - acc: 0.873 - ETA: 7:35 - loss: 0.3095 - acc: 0.873 - ETA: 7:29 - loss: 0.3096 - acc: 0.873 - ETA: 7:24 - loss: 0.3097 - acc: 0.873 - ETA: 7:18 - loss: 0.3093 - acc: 0.873 - ETA: 7:12 - loss: 0.3093 - acc: 0.873 - ETA: 7:07 - loss: 0.3092 - acc: 0.873 - ETA: 7:01 - loss: 0.3090 - acc: 0.873 - ETA: 6:55 - loss: 0.3090 - acc: 0.873 - ETA: 6:50 - loss: 0.3085 - acc: 0.873 - ETA: 6:44 - loss: 0.3086 - acc: 0.873 - ETA: 6:38 - loss: 0.3082 - acc: 0.873 - ETA: 6:32 - loss: 0.3075 - acc: 0.874 - ETA: 6:26 - loss: 0.3073 - acc: 0.874 - ETA: 6:21 - loss: 0.3077 - acc: 0.874 - ETA: 6:15 - loss: 0.3077 - acc: 0.874 - ETA: 6:09 - loss: 0.3075 - acc: 0.874 - ETA: 6:03 - loss: 0.3075 - acc: 0.874 - ETA: 5:58 - loss: 0.3071 - acc: 0.874 - ETA: 5:52 - loss: 0.3070 - acc: 0.874 - ETA: 5:46 - loss: 0.3066 - acc: 0.874 - ETA: 5:41 - loss: 0.3064 - acc: 0.874 - ETA: 5:35 - loss: 0.3061 - acc: 0.875 - ETA: 5:29 - loss: 0.3058 - acc: 0.875 - ETA: 5:23 - loss: 0.3058 - acc: 0.875 - ETA: 5:18 - loss: 0.3060 - acc: 0.875 - ETA: 5:12 - loss: 0.3057 - acc: 0.875 - ETA: 5:06 - loss: 0.3055 - acc: 0.8752"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - ETA: 5:00 - loss: 0.3051 - acc: 0.875 - ETA: 4:55 - loss: 0.3059 - acc: 0.875 - ETA: 4:49 - loss: 0.3059 - acc: 0.875 - ETA: 4:43 - loss: 0.3062 - acc: 0.874 - ETA: 4:37 - loss: 0.3071 - acc: 0.874 - ETA: 4:31 - loss: 0.3071 - acc: 0.874 - ETA: 4:26 - loss: 0.3073 - acc: 0.874 - ETA: 4:20 - loss: 0.3075 - acc: 0.874 - ETA: 4:14 - loss: 0.3074 - acc: 0.874 - ETA: 4:09 - loss: 0.3073 - acc: 0.874 - ETA: 4:03 - loss: 0.3073 - acc: 0.874 - ETA: 3:57 - loss: 0.3072 - acc: 0.874 - ETA: 3:51 - loss: 0.3073 - acc: 0.874 - ETA: 3:45 - loss: 0.3074 - acc: 0.874 - ETA: 3:40 - loss: 0.3072 - acc: 0.874 - ETA: 3:34 - loss: 0.3071 - acc: 0.874 - ETA: 3:28 - loss: 0.3070 - acc: 0.874 - ETA: 3:22 - loss: 0.3069 - acc: 0.874 - ETA: 3:16 - loss: 0.3067 - acc: 0.874 - ETA: 3:11 - loss: 0.3064 - acc: 0.874 - ETA: 3:05 - loss: 0.3058 - acc: 0.875 - ETA: 2:59 - loss: 0.3068 - acc: 0.874 - ETA: 2:53 - loss: 0.3067 - acc: 0.874 - ETA: 2:48 - loss: 0.3070 - acc: 0.874 - ETA: 2:42 - loss: 0.3074 - acc: 0.874 - ETA: 2:36 - loss: 0.3074 - acc: 0.874 - ETA: 2:30 - loss: 0.3074 - acc: 0.874 - ETA: 2:25 - loss: 0.3071 - acc: 0.874 - ETA: 2:19 - loss: 0.3069 - acc: 0.874 - ETA: 2:13 - loss: 0.3064 - acc: 0.874 - ETA: 2:07 - loss: 0.3066 - acc: 0.874 - ETA: 2:01 - loss: 0.3066 - acc: 0.874 - ETA: 1:56 - loss: 0.3065 - acc: 0.874 - ETA: 1:50 - loss: 0.3065 - acc: 0.874 - ETA: 1:44 - loss: 0.3062 - acc: 0.875 - ETA: 1:38 - loss: 0.3061 - acc: 0.875 - ETA: 1:32 - loss: 0.3058 - acc: 0.875 - ETA: 1:27 - loss: 0.3054 - acc: 0.875 - ETA: 1:21 - loss: 0.3050 - acc: 0.875 - ETA: 1:15 - loss: 0.3046 - acc: 0.875 - ETA: 1:09 - loss: 0.3041 - acc: 0.875 - ETA: 1:03 - loss: 0.3038 - acc: 0.875 - ETA: 58s - loss: 0.3037 - acc: 0.876 - ETA: 52s - loss: 0.3037 - acc: 0.87 - ETA: 46s - loss: 0.3037 - acc: 0.87 - ETA: 40s - loss: 0.3034 - acc: 0.87 - ETA: 34s - loss: 0.3032 - acc: 0.87 - ETA: 29s - loss: 0.3028 - acc: 0.87 - ETA: 23s - loss: 0.3023 - acc: 0.87 - ETA: 17s - loss: 0.3021 - acc: 0.87 - ETA: 11s - loss: 0.3018 - acc: 0.87 - ETA: 5s - loss: 0.3017 - acc: 0.8770 - 1790s 6s/step - loss: 0.3015 - acc: 0.8770 - val_loss: 0.2534 - val_acc: 0.8972\n",
      "Epoch 3/5\n",
      "  1/282 [..............................] - ETA: 55s - loss: 0.2684 - acc: 0.8958"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.224472). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2/282 [..............................] - ETA: 1:36 - loss: 0.2501 - acc: 0.8975"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.222850). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4/282 [..............................] - ETA: 1:43 - loss: 0.2427 - acc: 0.905 - ETA: 1:54 - loss: 0.2508 - acc: 0.9005"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.267493). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7/282 [..............................] - ETA: 2:06 - loss: 0.2448 - acc: 0.902 - ETA: 1:54 - loss: 0.2736 - acc: 0.898 - ETA: 1:48 - loss: 0.2585 - acc: 0.9043"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.221229). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211/282 [=====================>........] - ETA: 1:42 - loss: 0.2595 - acc: 0.900 - ETA: 1:41 - loss: 0.2764 - acc: 0.892 - ETA: 1:40 - loss: 0.2806 - acc: 0.890 - ETA: 1:37 - loss: 0.2767 - acc: 0.891 - ETA: 3:31 - loss: 0.2784 - acc: 0.888 - ETA: 6:15 - loss: 0.2770 - acc: 0.888 - ETA: 8:38 - loss: 0.2739 - acc: 0.889 - ETA: 10:40 - loss: 0.2675 - acc: 0.89 - ETA: 12:27 - loss: 0.2716 - acc: 0.89 - ETA: 14:00 - loss: 0.2789 - acc: 0.88 - ETA: 15:20 - loss: 0.2837 - acc: 0.88 - ETA: 16:35 - loss: 0.2881 - acc: 0.88 - ETA: 17:34 - loss: 0.2881 - acc: 0.88 - ETA: 18:26 - loss: 0.2901 - acc: 0.88 - ETA: 19:10 - loss: 0.2901 - acc: 0.88 - ETA: 19:24 - loss: 0.2900 - acc: 0.88 - ETA: 19:34 - loss: 0.2866 - acc: 0.88 - ETA: 19:42 - loss: 0.2934 - acc: 0.87 - ETA: 19:56 - loss: 0.2948 - acc: 0.87 - ETA: 20:04 - loss: 0.2913 - acc: 0.87 - ETA: 20:12 - loss: 0.2931 - acc: 0.87 - ETA: 20:19 - loss: 0.2909 - acc: 0.87 - ETA: 20:24 - loss: 0.2910 - acc: 0.87 - ETA: 20:30 - loss: 0.2921 - acc: 0.87 - ETA: 20:37 - loss: 0.2899 - acc: 0.87 - ETA: 20:44 - loss: 0.2872 - acc: 0.88 - ETA: 20:47 - loss: 0.2873 - acc: 0.88 - ETA: 20:51 - loss: 0.2857 - acc: 0.88 - ETA: 20:58 - loss: 0.2845 - acc: 0.88 - ETA: 21:01 - loss: 0.2856 - acc: 0.88 - ETA: 21:03 - loss: 0.2844 - acc: 0.88 - ETA: 21:04 - loss: 0.2827 - acc: 0.88 - ETA: 21:05 - loss: 0.2812 - acc: 0.88 - ETA: 21:03 - loss: 0.2823 - acc: 0.88 - ETA: 21:04 - loss: 0.2792 - acc: 0.88 - ETA: 21:04 - loss: 0.2815 - acc: 0.88 - ETA: 21:03 - loss: 0.2822 - acc: 0.88 - ETA: 21:03 - loss: 0.2814 - acc: 0.88 - ETA: 21:01 - loss: 0.2790 - acc: 0.88 - ETA: 21:01 - loss: 0.2812 - acc: 0.88 - ETA: 21:01 - loss: 0.2825 - acc: 0.88 - ETA: 21:00 - loss: 0.2832 - acc: 0.88 - ETA: 20:59 - loss: 0.2838 - acc: 0.88 - ETA: 20:57 - loss: 0.2835 - acc: 0.88 - ETA: 20:54 - loss: 0.2824 - acc: 0.88 - ETA: 20:52 - loss: 0.2812 - acc: 0.88 - ETA: 20:50 - loss: 0.2832 - acc: 0.88 - ETA: 20:46 - loss: 0.2828 - acc: 0.88 - ETA: 20:43 - loss: 0.2820 - acc: 0.88 - ETA: 20:39 - loss: 0.2818 - acc: 0.88 - ETA: 20:35 - loss: 0.2822 - acc: 0.88 - ETA: 20:33 - loss: 0.2827 - acc: 0.88 - ETA: 20:30 - loss: 0.2838 - acc: 0.88 - ETA: 20:27 - loss: 0.2844 - acc: 0.88 - ETA: 20:23 - loss: 0.2836 - acc: 0.88 - ETA: 20:20 - loss: 0.2835 - acc: 0.88 - ETA: 20:16 - loss: 0.2841 - acc: 0.88 - ETA: 20:12 - loss: 0.2836 - acc: 0.88 - ETA: 20:08 - loss: 0.2856 - acc: 0.88 - ETA: 20:05 - loss: 0.2842 - acc: 0.88 - ETA: 20:00 - loss: 0.2865 - acc: 0.88 - ETA: 19:56 - loss: 0.2882 - acc: 0.88 - ETA: 19:51 - loss: 0.2884 - acc: 0.88 - ETA: 19:47 - loss: 0.2901 - acc: 0.88 - ETA: 19:43 - loss: 0.2902 - acc: 0.88 - ETA: 19:38 - loss: 0.2902 - acc: 0.88 - ETA: 19:34 - loss: 0.2904 - acc: 0.88 - ETA: 19:31 - loss: 0.2908 - acc: 0.88 - ETA: 19:26 - loss: 0.2893 - acc: 0.88 - ETA: 19:22 - loss: 0.2900 - acc: 0.88 - ETA: 19:17 - loss: 0.2900 - acc: 0.88 - ETA: 19:13 - loss: 0.2893 - acc: 0.88 - ETA: 19:08 - loss: 0.2893 - acc: 0.88 - ETA: 19:03 - loss: 0.2880 - acc: 0.88 - ETA: 18:58 - loss: 0.2887 - acc: 0.88 - ETA: 18:54 - loss: 0.2885 - acc: 0.88 - ETA: 18:49 - loss: 0.2881 - acc: 0.88 - ETA: 18:44 - loss: 0.2890 - acc: 0.88 - ETA: 18:39 - loss: 0.2875 - acc: 0.88 - ETA: 18:34 - loss: 0.2868 - acc: 0.88 - ETA: 18:28 - loss: 0.2898 - acc: 0.88 - ETA: 18:23 - loss: 0.2893 - acc: 0.88 - ETA: 18:18 - loss: 0.2898 - acc: 0.88 - ETA: 18:13 - loss: 0.2896 - acc: 0.88 - ETA: 18:09 - loss: 0.2888 - acc: 0.88 - ETA: 18:03 - loss: 0.2883 - acc: 0.88 - ETA: 17:58 - loss: 0.2878 - acc: 0.88 - ETA: 17:53 - loss: 0.2874 - acc: 0.88 - ETA: 17:47 - loss: 0.2872 - acc: 0.88 - ETA: 17:42 - loss: 0.2873 - acc: 0.88 - ETA: 17:37 - loss: 0.2873 - acc: 0.88 - ETA: 17:33 - loss: 0.2871 - acc: 0.88 - ETA: 17:28 - loss: 0.2876 - acc: 0.88 - ETA: 17:23 - loss: 0.2869 - acc: 0.88 - ETA: 17:18 - loss: 0.2865 - acc: 0.88 - ETA: 17:12 - loss: 0.2867 - acc: 0.88 - ETA: 17:07 - loss: 0.2873 - acc: 0.88 - ETA: 17:01 - loss: 0.2878 - acc: 0.88 - ETA: 16:55 - loss: 0.2874 - acc: 0.88 - ETA: 16:50 - loss: 0.2876 - acc: 0.88 - ETA: 16:44 - loss: 0.2873 - acc: 0.88 - ETA: 16:38 - loss: 0.2863 - acc: 0.88 - ETA: 16:33 - loss: 0.2851 - acc: 0.88 - ETA: 16:28 - loss: 0.2841 - acc: 0.88 - ETA: 16:22 - loss: 0.2835 - acc: 0.88 - ETA: 16:17 - loss: 0.2835 - acc: 0.88 - ETA: 16:11 - loss: 0.2836 - acc: 0.88 - ETA: 16:06 - loss: 0.2839 - acc: 0.88 - ETA: 16:01 - loss: 0.2833 - acc: 0.88 - ETA: 15:55 - loss: 0.2831 - acc: 0.88 - ETA: 15:50 - loss: 0.2833 - acc: 0.88 - ETA: 15:45 - loss: 0.2841 - acc: 0.88 - ETA: 15:39 - loss: 0.2837 - acc: 0.88 - ETA: 15:34 - loss: 0.2828 - acc: 0.88 - ETA: 15:29 - loss: 0.2825 - acc: 0.88 - ETA: 15:23 - loss: 0.2826 - acc: 0.88 - ETA: 15:18 - loss: 0.2824 - acc: 0.88 - ETA: 15:12 - loss: 0.2816 - acc: 0.88 - ETA: 15:07 - loss: 0.2807 - acc: 0.88 - ETA: 15:01 - loss: 0.2802 - acc: 0.88 - ETA: 14:56 - loss: 0.2799 - acc: 0.88 - ETA: 14:51 - loss: 0.2799 - acc: 0.88 - ETA: 14:45 - loss: 0.2798 - acc: 0.88 - ETA: 14:40 - loss: 0.2791 - acc: 0.88 - ETA: 14:34 - loss: 0.2791 - acc: 0.88 - ETA: 14:29 - loss: 0.2794 - acc: 0.88 - ETA: 14:23 - loss: 0.2786 - acc: 0.88 - ETA: 14:18 - loss: 0.2782 - acc: 0.88 - ETA: 14:12 - loss: 0.2780 - acc: 0.88 - ETA: 14:06 - loss: 0.2776 - acc: 0.88 - ETA: 14:01 - loss: 0.2771 - acc: 0.88 - ETA: 13:55 - loss: 0.2765 - acc: 0.88 - ETA: 13:50 - loss: 0.2761 - acc: 0.88 - ETA: 13:44 - loss: 0.2755 - acc: 0.88 - ETA: 13:38 - loss: 0.2755 - acc: 0.88 - ETA: 13:32 - loss: 0.2756 - acc: 0.88 - ETA: 13:27 - loss: 0.2763 - acc: 0.88 - ETA: 13:21 - loss: 0.2765 - acc: 0.88 - ETA: 13:15 - loss: 0.2771 - acc: 0.88 - ETA: 13:09 - loss: 0.2765 - acc: 0.88 - ETA: 13:03 - loss: 0.2762 - acc: 0.88 - ETA: 12:57 - loss: 0.2755 - acc: 0.88 - ETA: 12:52 - loss: 0.2758 - acc: 0.88 - ETA: 12:46 - loss: 0.2761 - acc: 0.88 - ETA: 12:40 - loss: 0.2759 - acc: 0.88 - ETA: 12:35 - loss: 0.2764 - acc: 0.88 - ETA: 12:29 - loss: 0.2765 - acc: 0.88 - ETA: 12:23 - loss: 0.2762 - acc: 0.88 - ETA: 12:18 - loss: 0.2765 - acc: 0.88 - ETA: 12:12 - loss: 0.2767 - acc: 0.88 - ETA: 12:06 - loss: 0.2762 - acc: 0.88 - ETA: 12:00 - loss: 0.2760 - acc: 0.88 - ETA: 11:55 - loss: 0.2759 - acc: 0.88 - ETA: 11:49 - loss: 0.2756 - acc: 0.88 - ETA: 11:44 - loss: 0.2749 - acc: 0.88 - ETA: 11:38 - loss: 0.2749 - acc: 0.88 - ETA: 11:32 - loss: 0.2749 - acc: 0.88 - ETA: 11:27 - loss: 0.2751 - acc: 0.88 - ETA: 11:21 - loss: 0.2746 - acc: 0.88 - ETA: 11:15 - loss: 0.2745 - acc: 0.88 - ETA: 11:09 - loss: 0.2751 - acc: 0.88 - ETA: 11:03 - loss: 0.2755 - acc: 0.88 - ETA: 10:57 - loss: 0.2751 - acc: 0.88 - ETA: 10:51 - loss: 0.2749 - acc: 0.88 - ETA: 10:46 - loss: 0.2753 - acc: 0.88 - ETA: 10:40 - loss: 0.2757 - acc: 0.88 - ETA: 10:34 - loss: 0.2753 - acc: 0.88 - ETA: 10:28 - loss: 0.2747 - acc: 0.88 - ETA: 10:22 - loss: 0.2740 - acc: 0.88 - ETA: 10:16 - loss: 0.2737 - acc: 0.88 - ETA: 10:10 - loss: 0.2732 - acc: 0.88 - ETA: 10:04 - loss: 0.2727 - acc: 0.88 - ETA: 9:58 - loss: 0.2723 - acc: 0.8892 - ETA: 9:52 - loss: 0.2721 - acc: 0.889 - ETA: 9:47 - loss: 0.2717 - acc: 0.889 - ETA: 9:41 - loss: 0.2721 - acc: 0.889 - ETA: 9:35 - loss: 0.2716 - acc: 0.889 - ETA: 9:29 - loss: 0.2715 - acc: 0.889 - ETA: 9:24 - loss: 0.2718 - acc: 0.889 - ETA: 9:18 - loss: 0.2714 - acc: 0.889 - ETA: 9:12 - loss: 0.2720 - acc: 0.889 - ETA: 9:07 - loss: 0.2719 - acc: 0.889 - ETA: 9:01 - loss: 0.2719 - acc: 0.889 - ETA: 8:55 - loss: 0.2720 - acc: 0.889 - ETA: 8:49 - loss: 0.2720 - acc: 0.888 - ETA: 8:43 - loss: 0.2720 - acc: 0.888 - ETA: 8:38 - loss: 0.2724 - acc: 0.888 - ETA: 8:32 - loss: 0.2725 - acc: 0.888 - ETA: 8:26 - loss: 0.2727 - acc: 0.888 - ETA: 8:20 - loss: 0.2732 - acc: 0.888 - ETA: 8:14 - loss: 0.2736 - acc: 0.888 - ETA: 8:08 - loss: 0.2735 - acc: 0.888 - ETA: 8:02 - loss: 0.2732 - acc: 0.888 - ETA: 7:56 - loss: 0.2730 - acc: 0.888 - ETA: 7:51 - loss: 0.2731 - acc: 0.888 - ETA: 7:45 - loss: 0.2737 - acc: 0.888 - ETA: 7:39 - loss: 0.2740 - acc: 0.888 - ETA: 7:33 - loss: 0.2743 - acc: 0.887 - ETA: 7:27 - loss: 0.2741 - acc: 0.887 - ETA: 7:21 - loss: 0.2746 - acc: 0.887 - ETA: 7:15 - loss: 0.2748 - acc: 0.887 - ETA: 7:09 - loss: 0.2752 - acc: 0.887 - ETA: 7:04 - loss: 0.2751 - acc: 0.887 - ETA: 6:58 - loss: 0.2752 - acc: 0.8876"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - ETA: 6:52 - loss: 0.2751 - acc: 0.887 - ETA: 6:46 - loss: 0.2756 - acc: 0.887 - ETA: 6:40 - loss: 0.2760 - acc: 0.887 - ETA: 6:34 - loss: 0.2765 - acc: 0.887 - ETA: 6:28 - loss: 0.2758 - acc: 0.887 - ETA: 6:22 - loss: 0.2757 - acc: 0.887 - ETA: 6:17 - loss: 0.2755 - acc: 0.887 - ETA: 6:11 - loss: 0.2751 - acc: 0.888 - ETA: 6:05 - loss: 0.2751 - acc: 0.887 - ETA: 5:59 - loss: 0.2749 - acc: 0.888 - ETA: 5:53 - loss: 0.2749 - acc: 0.888 - ETA: 5:47 - loss: 0.2750 - acc: 0.887 - ETA: 5:41 - loss: 0.2751 - acc: 0.887 - ETA: 5:36 - loss: 0.2747 - acc: 0.888 - ETA: 5:30 - loss: 0.2749 - acc: 0.887 - ETA: 5:24 - loss: 0.2751 - acc: 0.888 - ETA: 5:18 - loss: 0.2756 - acc: 0.887 - ETA: 5:12 - loss: 0.2753 - acc: 0.887 - ETA: 5:06 - loss: 0.2751 - acc: 0.888 - ETA: 5:00 - loss: 0.2751 - acc: 0.888 - ETA: 4:54 - loss: 0.2752 - acc: 0.888 - ETA: 4:48 - loss: 0.2756 - acc: 0.887 - ETA: 4:42 - loss: 0.2757 - acc: 0.887 - ETA: 4:37 - loss: 0.2759 - acc: 0.887 - ETA: 4:31 - loss: 0.2755 - acc: 0.887 - ETA: 4:25 - loss: 0.2759 - acc: 0.887 - ETA: 4:19 - loss: 0.2753 - acc: 0.888 - ETA: 4:13 - loss: 0.2750 - acc: 0.888 - ETA: 4:07 - loss: 0.2750 - acc: 0.888 - ETA: 4:01 - loss: 0.2746 - acc: 0.888 - ETA: 3:55 - loss: 0.2746 - acc: 0.888 - ETA: 3:49 - loss: 0.2743 - acc: 0.888 - ETA: 3:44 - loss: 0.2740 - acc: 0.888 - ETA: 3:38 - loss: 0.2736 - acc: 0.888 - ETA: 3:32 - loss: 0.2741 - acc: 0.888 - ETA: 3:26 - loss: 0.2736 - acc: 0.888 - ETA: 3:20 - loss: 0.2733 - acc: 0.889 - ETA: 3:14 - loss: 0.2731 - acc: 0.889 - ETA: 3:08 - loss: 0.2732 - acc: 0.889 - ETA: 3:02 - loss: 0.2730 - acc: 0.889 - ETA: 2:57 - loss: 0.2729 - acc: 0.889 - ETA: 2:51 - loss: 0.2732 - acc: 0.889 - ETA: 2:45 - loss: 0.2730 - acc: 0.889 - ETA: 2:39 - loss: 0.2729 - acc: 0.889 - ETA: 2:33 - loss: 0.2725 - acc: 0.889 - ETA: 2:27 - loss: 0.2722 - acc: 0.889 - ETA: 2:21 - loss: 0.2720 - acc: 0.889 - ETA: 2:15 - loss: 0.2723 - acc: 0.889 - ETA: 2:09 - loss: 0.2716 - acc: 0.889 - ETA: 2:03 - loss: 0.2715 - acc: 0.889 - ETA: 1:58 - loss: 0.2716 - acc: 0.889 - ETA: 1:52 - loss: 0.2713 - acc: 0.889 - ETA: 1:46 - loss: 0.2713 - acc: 0.889 - ETA: 1:40 - loss: 0.2716 - acc: 0.889 - ETA: 1:34 - loss: 0.2715 - acc: 0.889 - ETA: 1:28 - loss: 0.2711 - acc: 0.889 - ETA: 1:22 - loss: 0.2713 - acc: 0.889 - ETA: 1:16 - loss: 0.2711 - acc: 0.889 - ETA: 1:10 - loss: 0.2711 - acc: 0.890 - ETA: 1:04 - loss: 0.2714 - acc: 0.889 - ETA: 59s - loss: 0.2709 - acc: 0.890 - ETA: 53s - loss: 0.2707 - acc: 0.89 - ETA: 47s - loss: 0.2703 - acc: 0.89 - ETA: 41s - loss: 0.2703 - acc: 0.89 - ETA: 35s - loss: 0.2706 - acc: 0.89 - ETA: 29s - loss: 0.2706 - acc: 0.89 - ETA: 23s - loss: 0.2706 - acc: 0.89 - ETA: 17s - loss: 0.2707 - acc: 0.89 - ETA: 11s - loss: 0.2705 - acc: 0.89 - ETA: 5s - loss: 0.2704 - acc: 0.8902 - 1816s 6s/step - loss: 0.2698 - acc: 0.8905 - val_loss: 0.2243 - val_acc: 0.9087\n",
      "Epoch 4/5\n",
      "  1/282 [..............................] - ETA: 54s - loss: 0.1535 - acc: 0.9443"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.260731). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2/282 [..............................] - ETA: 1:47 - loss: 0.1893 - acc: 0.9303"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.307598). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3/282 [..............................] - ETA: 2:04 - loss: 0.2248 - acc: 0.9138"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.354465). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4/282 [..............................] - ETA: 2:19 - loss: 0.2704 - acc: 0.8897"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.360812). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5/282 [..............................] - ETA: 2:29 - loss: 0.2753 - acc: 0.8858"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.367158). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8/282 [..............................] - ETA: 2:36 - loss: 0.2882 - acc: 0.880 - ETA: 2:34 - loss: 0.2981 - acc: 0.874 - ETA: 2:29 - loss: 0.2914 - acc: 0.8767"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.341766). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9/282 [..............................] - ETA: 2:19 - loss: 0.2849 - acc: 0.8798"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.329066). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10/282 [>.............................] - ETA: 2:17 - loss: 0.2714 - acc: 0.8862"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.295382). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13/282 [>.............................] - ETA: 2:16 - loss: 0.2705 - acc: 0.887 - ETA: 3:31 - loss: 0.2681 - acc: 0.889 - ETA: 6:24 - loss: 0.2647 - acc: 0.8914"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.261214). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14/282 [>.............................] - ETA: 8:45 - loss: 0.2667 - acc: 0.8906"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.240704). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17/282 [>.............................] - ETA: 10:50 - loss: 0.2652 - acc: 0.89 - ETA: 12:40 - loss: 0.2638 - acc: 0.89 - ETA: 14:20 - loss: 0.2607 - acc: 0.8942"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.259261). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20/282 [=>............................] - ETA: 15:41 - loss: 0.2636 - acc: 0.89 - ETA: 16:54 - loss: 0.2638 - acc: 0.89 - ETA: 18:00 - loss: 0.2627 - acc: 0.8921"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.276838). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22/282 [=>............................] - ETA: 18:59 - loss: 0.2594 - acc: 0.89 - ETA: 19:47 - loss: 0.2555 - acc: 0.8960"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.250464). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25/282 [=>............................] - ETA: 19:57 - loss: 0.2556 - acc: 0.89 - ETA: 20:08 - loss: 0.2551 - acc: 0.89 - ETA: 20:18 - loss: 0.2502 - acc: 0.8979"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.202616). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229/282 [=======================>......] - ETA: 20:27 - loss: 0.2549 - acc: 0.89 - ETA: 20:35 - loss: 0.2522 - acc: 0.89 - ETA: 20:42 - loss: 0.2556 - acc: 0.89 - ETA: 20:47 - loss: 0.2527 - acc: 0.89 - ETA: 20:50 - loss: 0.2561 - acc: 0.89 - ETA: 20:53 - loss: 0.2540 - acc: 0.89 - ETA: 20:56 - loss: 0.2548 - acc: 0.89 - ETA: 21:00 - loss: 0.2570 - acc: 0.89 - ETA: 21:00 - loss: 0.2583 - acc: 0.89 - ETA: 21:01 - loss: 0.2592 - acc: 0.89 - ETA: 21:03 - loss: 0.2557 - acc: 0.89 - ETA: 21:03 - loss: 0.2560 - acc: 0.89 - ETA: 21:03 - loss: 0.2574 - acc: 0.89 - ETA: 21:01 - loss: 0.2551 - acc: 0.89 - ETA: 21:00 - loss: 0.2571 - acc: 0.89 - ETA: 20:59 - loss: 0.2550 - acc: 0.89 - ETA: 20:58 - loss: 0.2535 - acc: 0.89 - ETA: 20:57 - loss: 0.2525 - acc: 0.89 - ETA: 20:57 - loss: 0.2512 - acc: 0.89 - ETA: 20:56 - loss: 0.2500 - acc: 0.89 - ETA: 20:54 - loss: 0.2489 - acc: 0.89 - ETA: 20:52 - loss: 0.2498 - acc: 0.89 - ETA: 20:47 - loss: 0.2485 - acc: 0.89 - ETA: 20:45 - loss: 0.2482 - acc: 0.89 - ETA: 20:43 - loss: 0.2493 - acc: 0.89 - ETA: 20:39 - loss: 0.2485 - acc: 0.89 - ETA: 20:37 - loss: 0.2476 - acc: 0.89 - ETA: 20:34 - loss: 0.2460 - acc: 0.90 - ETA: 20:30 - loss: 0.2451 - acc: 0.90 - ETA: 20:28 - loss: 0.2443 - acc: 0.90 - ETA: 20:24 - loss: 0.2431 - acc: 0.90 - ETA: 20:21 - loss: 0.2457 - acc: 0.90 - ETA: 20:18 - loss: 0.2469 - acc: 0.90 - ETA: 20:15 - loss: 0.2474 - acc: 0.89 - ETA: 20:12 - loss: 0.2460 - acc: 0.90 - ETA: 20:08 - loss: 0.2449 - acc: 0.90 - ETA: 20:04 - loss: 0.2449 - acc: 0.90 - ETA: 20:01 - loss: 0.2449 - acc: 0.90 - ETA: 19:58 - loss: 0.2455 - acc: 0.90 - ETA: 19:54 - loss: 0.2460 - acc: 0.90 - ETA: 19:50 - loss: 0.2466 - acc: 0.89 - ETA: 19:46 - loss: 0.2462 - acc: 0.89 - ETA: 19:42 - loss: 0.2454 - acc: 0.90 - ETA: 19:38 - loss: 0.2453 - acc: 0.90 - ETA: 19:33 - loss: 0.2439 - acc: 0.90 - ETA: 19:29 - loss: 0.2430 - acc: 0.90 - ETA: 19:25 - loss: 0.2414 - acc: 0.90 - ETA: 19:20 - loss: 0.2424 - acc: 0.90 - ETA: 19:15 - loss: 0.2426 - acc: 0.90 - ETA: 19:09 - loss: 0.2431 - acc: 0.90 - ETA: 19:05 - loss: 0.2434 - acc: 0.90 - ETA: 19:01 - loss: 0.2452 - acc: 0.90 - ETA: 18:56 - loss: 0.2446 - acc: 0.90 - ETA: 18:51 - loss: 0.2437 - acc: 0.90 - ETA: 18:46 - loss: 0.2425 - acc: 0.90 - ETA: 18:41 - loss: 0.2421 - acc: 0.90 - ETA: 18:36 - loss: 0.2445 - acc: 0.90 - ETA: 18:32 - loss: 0.2447 - acc: 0.90 - ETA: 18:27 - loss: 0.2452 - acc: 0.90 - ETA: 18:23 - loss: 0.2450 - acc: 0.90 - ETA: 18:17 - loss: 0.2457 - acc: 0.90 - ETA: 18:13 - loss: 0.2455 - acc: 0.90 - ETA: 18:08 - loss: 0.2458 - acc: 0.90 - ETA: 18:03 - loss: 0.2465 - acc: 0.89 - ETA: 17:59 - loss: 0.2462 - acc: 0.89 - ETA: 17:54 - loss: 0.2457 - acc: 0.90 - ETA: 17:49 - loss: 0.2465 - acc: 0.89 - ETA: 17:44 - loss: 0.2474 - acc: 0.89 - ETA: 17:40 - loss: 0.2469 - acc: 0.89 - ETA: 17:35 - loss: 0.2471 - acc: 0.89 - ETA: 17:31 - loss: 0.2486 - acc: 0.89 - ETA: 17:26 - loss: 0.2479 - acc: 0.89 - ETA: 17:21 - loss: 0.2478 - acc: 0.89 - ETA: 17:17 - loss: 0.2475 - acc: 0.89 - ETA: 17:11 - loss: 0.2485 - acc: 0.89 - ETA: 17:06 - loss: 0.2478 - acc: 0.89 - ETA: 17:01 - loss: 0.2472 - acc: 0.89 - ETA: 16:56 - loss: 0.2486 - acc: 0.89 - ETA: 16:51 - loss: 0.2480 - acc: 0.89 - ETA: 16:46 - loss: 0.2482 - acc: 0.89 - ETA: 16:40 - loss: 0.2481 - acc: 0.89 - ETA: 16:35 - loss: 0.2475 - acc: 0.89 - ETA: 16:30 - loss: 0.2473 - acc: 0.89 - ETA: 16:24 - loss: 0.2467 - acc: 0.89 - ETA: 16:19 - loss: 0.2464 - acc: 0.89 - ETA: 16:14 - loss: 0.2465 - acc: 0.89 - ETA: 16:08 - loss: 0.2468 - acc: 0.89 - ETA: 16:03 - loss: 0.2478 - acc: 0.89 - ETA: 15:57 - loss: 0.2486 - acc: 0.89 - ETA: 15:52 - loss: 0.2482 - acc: 0.89 - ETA: 15:47 - loss: 0.2486 - acc: 0.89 - ETA: 15:42 - loss: 0.2496 - acc: 0.89 - ETA: 15:36 - loss: 0.2497 - acc: 0.89 - ETA: 15:31 - loss: 0.2489 - acc: 0.89 - ETA: 15:25 - loss: 0.2484 - acc: 0.89 - ETA: 15:20 - loss: 0.2485 - acc: 0.89 - ETA: 15:15 - loss: 0.2477 - acc: 0.89 - ETA: 15:10 - loss: 0.2477 - acc: 0.89 - ETA: 15:04 - loss: 0.2475 - acc: 0.89 - ETA: 14:59 - loss: 0.2477 - acc: 0.89 - ETA: 14:53 - loss: 0.2473 - acc: 0.89 - ETA: 14:48 - loss: 0.2467 - acc: 0.89 - ETA: 14:42 - loss: 0.2462 - acc: 0.89 - ETA: 14:37 - loss: 0.2453 - acc: 0.90 - ETA: 14:32 - loss: 0.2453 - acc: 0.90 - ETA: 14:26 - loss: 0.2455 - acc: 0.89 - ETA: 14:21 - loss: 0.2458 - acc: 0.89 - ETA: 14:15 - loss: 0.2455 - acc: 0.89 - ETA: 14:09 - loss: 0.2448 - acc: 0.90 - ETA: 14:04 - loss: 0.2450 - acc: 0.90 - ETA: 13:58 - loss: 0.2454 - acc: 0.89 - ETA: 13:53 - loss: 0.2461 - acc: 0.89 - ETA: 13:47 - loss: 0.2471 - acc: 0.89 - ETA: 13:42 - loss: 0.2471 - acc: 0.89 - ETA: 13:36 - loss: 0.2471 - acc: 0.89 - ETA: 13:31 - loss: 0.2472 - acc: 0.89 - ETA: 13:25 - loss: 0.2478 - acc: 0.89 - ETA: 13:20 - loss: 0.2480 - acc: 0.89 - ETA: 13:14 - loss: 0.2474 - acc: 0.89 - ETA: 13:09 - loss: 0.2467 - acc: 0.89 - ETA: 13:03 - loss: 0.2461 - acc: 0.89 - ETA: 12:57 - loss: 0.2461 - acc: 0.89 - ETA: 12:51 - loss: 0.2460 - acc: 0.89 - ETA: 12:46 - loss: 0.2455 - acc: 0.89 - ETA: 12:40 - loss: 0.2451 - acc: 0.89 - ETA: 12:35 - loss: 0.2448 - acc: 0.89 - ETA: 12:29 - loss: 0.2445 - acc: 0.89 - ETA: 12:23 - loss: 0.2444 - acc: 0.89 - ETA: 12:17 - loss: 0.2442 - acc: 0.89 - ETA: 12:12 - loss: 0.2441 - acc: 0.89 - ETA: 12:06 - loss: 0.2433 - acc: 0.90 - ETA: 12:01 - loss: 0.2433 - acc: 0.90 - ETA: 11:55 - loss: 0.2436 - acc: 0.90 - ETA: 11:50 - loss: 0.2433 - acc: 0.90 - ETA: 11:44 - loss: 0.2428 - acc: 0.90 - ETA: 11:39 - loss: 0.2423 - acc: 0.90 - ETA: 11:33 - loss: 0.2420 - acc: 0.90 - ETA: 11:27 - loss: 0.2413 - acc: 0.90 - ETA: 11:21 - loss: 0.2426 - acc: 0.90 - ETA: 11:15 - loss: 0.2416 - acc: 0.90 - ETA: 11:10 - loss: 0.2418 - acc: 0.90 - ETA: 11:04 - loss: 0.2417 - acc: 0.90 - ETA: 10:58 - loss: 0.2417 - acc: 0.90 - ETA: 10:53 - loss: 0.2414 - acc: 0.90 - ETA: 10:47 - loss: 0.2413 - acc: 0.90 - ETA: 10:41 - loss: 0.2415 - acc: 0.90 - ETA: 10:36 - loss: 0.2420 - acc: 0.90 - ETA: 10:30 - loss: 0.2419 - acc: 0.90 - ETA: 10:24 - loss: 0.2423 - acc: 0.90 - ETA: 10:19 - loss: 0.2425 - acc: 0.90 - ETA: 10:13 - loss: 0.2422 - acc: 0.90 - ETA: 10:07 - loss: 0.2421 - acc: 0.90 - ETA: 10:01 - loss: 0.2418 - acc: 0.90 - ETA: 9:55 - loss: 0.2422 - acc: 0.9013 - ETA: 9:50 - loss: 0.2422 - acc: 0.901 - ETA: 9:44 - loss: 0.2425 - acc: 0.901 - ETA: 9:38 - loss: 0.2421 - acc: 0.901 - ETA: 9:33 - loss: 0.2423 - acc: 0.901 - ETA: 9:27 - loss: 0.2423 - acc: 0.901 - ETA: 9:21 - loss: 0.2422 - acc: 0.901 - ETA: 9:15 - loss: 0.2420 - acc: 0.901 - ETA: 9:10 - loss: 0.2422 - acc: 0.901 - ETA: 9:04 - loss: 0.2417 - acc: 0.901 - ETA: 8:58 - loss: 0.2412 - acc: 0.901 - ETA: 8:53 - loss: 0.2412 - acc: 0.901 - ETA: 8:47 - loss: 0.2406 - acc: 0.902 - ETA: 8:41 - loss: 0.2403 - acc: 0.902 - ETA: 8:36 - loss: 0.2409 - acc: 0.902 - ETA: 8:30 - loss: 0.2406 - acc: 0.902 - ETA: 8:24 - loss: 0.2401 - acc: 0.902 - ETA: 8:18 - loss: 0.2399 - acc: 0.902 - ETA: 8:12 - loss: 0.2397 - acc: 0.902 - ETA: 8:07 - loss: 0.2395 - acc: 0.902 - ETA: 8:01 - loss: 0.2394 - acc: 0.902 - ETA: 7:55 - loss: 0.2392 - acc: 0.902 - ETA: 7:49 - loss: 0.2388 - acc: 0.903 - ETA: 7:44 - loss: 0.2385 - acc: 0.903 - ETA: 7:38 - loss: 0.2381 - acc: 0.903 - ETA: 7:32 - loss: 0.2378 - acc: 0.903 - ETA: 7:27 - loss: 0.2374 - acc: 0.903 - ETA: 7:21 - loss: 0.2377 - acc: 0.903 - ETA: 7:15 - loss: 0.2380 - acc: 0.903 - ETA: 7:09 - loss: 0.2376 - acc: 0.903 - ETA: 7:03 - loss: 0.2374 - acc: 0.903 - ETA: 6:58 - loss: 0.2373 - acc: 0.903 - ETA: 6:52 - loss: 0.2374 - acc: 0.903 - ETA: 6:46 - loss: 0.2373 - acc: 0.903 - ETA: 6:40 - loss: 0.2367 - acc: 0.903 - ETA: 6:35 - loss: 0.2367 - acc: 0.903 - ETA: 6:29 - loss: 0.2367 - acc: 0.903 - ETA: 6:23 - loss: 0.2372 - acc: 0.903 - ETA: 6:17 - loss: 0.2369 - acc: 0.903 - ETA: 6:12 - loss: 0.2364 - acc: 0.903 - ETA: 6:06 - loss: 0.2363 - acc: 0.903 - ETA: 6:00 - loss: 0.2365 - acc: 0.903 - ETA: 5:54 - loss: 0.2367 - acc: 0.903 - ETA: 5:48 - loss: 0.2366 - acc: 0.903 - ETA: 5:43 - loss: 0.2366 - acc: 0.903 - ETA: 5:37 - loss: 0.2372 - acc: 0.903 - ETA: 5:31 - loss: 0.2371 - acc: 0.903 - ETA: 5:25 - loss: 0.2372 - acc: 0.903 - ETA: 5:19 - loss: 0.2370 - acc: 0.903 - ETA: 5:14 - loss: 0.2368 - acc: 0.903 - ETA: 5:08 - loss: 0.2367 - acc: 0.9036"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - ETA: 5:02 - loss: 0.2364 - acc: 0.903 - ETA: 4:56 - loss: 0.2365 - acc: 0.903 - ETA: 4:50 - loss: 0.2363 - acc: 0.903 - ETA: 4:45 - loss: 0.2363 - acc: 0.903 - ETA: 4:39 - loss: 0.2362 - acc: 0.903 - ETA: 4:33 - loss: 0.2360 - acc: 0.903 - ETA: 4:27 - loss: 0.2359 - acc: 0.903 - ETA: 4:22 - loss: 0.2357 - acc: 0.904 - ETA: 4:16 - loss: 0.2357 - acc: 0.904 - ETA: 4:10 - loss: 0.2357 - acc: 0.904 - ETA: 4:04 - loss: 0.2357 - acc: 0.903 - ETA: 3:58 - loss: 0.2360 - acc: 0.903 - ETA: 3:53 - loss: 0.2359 - acc: 0.903 - ETA: 3:47 - loss: 0.2356 - acc: 0.903 - ETA: 3:41 - loss: 0.2353 - acc: 0.904 - ETA: 3:35 - loss: 0.2351 - acc: 0.904 - ETA: 3:29 - loss: 0.2349 - acc: 0.904 - ETA: 3:24 - loss: 0.2351 - acc: 0.904 - ETA: 3:18 - loss: 0.2352 - acc: 0.904 - ETA: 3:12 - loss: 0.2349 - acc: 0.904 - ETA: 3:06 - loss: 0.2350 - acc: 0.904 - ETA: 3:00 - loss: 0.2351 - acc: 0.904 - ETA: 2:55 - loss: 0.2347 - acc: 0.904 - ETA: 2:49 - loss: 0.2348 - acc: 0.904 - ETA: 2:43 - loss: 0.2346 - acc: 0.904 - ETA: 2:37 - loss: 0.2343 - acc: 0.904 - ETA: 2:31 - loss: 0.2340 - acc: 0.904 - ETA: 2:25 - loss: 0.2337 - acc: 0.904 - ETA: 2:20 - loss: 0.2335 - acc: 0.904 - ETA: 2:14 - loss: 0.2338 - acc: 0.904 - ETA: 2:08 - loss: 0.2346 - acc: 0.904 - ETA: 2:02 - loss: 0.2342 - acc: 0.904 - ETA: 1:56 - loss: 0.2339 - acc: 0.904 - ETA: 1:50 - loss: 0.2340 - acc: 0.904 - ETA: 1:45 - loss: 0.2341 - acc: 0.904 - ETA: 1:39 - loss: 0.2342 - acc: 0.904 - ETA: 1:33 - loss: 0.2345 - acc: 0.904 - ETA: 1:27 - loss: 0.2343 - acc: 0.904 - ETA: 1:21 - loss: 0.2342 - acc: 0.904 - ETA: 1:15 - loss: 0.2342 - acc: 0.904 - ETA: 1:10 - loss: 0.2345 - acc: 0.904 - ETA: 1:04 - loss: 0.2346 - acc: 0.904 - ETA: 58s - loss: 0.2343 - acc: 0.904 - ETA: 52s - loss: 0.2346 - acc: 0.90 - ETA: 46s - loss: 0.2350 - acc: 0.90 - ETA: 40s - loss: 0.2347 - acc: 0.90 - ETA: 35s - loss: 0.2346 - acc: 0.90 - ETA: 29s - loss: 0.2344 - acc: 0.90 - ETA: 23s - loss: 0.2345 - acc: 0.90 - ETA: 17s - loss: 0.2346 - acc: 0.90 - ETA: 11s - loss: 0.2347 - acc: 0.90 - ETA: 5s - loss: 0.2347 - acc: 0.9045 - 1794s 6s/step - loss: 0.2344 - acc: 0.9047 - val_loss: 0.2112 - val_acc: 0.9149\n",
      "Epoch 5/5\n",
      "  2/282 [..............................] - ETA: 59s - loss: 0.1521 - acc: 0.94 - ETA: 1:17 - loss: 0.1977 - acc: 0.9215"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.193837). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4/282 [..............................] - ETA: 1:36 - loss: 0.1914 - acc: 0.921 - ETA: 1:27 - loss: 0.1993 - acc: 0.9189"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.190732). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5/282 [..............................] - ETA: 1:34 - loss: 0.1951 - acc: 0.9202"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.249630). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6/282 [..............................] - ETA: 1:50 - loss: 0.2063 - acc: 0.9166"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.252735). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7/282 [..............................] - ETA: 2:05 - loss: 0.2157 - acc: 0.9131"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.251942). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8/282 [..............................] - ETA: 2:04 - loss: 0.2184 - acc: 0.9124"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.250786). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14/282 [>.............................] - ETA: 1:57 - loss: 0.2243 - acc: 0.910 - ETA: 1:52 - loss: 0.2232 - acc: 0.909 - ETA: 1:53 - loss: 0.2259 - acc: 0.909 - ETA: 3:34 - loss: 0.2203 - acc: 0.912 - ETA: 6:28 - loss: 0.2197 - acc: 0.913 - ETA: 8:49 - loss: 0.2209 - acc: 0.9120"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.252426). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16/282 [>.............................] - ETA: 10:57 - loss: 0.2175 - acc: 0.91 - ETA: 12:46 - loss: 0.2182 - acc: 0.9124"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.249008). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17/282 [>.............................] - ETA: 14:19 - loss: 0.2196 - acc: 0.9119"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.249492). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18/282 [>.............................] - ETA: 15:40 - loss: 0.2178 - acc: 0.9127"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.261210). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20/282 [=>............................] - ETA: 16:56 - loss: 0.2180 - acc: 0.91 - ETA: 18:01 - loss: 0.2205 - acc: 0.9101"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.257792). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22/282 [=>............................] - ETA: 19:00 - loss: 0.2234 - acc: 0.90 - ETA: 19:50 - loss: 0.2230 - acc: 0.9092"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.202621). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226/282 [=======================>......] - ETA: 20:01 - loss: 0.2239 - acc: 0.90 - ETA: 20:11 - loss: 0.2239 - acc: 0.90 - ETA: 20:21 - loss: 0.2245 - acc: 0.90 - ETA: 20:29 - loss: 0.2309 - acc: 0.90 - ETA: 20:37 - loss: 0.2324 - acc: 0.90 - ETA: 20:45 - loss: 0.2308 - acc: 0.90 - ETA: 20:51 - loss: 0.2323 - acc: 0.90 - ETA: 20:55 - loss: 0.2335 - acc: 0.90 - ETA: 20:57 - loss: 0.2330 - acc: 0.90 - ETA: 20:59 - loss: 0.2320 - acc: 0.90 - ETA: 21:02 - loss: 0.2330 - acc: 0.90 - ETA: 21:02 - loss: 0.2288 - acc: 0.90 - ETA: 21:04 - loss: 0.2323 - acc: 0.90 - ETA: 21:03 - loss: 0.2324 - acc: 0.90 - ETA: 21:04 - loss: 0.2329 - acc: 0.90 - ETA: 21:04 - loss: 0.2355 - acc: 0.90 - ETA: 21:02 - loss: 0.2368 - acc: 0.90 - ETA: 21:02 - loss: 0.2377 - acc: 0.90 - ETA: 21:01 - loss: 0.2387 - acc: 0.90 - ETA: 21:00 - loss: 0.2403 - acc: 0.90 - ETA: 20:56 - loss: 0.2378 - acc: 0.90 - ETA: 20:55 - loss: 0.2381 - acc: 0.90 - ETA: 20:53 - loss: 0.2407 - acc: 0.90 - ETA: 20:50 - loss: 0.2410 - acc: 0.90 - ETA: 20:46 - loss: 0.2471 - acc: 0.90 - ETA: 20:44 - loss: 0.2447 - acc: 0.90 - ETA: 20:42 - loss: 0.2470 - acc: 0.90 - ETA: 20:40 - loss: 0.2459 - acc: 0.90 - ETA: 20:37 - loss: 0.2462 - acc: 0.90 - ETA: 20:34 - loss: 0.2476 - acc: 0.89 - ETA: 20:31 - loss: 0.2481 - acc: 0.89 - ETA: 20:28 - loss: 0.2474 - acc: 0.89 - ETA: 20:25 - loss: 0.2476 - acc: 0.89 - ETA: 20:20 - loss: 0.2459 - acc: 0.90 - ETA: 20:16 - loss: 0.2446 - acc: 0.90 - ETA: 20:13 - loss: 0.2451 - acc: 0.90 - ETA: 20:12 - loss: 0.2441 - acc: 0.90 - ETA: 20:08 - loss: 0.2444 - acc: 0.90 - ETA: 20:06 - loss: 0.2454 - acc: 0.90 - ETA: 20:02 - loss: 0.2456 - acc: 0.90 - ETA: 19:58 - loss: 0.2469 - acc: 0.90 - ETA: 19:54 - loss: 0.2486 - acc: 0.90 - ETA: 19:51 - loss: 0.2480 - acc: 0.90 - ETA: 19:46 - loss: 0.2485 - acc: 0.90 - ETA: 19:42 - loss: 0.2486 - acc: 0.90 - ETA: 19:37 - loss: 0.2472 - acc: 0.90 - ETA: 19:33 - loss: 0.2456 - acc: 0.90 - ETA: 19:29 - loss: 0.2451 - acc: 0.90 - ETA: 19:24 - loss: 0.2450 - acc: 0.90 - ETA: 19:20 - loss: 0.2463 - acc: 0.90 - ETA: 19:16 - loss: 0.2460 - acc: 0.90 - ETA: 19:12 - loss: 0.2452 - acc: 0.90 - ETA: 19:07 - loss: 0.2442 - acc: 0.90 - ETA: 19:03 - loss: 0.2433 - acc: 0.90 - ETA: 18:59 - loss: 0.2432 - acc: 0.90 - ETA: 18:54 - loss: 0.2430 - acc: 0.90 - ETA: 18:50 - loss: 0.2430 - acc: 0.90 - ETA: 18:45 - loss: 0.2425 - acc: 0.90 - ETA: 18:41 - loss: 0.2416 - acc: 0.90 - ETA: 18:36 - loss: 0.2417 - acc: 0.90 - ETA: 18:32 - loss: 0.2410 - acc: 0.90 - ETA: 18:27 - loss: 0.2398 - acc: 0.90 - ETA: 18:22 - loss: 0.2386 - acc: 0.90 - ETA: 18:18 - loss: 0.2389 - acc: 0.90 - ETA: 18:13 - loss: 0.2388 - acc: 0.90 - ETA: 18:08 - loss: 0.2382 - acc: 0.90 - ETA: 18:03 - loss: 0.2387 - acc: 0.90 - ETA: 17:59 - loss: 0.2387 - acc: 0.90 - ETA: 17:54 - loss: 0.2380 - acc: 0.90 - ETA: 17:49 - loss: 0.2386 - acc: 0.90 - ETA: 17:43 - loss: 0.2381 - acc: 0.90 - ETA: 17:39 - loss: 0.2378 - acc: 0.90 - ETA: 17:34 - loss: 0.2381 - acc: 0.90 - ETA: 17:29 - loss: 0.2373 - acc: 0.90 - ETA: 17:24 - loss: 0.2374 - acc: 0.90 - ETA: 17:19 - loss: 0.2377 - acc: 0.90 - ETA: 17:14 - loss: 0.2379 - acc: 0.90 - ETA: 17:09 - loss: 0.2378 - acc: 0.90 - ETA: 17:03 - loss: 0.2378 - acc: 0.90 - ETA: 16:58 - loss: 0.2377 - acc: 0.90 - ETA: 16:53 - loss: 0.2370 - acc: 0.90 - ETA: 16:48 - loss: 0.2364 - acc: 0.90 - ETA: 16:43 - loss: 0.2365 - acc: 0.90 - ETA: 16:38 - loss: 0.2365 - acc: 0.90 - ETA: 16:33 - loss: 0.2361 - acc: 0.90 - ETA: 16:27 - loss: 0.2355 - acc: 0.90 - ETA: 16:22 - loss: 0.2348 - acc: 0.90 - ETA: 16:16 - loss: 0.2342 - acc: 0.90 - ETA: 16:11 - loss: 0.2342 - acc: 0.90 - ETA: 16:06 - loss: 0.2333 - acc: 0.90 - ETA: 16:01 - loss: 0.2326 - acc: 0.90 - ETA: 15:55 - loss: 0.2318 - acc: 0.90 - ETA: 15:50 - loss: 0.2312 - acc: 0.90 - ETA: 15:45 - loss: 0.2301 - acc: 0.90 - ETA: 15:39 - loss: 0.2295 - acc: 0.90 - ETA: 15:33 - loss: 0.2293 - acc: 0.90 - ETA: 15:28 - loss: 0.2284 - acc: 0.90 - ETA: 15:22 - loss: 0.2277 - acc: 0.90 - ETA: 15:17 - loss: 0.2276 - acc: 0.90 - ETA: 15:12 - loss: 0.2285 - acc: 0.90 - ETA: 15:06 - loss: 0.2297 - acc: 0.90 - ETA: 15:01 - loss: 0.2314 - acc: 0.90 - ETA: 14:56 - loss: 0.2311 - acc: 0.90 - ETA: 14:50 - loss: 0.2317 - acc: 0.90 - ETA: 14:45 - loss: 0.2315 - acc: 0.90 - ETA: 14:39 - loss: 0.2321 - acc: 0.90 - ETA: 14:34 - loss: 0.2319 - acc: 0.90 - ETA: 14:29 - loss: 0.2318 - acc: 0.90 - ETA: 14:23 - loss: 0.2321 - acc: 0.90 - ETA: 14:18 - loss: 0.2330 - acc: 0.90 - ETA: 14:12 - loss: 0.2334 - acc: 0.90 - ETA: 14:06 - loss: 0.2329 - acc: 0.90 - ETA: 14:01 - loss: 0.2325 - acc: 0.90 - ETA: 13:55 - loss: 0.2320 - acc: 0.90 - ETA: 13:50 - loss: 0.2316 - acc: 0.90 - ETA: 13:44 - loss: 0.2311 - acc: 0.90 - ETA: 13:38 - loss: 0.2307 - acc: 0.90 - ETA: 13:33 - loss: 0.2311 - acc: 0.90 - ETA: 13:27 - loss: 0.2317 - acc: 0.90 - ETA: 13:22 - loss: 0.2316 - acc: 0.90 - ETA: 13:16 - loss: 0.2312 - acc: 0.90 - ETA: 13:11 - loss: 0.2309 - acc: 0.90 - ETA: 13:05 - loss: 0.2307 - acc: 0.90 - ETA: 13:00 - loss: 0.2305 - acc: 0.90 - ETA: 12:54 - loss: 0.2298 - acc: 0.90 - ETA: 12:48 - loss: 0.2290 - acc: 0.90 - ETA: 12:43 - loss: 0.2302 - acc: 0.90 - ETA: 12:38 - loss: 0.2303 - acc: 0.90 - ETA: 12:32 - loss: 0.2307 - acc: 0.90 - ETA: 12:27 - loss: 0.2300 - acc: 0.90 - ETA: 12:22 - loss: 0.2301 - acc: 0.90 - ETA: 12:16 - loss: 0.2303 - acc: 0.90 - ETA: 12:11 - loss: 0.2303 - acc: 0.90 - ETA: 12:05 - loss: 0.2302 - acc: 0.90 - ETA: 12:00 - loss: 0.2304 - acc: 0.90 - ETA: 11:54 - loss: 0.2301 - acc: 0.90 - ETA: 11:48 - loss: 0.2301 - acc: 0.90 - ETA: 11:43 - loss: 0.2295 - acc: 0.90 - ETA: 11:37 - loss: 0.2297 - acc: 0.90 - ETA: 11:32 - loss: 0.2293 - acc: 0.90 - ETA: 11:26 - loss: 0.2292 - acc: 0.90 - ETA: 11:20 - loss: 0.2287 - acc: 0.90 - ETA: 11:15 - loss: 0.2284 - acc: 0.90 - ETA: 11:09 - loss: 0.2282 - acc: 0.90 - ETA: 11:04 - loss: 0.2282 - acc: 0.90 - ETA: 10:58 - loss: 0.2281 - acc: 0.90 - ETA: 10:52 - loss: 0.2279 - acc: 0.90 - ETA: 10:47 - loss: 0.2275 - acc: 0.90 - ETA: 10:41 - loss: 0.2272 - acc: 0.90 - ETA: 10:36 - loss: 0.2271 - acc: 0.90 - ETA: 10:30 - loss: 0.2278 - acc: 0.90 - ETA: 10:24 - loss: 0.2280 - acc: 0.90 - ETA: 10:19 - loss: 0.2274 - acc: 0.90 - ETA: 10:13 - loss: 0.2272 - acc: 0.90 - ETA: 10:07 - loss: 0.2270 - acc: 0.90 - ETA: 10:02 - loss: 0.2275 - acc: 0.90 - ETA: 9:56 - loss: 0.2277 - acc: 0.9089 - ETA: 9:50 - loss: 0.2279 - acc: 0.908 - ETA: 9:45 - loss: 0.2277 - acc: 0.908 - ETA: 9:39 - loss: 0.2281 - acc: 0.908 - ETA: 9:34 - loss: 0.2281 - acc: 0.908 - ETA: 9:28 - loss: 0.2277 - acc: 0.908 - ETA: 9:22 - loss: 0.2274 - acc: 0.908 - ETA: 9:17 - loss: 0.2268 - acc: 0.909 - ETA: 9:11 - loss: 0.2271 - acc: 0.909 - ETA: 9:05 - loss: 0.2267 - acc: 0.909 - ETA: 9:00 - loss: 0.2265 - acc: 0.909 - ETA: 8:54 - loss: 0.2268 - acc: 0.909 - ETA: 8:48 - loss: 0.2267 - acc: 0.909 - ETA: 8:43 - loss: 0.2265 - acc: 0.909 - ETA: 8:37 - loss: 0.2263 - acc: 0.909 - ETA: 8:31 - loss: 0.2261 - acc: 0.909 - ETA: 8:25 - loss: 0.2262 - acc: 0.909 - ETA: 8:20 - loss: 0.2262 - acc: 0.909 - ETA: 8:14 - loss: 0.2259 - acc: 0.909 - ETA: 8:08 - loss: 0.2256 - acc: 0.909 - ETA: 8:02 - loss: 0.2253 - acc: 0.909 - ETA: 7:57 - loss: 0.2251 - acc: 0.909 - ETA: 7:51 - loss: 0.2244 - acc: 0.910 - ETA: 7:45 - loss: 0.2249 - acc: 0.910 - ETA: 7:39 - loss: 0.2256 - acc: 0.909 - ETA: 7:34 - loss: 0.2256 - acc: 0.909 - ETA: 7:28 - loss: 0.2254 - acc: 0.909 - ETA: 7:22 - loss: 0.2250 - acc: 0.910 - ETA: 7:17 - loss: 0.2246 - acc: 0.910 - ETA: 7:11 - loss: 0.2244 - acc: 0.910 - ETA: 7:05 - loss: 0.2243 - acc: 0.910 - ETA: 6:59 - loss: 0.2246 - acc: 0.910 - ETA: 6:54 - loss: 0.2246 - acc: 0.910 - ETA: 6:48 - loss: 0.2249 - acc: 0.910 - ETA: 6:42 - loss: 0.2251 - acc: 0.910 - ETA: 6:36 - loss: 0.2250 - acc: 0.910 - ETA: 6:31 - loss: 0.2251 - acc: 0.910 - ETA: 6:25 - loss: 0.2250 - acc: 0.910 - ETA: 6:19 - loss: 0.2248 - acc: 0.910 - ETA: 6:13 - loss: 0.2248 - acc: 0.910 - ETA: 6:07 - loss: 0.2244 - acc: 0.910 - ETA: 6:02 - loss: 0.2249 - acc: 0.910 - ETA: 5:56 - loss: 0.2252 - acc: 0.909 - ETA: 5:50 - loss: 0.2248 - acc: 0.910 - ETA: 5:44 - loss: 0.2252 - acc: 0.909 - ETA: 5:39 - loss: 0.2251 - acc: 0.909 - ETA: 5:33 - loss: 0.2250 - acc: 0.910 - ETA: 5:27 - loss: 0.2251 - acc: 0.9100"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - ETA: 5:21 - loss: 0.2248 - acc: 0.910 - ETA: 5:15 - loss: 0.2248 - acc: 0.910 - ETA: 5:10 - loss: 0.2244 - acc: 0.910 - ETA: 5:04 - loss: 0.2248 - acc: 0.910 - ETA: 4:58 - loss: 0.2247 - acc: 0.910 - ETA: 4:52 - loss: 0.2249 - acc: 0.910 - ETA: 4:46 - loss: 0.2248 - acc: 0.910 - ETA: 4:41 - loss: 0.2250 - acc: 0.909 - ETA: 4:35 - loss: 0.2249 - acc: 0.909 - ETA: 4:29 - loss: 0.2250 - acc: 0.909 - ETA: 4:23 - loss: 0.2252 - acc: 0.909 - ETA: 4:17 - loss: 0.2251 - acc: 0.909 - ETA: 4:12 - loss: 0.2254 - acc: 0.909 - ETA: 4:06 - loss: 0.2251 - acc: 0.909 - ETA: 4:00 - loss: 0.2250 - acc: 0.909 - ETA: 3:54 - loss: 0.2252 - acc: 0.909 - ETA: 3:48 - loss: 0.2251 - acc: 0.909 - ETA: 3:42 - loss: 0.2247 - acc: 0.910 - ETA: 3:37 - loss: 0.2252 - acc: 0.909 - ETA: 3:31 - loss: 0.2249 - acc: 0.909 - ETA: 3:25 - loss: 0.2250 - acc: 0.909 - ETA: 3:19 - loss: 0.2249 - acc: 0.909 - ETA: 3:13 - loss: 0.2247 - acc: 0.909 - ETA: 3:07 - loss: 0.2245 - acc: 0.910 - ETA: 3:02 - loss: 0.2244 - acc: 0.909 - ETA: 2:56 - loss: 0.2248 - acc: 0.909 - ETA: 2:50 - loss: 0.2246 - acc: 0.909 - ETA: 2:44 - loss: 0.2251 - acc: 0.909 - ETA: 2:38 - loss: 0.2253 - acc: 0.909 - ETA: 2:32 - loss: 0.2249 - acc: 0.909 - ETA: 2:26 - loss: 0.2247 - acc: 0.909 - ETA: 2:21 - loss: 0.2244 - acc: 0.909 - ETA: 2:15 - loss: 0.2242 - acc: 0.909 - ETA: 2:09 - loss: 0.2241 - acc: 0.910 - ETA: 2:03 - loss: 0.2241 - acc: 0.910 - ETA: 1:57 - loss: 0.2240 - acc: 0.910 - ETA: 1:51 - loss: 0.2240 - acc: 0.910 - ETA: 1:45 - loss: 0.2237 - acc: 0.910 - ETA: 1:39 - loss: 0.2234 - acc: 0.910 - ETA: 1:34 - loss: 0.2235 - acc: 0.910 - ETA: 1:28 - loss: 0.2233 - acc: 0.910 - ETA: 1:22 - loss: 0.2237 - acc: 0.910 - ETA: 1:16 - loss: 0.2236 - acc: 0.910 - ETA: 1:10 - loss: 0.2232 - acc: 0.910 - ETA: 1:04 - loss: 0.2233 - acc: 0.910 - ETA: 58s - loss: 0.2230 - acc: 0.910 - ETA: 52s - loss: 0.2231 - acc: 0.91 - ETA: 47s - loss: 0.2229 - acc: 0.91 - ETA: 41s - loss: 0.2229 - acc: 0.91 - ETA: 35s - loss: 0.2226 - acc: 0.91 - ETA: 29s - loss: 0.2227 - acc: 0.91 - ETA: 23s - loss: 0.2225 - acc: 0.91 - ETA: 17s - loss: 0.2221 - acc: 0.91 - ETA: 11s - loss: 0.2220 - acc: 0.91 - ETA: 5s - loss: 0.2221 - acc: 0.9108 - 1785s 6s/step - loss: 0.2221 - acc: 0.9108 - val_loss: 0.2019 - val_acc: 0.9193\n",
      "Model training time: 151.6 minutes\n",
      "truth dimensions:  (5316, 10007)\n",
      "truth_4_dimension_size: (333, 626)\n",
      "slide4_size (167, 313)\n",
      "truth dimensions:  (5316, 10007)\n",
      "truth_4_dimension_size: (333, 626)\n",
      "slide4_size (167, 313)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14538\n",
      "39456\n",
      "Epoch 1/5\n",
      "  1/282 [..............................] - ETA: 47:34 - loss: 0.2003 - acc: 0.9235"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.353471). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2/282 [..............................] - ETA: 43:56 - loss: 0.2667 - acc: 0.8887"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.338899). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4/282 [..............................] - ETA: 43:24 - loss: 0.2304 - acc: 0.90 - ETA: 42:49 - loss: 0.2188 - acc: 0.9089"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.346644). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6/282 [..............................] - ETA: 42:28 - loss: 0.2045 - acc: 0.91 - ETA: 41:45 - loss: 0.2121 - acc: 0.9160"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.367634). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11/282 [>.............................] - ETA: 41:16 - loss: 0.2342 - acc: 0.90 - ETA: 40:56 - loss: 0.2285 - acc: 0.90 - ETA: 40:52 - loss: 0.2283 - acc: 0.90 - ETA: 40:43 - loss: 0.2353 - acc: 0.90 - ETA: 40:25 - loss: 0.2313 - acc: 0.9060"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.332072). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12/282 [>.............................] - ETA: 39:08 - loss: 0.2327 - acc: 0.9047"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.306612). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13/282 [>.............................] - ETA: 38:03 - loss: 0.2288 - acc: 0.9067"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.250948). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217/282 [======================>.......] - ETA: 37:07 - loss: 0.2254 - acc: 0.90 - ETA: 36:14 - loss: 0.2256 - acc: 0.90 - ETA: 35:39 - loss: 0.2291 - acc: 0.90 - ETA: 35:02 - loss: 0.2281 - acc: 0.90 - ETA: 34:25 - loss: 0.2332 - acc: 0.90 - ETA: 33:54 - loss: 0.2350 - acc: 0.90 - ETA: 33:26 - loss: 0.2334 - acc: 0.90 - ETA: 32:59 - loss: 0.2330 - acc: 0.90 - ETA: 32:33 - loss: 0.2309 - acc: 0.90 - ETA: 32:09 - loss: 0.2297 - acc: 0.90 - ETA: 31:44 - loss: 0.2236 - acc: 0.90 - ETA: 31:20 - loss: 0.2237 - acc: 0.90 - ETA: 31:01 - loss: 0.2252 - acc: 0.90 - ETA: 30:42 - loss: 0.2213 - acc: 0.90 - ETA: 30:26 - loss: 0.2181 - acc: 0.91 - ETA: 30:09 - loss: 0.2180 - acc: 0.91 - ETA: 29:52 - loss: 0.2186 - acc: 0.91 - ETA: 29:36 - loss: 0.2201 - acc: 0.90 - ETA: 29:22 - loss: 0.2198 - acc: 0.90 - ETA: 29:06 - loss: 0.2174 - acc: 0.91 - ETA: 28:53 - loss: 0.2169 - acc: 0.91 - ETA: 28:38 - loss: 0.2160 - acc: 0.91 - ETA: 28:26 - loss: 0.2149 - acc: 0.91 - ETA: 28:14 - loss: 0.2160 - acc: 0.91 - ETA: 28:01 - loss: 0.2160 - acc: 0.91 - ETA: 27:50 - loss: 0.2161 - acc: 0.91 - ETA: 27:37 - loss: 0.2145 - acc: 0.91 - ETA: 27:27 - loss: 0.2157 - acc: 0.91 - ETA: 27:17 - loss: 0.2157 - acc: 0.91 - ETA: 27:06 - loss: 0.2159 - acc: 0.91 - ETA: 26:56 - loss: 0.2176 - acc: 0.91 - ETA: 26:45 - loss: 0.2186 - acc: 0.90 - ETA: 26:36 - loss: 0.2205 - acc: 0.90 - ETA: 26:28 - loss: 0.2207 - acc: 0.90 - ETA: 26:19 - loss: 0.2191 - acc: 0.90 - ETA: 26:41 - loss: 0.2198 - acc: 0.90 - ETA: 27:39 - loss: 0.2181 - acc: 0.91 - ETA: 27:30 - loss: 0.2178 - acc: 0.91 - ETA: 27:23 - loss: 0.2169 - acc: 0.91 - ETA: 27:12 - loss: 0.2159 - acc: 0.91 - ETA: 26:59 - loss: 0.2167 - acc: 0.91 - ETA: 26:51 - loss: 0.2184 - acc: 0.91 - ETA: 26:40 - loss: 0.2185 - acc: 0.91 - ETA: 26:30 - loss: 0.2183 - acc: 0.91 - ETA: 26:19 - loss: 0.2185 - acc: 0.90 - ETA: 26:09 - loss: 0.2183 - acc: 0.91 - ETA: 25:59 - loss: 0.2170 - acc: 0.91 - ETA: 25:49 - loss: 0.2172 - acc: 0.91 - ETA: 25:38 - loss: 0.2168 - acc: 0.91 - ETA: 25:27 - loss: 0.2167 - acc: 0.91 - ETA: 25:16 - loss: 0.2169 - acc: 0.91 - ETA: 25:06 - loss: 0.2165 - acc: 0.91 - ETA: 24:57 - loss: 0.2161 - acc: 0.91 - ETA: 24:47 - loss: 0.2160 - acc: 0.91 - ETA: 24:38 - loss: 0.2158 - acc: 0.91 - ETA: 24:29 - loss: 0.2146 - acc: 0.91 - ETA: 24:19 - loss: 0.2142 - acc: 0.91 - ETA: 24:11 - loss: 0.2145 - acc: 0.91 - ETA: 24:02 - loss: 0.2128 - acc: 0.91 - ETA: 23:53 - loss: 0.2128 - acc: 0.91 - ETA: 23:44 - loss: 0.2130 - acc: 0.91 - ETA: 23:35 - loss: 0.2135 - acc: 0.91 - ETA: 23:26 - loss: 0.2124 - acc: 0.91 - ETA: 23:18 - loss: 0.2122 - acc: 0.91 - ETA: 23:10 - loss: 0.2122 - acc: 0.91 - ETA: 23:01 - loss: 0.2124 - acc: 0.91 - ETA: 22:53 - loss: 0.2136 - acc: 0.91 - ETA: 22:43 - loss: 0.2132 - acc: 0.91 - ETA: 22:34 - loss: 0.2131 - acc: 0.91 - ETA: 22:26 - loss: 0.2127 - acc: 0.91 - ETA: 22:17 - loss: 0.2119 - acc: 0.91 - ETA: 22:08 - loss: 0.2125 - acc: 0.91 - ETA: 22:00 - loss: 0.2122 - acc: 0.91 - ETA: 21:52 - loss: 0.2111 - acc: 0.91 - ETA: 21:44 - loss: 0.2116 - acc: 0.91 - ETA: 21:36 - loss: 0.2108 - acc: 0.91 - ETA: 21:28 - loss: 0.2106 - acc: 0.91 - ETA: 21:20 - loss: 0.2097 - acc: 0.91 - ETA: 21:11 - loss: 0.2109 - acc: 0.91 - ETA: 21:03 - loss: 0.2107 - acc: 0.91 - ETA: 20:55 - loss: 0.2103 - acc: 0.91 - ETA: 20:47 - loss: 0.2101 - acc: 0.91 - ETA: 20:39 - loss: 0.2103 - acc: 0.91 - ETA: 20:32 - loss: 0.2094 - acc: 0.91 - ETA: 20:24 - loss: 0.2099 - acc: 0.91 - ETA: 20:17 - loss: 0.2113 - acc: 0.91 - ETA: 20:09 - loss: 0.2105 - acc: 0.91 - ETA: 20:01 - loss: 0.2098 - acc: 0.91 - ETA: 19:54 - loss: 0.2098 - acc: 0.91 - ETA: 19:46 - loss: 0.2099 - acc: 0.91 - ETA: 19:39 - loss: 0.2100 - acc: 0.91 - ETA: 19:31 - loss: 0.2106 - acc: 0.91 - ETA: 19:23 - loss: 0.2107 - acc: 0.91 - ETA: 19:15 - loss: 0.2102 - acc: 0.91 - ETA: 19:08 - loss: 0.2097 - acc: 0.91 - ETA: 19:01 - loss: 0.2107 - acc: 0.91 - ETA: 18:53 - loss: 0.2100 - acc: 0.91 - ETA: 18:46 - loss: 0.2099 - acc: 0.91 - ETA: 18:38 - loss: 0.2096 - acc: 0.91 - ETA: 18:31 - loss: 0.2094 - acc: 0.91 - ETA: 18:23 - loss: 0.2102 - acc: 0.91 - ETA: 18:16 - loss: 0.2100 - acc: 0.91 - ETA: 18:09 - loss: 0.2104 - acc: 0.91 - ETA: 18:02 - loss: 0.2109 - acc: 0.91 - ETA: 17:54 - loss: 0.2114 - acc: 0.91 - ETA: 17:47 - loss: 0.2117 - acc: 0.91 - ETA: 17:40 - loss: 0.2112 - acc: 0.91 - ETA: 17:33 - loss: 0.2120 - acc: 0.91 - ETA: 17:26 - loss: 0.2123 - acc: 0.91 - ETA: 17:18 - loss: 0.2125 - acc: 0.91 - ETA: 17:11 - loss: 0.2125 - acc: 0.91 - ETA: 17:04 - loss: 0.2126 - acc: 0.91 - ETA: 16:57 - loss: 0.2131 - acc: 0.91 - ETA: 16:50 - loss: 0.2128 - acc: 0.91 - ETA: 16:43 - loss: 0.2123 - acc: 0.91 - ETA: 16:36 - loss: 0.2121 - acc: 0.91 - ETA: 16:29 - loss: 0.2115 - acc: 0.91 - ETA: 16:21 - loss: 0.2110 - acc: 0.91 - ETA: 16:15 - loss: 0.2138 - acc: 0.91 - ETA: 16:07 - loss: 0.2145 - acc: 0.91 - ETA: 16:00 - loss: 0.2149 - acc: 0.91 - ETA: 15:54 - loss: 0.2150 - acc: 0.91 - ETA: 15:46 - loss: 0.2153 - acc: 0.91 - ETA: 15:40 - loss: 0.2155 - acc: 0.91 - ETA: 15:32 - loss: 0.2157 - acc: 0.91 - ETA: 15:25 - loss: 0.2154 - acc: 0.91 - ETA: 15:19 - loss: 0.2159 - acc: 0.91 - ETA: 15:12 - loss: 0.2157 - acc: 0.91 - ETA: 15:05 - loss: 0.2158 - acc: 0.91 - ETA: 14:58 - loss: 0.2155 - acc: 0.91 - ETA: 14:51 - loss: 0.2157 - acc: 0.91 - ETA: 14:44 - loss: 0.2161 - acc: 0.91 - ETA: 14:37 - loss: 0.2163 - acc: 0.91 - ETA: 14:30 - loss: 0.2163 - acc: 0.91 - ETA: 14:24 - loss: 0.2158 - acc: 0.91 - ETA: 14:16 - loss: 0.2160 - acc: 0.91 - ETA: 14:09 - loss: 0.2160 - acc: 0.91 - ETA: 14:03 - loss: 0.2158 - acc: 0.91 - ETA: 13:56 - loss: 0.2158 - acc: 0.91 - ETA: 13:49 - loss: 0.2163 - acc: 0.91 - ETA: 13:43 - loss: 0.2165 - acc: 0.91 - ETA: 13:36 - loss: 0.2161 - acc: 0.91 - ETA: 13:29 - loss: 0.2163 - acc: 0.91 - ETA: 13:22 - loss: 0.2165 - acc: 0.91 - ETA: 13:15 - loss: 0.2162 - acc: 0.91 - ETA: 13:09 - loss: 0.2157 - acc: 0.91 - ETA: 13:02 - loss: 0.2156 - acc: 0.91 - ETA: 12:55 - loss: 0.2152 - acc: 0.91 - ETA: 12:49 - loss: 0.2150 - acc: 0.91 - ETA: 12:42 - loss: 0.2149 - acc: 0.91 - ETA: 12:35 - loss: 0.2142 - acc: 0.91 - ETA: 12:29 - loss: 0.2138 - acc: 0.91 - ETA: 12:22 - loss: 0.2141 - acc: 0.91 - ETA: 12:15 - loss: 0.2143 - acc: 0.91 - ETA: 12:09 - loss: 0.2140 - acc: 0.91 - ETA: 12:02 - loss: 0.2136 - acc: 0.91 - ETA: 11:56 - loss: 0.2143 - acc: 0.91 - ETA: 11:49 - loss: 0.2142 - acc: 0.91 - ETA: 11:42 - loss: 0.2141 - acc: 0.91 - ETA: 11:36 - loss: 0.2137 - acc: 0.91 - ETA: 11:29 - loss: 0.2139 - acc: 0.91 - ETA: 11:23 - loss: 0.2141 - acc: 0.91 - ETA: 11:16 - loss: 0.2140 - acc: 0.91 - ETA: 11:09 - loss: 0.2153 - acc: 0.91 - ETA: 11:03 - loss: 0.2153 - acc: 0.91 - ETA: 10:56 - loss: 0.2150 - acc: 0.91 - ETA: 10:50 - loss: 0.2146 - acc: 0.91 - ETA: 10:43 - loss: 0.2144 - acc: 0.91 - ETA: 10:37 - loss: 0.2140 - acc: 0.91 - ETA: 10:30 - loss: 0.2138 - acc: 0.91 - ETA: 10:24 - loss: 0.2139 - acc: 0.91 - ETA: 10:17 - loss: 0.2140 - acc: 0.91 - ETA: 10:11 - loss: 0.2153 - acc: 0.91 - ETA: 10:04 - loss: 0.2152 - acc: 0.91 - ETA: 9:58 - loss: 0.2153 - acc: 0.9127 - ETA: 9:51 - loss: 0.2157 - acc: 0.912 - ETA: 9:45 - loss: 0.2153 - acc: 0.912 - ETA: 9:38 - loss: 0.2147 - acc: 0.913 - ETA: 9:32 - loss: 0.2147 - acc: 0.913 - ETA: 9:25 - loss: 0.2142 - acc: 0.913 - ETA: 9:19 - loss: 0.2141 - acc: 0.913 - ETA: 9:12 - loss: 0.2139 - acc: 0.913 - ETA: 9:06 - loss: 0.2134 - acc: 0.913 - ETA: 8:59 - loss: 0.2143 - acc: 0.913 - ETA: 8:53 - loss: 0.2141 - acc: 0.913 - ETA: 8:46 - loss: 0.2142 - acc: 0.913 - ETA: 8:40 - loss: 0.2141 - acc: 0.913 - ETA: 8:33 - loss: 0.2150 - acc: 0.912 - ETA: 8:27 - loss: 0.2147 - acc: 0.912 - ETA: 8:21 - loss: 0.2149 - acc: 0.912 - ETA: 8:14 - loss: 0.2149 - acc: 0.912 - ETA: 8:08 - loss: 0.2146 - acc: 0.912 - ETA: 8:01 - loss: 0.2148 - acc: 0.912 - ETA: 7:55 - loss: 0.2152 - acc: 0.912 - ETA: 7:48 - loss: 0.2155 - acc: 0.912 - ETA: 7:42 - loss: 0.2156 - acc: 0.912 - ETA: 7:35 - loss: 0.2161 - acc: 0.912 - ETA: 7:29 - loss: 0.2162 - acc: 0.912 - ETA: 7:23 - loss: 0.2165 - acc: 0.912 - ETA: 7:16 - loss: 0.2162 - acc: 0.912 - ETA: 7:10 - loss: 0.2160 - acc: 0.912 - ETA: 7:03 - loss: 0.2162 - acc: 0.912 - ETA: 6:57 - loss: 0.2165 - acc: 0.912 - ETA: 6:50 - loss: 0.2165 - acc: 0.9121"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - ETA: 6:44 - loss: 0.2160 - acc: 0.912 - ETA: 6:38 - loss: 0.2159 - acc: 0.912 - ETA: 6:31 - loss: 0.2157 - acc: 0.912 - ETA: 6:25 - loss: 0.2154 - acc: 0.912 - ETA: 6:18 - loss: 0.2157 - acc: 0.912 - ETA: 6:12 - loss: 0.2154 - acc: 0.912 - ETA: 6:06 - loss: 0.2151 - acc: 0.912 - ETA: 5:59 - loss: 0.2151 - acc: 0.912 - ETA: 5:53 - loss: 0.2149 - acc: 0.912 - ETA: 5:46 - loss: 0.2150 - acc: 0.912 - ETA: 5:40 - loss: 0.2152 - acc: 0.912 - ETA: 5:34 - loss: 0.2153 - acc: 0.912 - ETA: 5:27 - loss: 0.2155 - acc: 0.912 - ETA: 5:21 - loss: 0.2155 - acc: 0.912 - ETA: 5:15 - loss: 0.2150 - acc: 0.912 - ETA: 5:08 - loss: 0.2151 - acc: 0.912 - ETA: 5:02 - loss: 0.2151 - acc: 0.912 - ETA: 4:56 - loss: 0.2153 - acc: 0.912 - ETA: 4:49 - loss: 0.2149 - acc: 0.912 - ETA: 4:43 - loss: 0.2146 - acc: 0.912 - ETA: 4:36 - loss: 0.2153 - acc: 0.912 - ETA: 4:30 - loss: 0.2151 - acc: 0.912 - ETA: 4:24 - loss: 0.2155 - acc: 0.912 - ETA: 4:17 - loss: 0.2152 - acc: 0.912 - ETA: 4:11 - loss: 0.2153 - acc: 0.912 - ETA: 4:05 - loss: 0.2152 - acc: 0.912 - ETA: 3:58 - loss: 0.2149 - acc: 0.912 - ETA: 3:52 - loss: 0.2147 - acc: 0.913 - ETA: 3:46 - loss: 0.2150 - acc: 0.912 - ETA: 3:39 - loss: 0.2151 - acc: 0.912 - ETA: 3:33 - loss: 0.2155 - acc: 0.912 - ETA: 3:27 - loss: 0.2154 - acc: 0.912 - ETA: 3:20 - loss: 0.2150 - acc: 0.912 - ETA: 3:14 - loss: 0.2150 - acc: 0.912 - ETA: 3:08 - loss: 0.2154 - acc: 0.912 - ETA: 3:01 - loss: 0.2153 - acc: 0.912 - ETA: 2:55 - loss: 0.2151 - acc: 0.912 - ETA: 2:49 - loss: 0.2148 - acc: 0.912 - ETA: 2:42 - loss: 0.2153 - acc: 0.912 - ETA: 2:36 - loss: 0.2151 - acc: 0.912 - ETA: 2:30 - loss: 0.2155 - acc: 0.912 - ETA: 2:24 - loss: 0.2153 - acc: 0.912 - ETA: 2:17 - loss: 0.2153 - acc: 0.912 - ETA: 2:11 - loss: 0.2152 - acc: 0.912 - ETA: 2:05 - loss: 0.2152 - acc: 0.912 - ETA: 1:58 - loss: 0.2151 - acc: 0.912 - ETA: 1:52 - loss: 0.2152 - acc: 0.912 - ETA: 1:46 - loss: 0.2153 - acc: 0.912 - ETA: 1:40 - loss: 0.2150 - acc: 0.912 - ETA: 1:33 - loss: 0.2148 - acc: 0.912 - ETA: 1:27 - loss: 0.2151 - acc: 0.912 - ETA: 1:21 - loss: 0.2151 - acc: 0.912 - ETA: 1:15 - loss: 0.2148 - acc: 0.912 - ETA: 1:08 - loss: 0.2145 - acc: 0.912 - ETA: 1:02 - loss: 0.2144 - acc: 0.912 - ETA: 56s - loss: 0.2144 - acc: 0.912 - ETA: 49s - loss: 0.2146 - acc: 0.91 - ETA: 43s - loss: 0.2144 - acc: 0.91 - ETA: 37s - loss: 0.2141 - acc: 0.91 - ETA: 31s - loss: 0.2141 - acc: 0.91 - ETA: 24s - loss: 0.2140 - acc: 0.91 - ETA: 18s - loss: 0.2136 - acc: 0.91 - ETA: 12s - loss: 0.2139 - acc: 0.91 - ETA: 6s - loss: 0.2136 - acc: 0.9133 - 1907s 7s/step - loss: 0.2132 - acc: 0.9135 - val_loss: 0.2195 - val_acc: 0.9127\n",
      "Epoch 2/5\n",
      "  1/282 [..............................] - ETA: 57s - loss: 0.2011 - acc: 0.9105"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.264618). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2/282 [..............................] - ETA: 1:58 - loss: 0.2720 - acc: 0.8881"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.345183). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3/282 [..............................] - ETA: 2:30 - loss: 0.2567 - acc: 0.8950"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.296732). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4/282 [..............................] - ETA: 2:31 - loss: 0.2383 - acc: 0.9034"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.345140). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5/282 [..............................] - ETA: 2:36 - loss: 0.2169 - acc: 0.9135"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.369111). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6/282 [..............................] - ETA: 2:41 - loss: 0.2285 - acc: 0.9082"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.381329). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8/282 [..............................] - ETA: 2:43 - loss: 0.2375 - acc: 0.903 - ETA: 2:31 - loss: 0.2459 - acc: 0.9014"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.332921). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10/282 [>.............................] - ETA: 2:21 - loss: 0.2357 - acc: 0.906 - ETA: 2:13 - loss: 0.2334 - acc: 0.9070"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.280675). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11/282 [>.............................] - ETA: 2:09 - loss: 0.2323 - acc: 0.9068"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.324133). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12/282 [>.............................] - ETA: 3:25 - loss: 0.2265 - acc: 0.9091"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.190351). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216/282 [=====================>........] - ETA: 6:03 - loss: 0.2182 - acc: 0.912 - ETA: 8:30 - loss: 0.2153 - acc: 0.913 - ETA: 10:24 - loss: 0.2182 - acc: 0.91 - ETA: 12:07 - loss: 0.2197 - acc: 0.91 - ETA: 13:45 - loss: 0.2292 - acc: 0.91 - ETA: 15:08 - loss: 0.2262 - acc: 0.91 - ETA: 16:14 - loss: 0.2255 - acc: 0.91 - ETA: 17:20 - loss: 0.2258 - acc: 0.91 - ETA: 18:15 - loss: 0.2259 - acc: 0.91 - ETA: 19:06 - loss: 0.2275 - acc: 0.90 - ETA: 19:20 - loss: 0.2269 - acc: 0.90 - ETA: 19:35 - loss: 0.2235 - acc: 0.91 - ETA: 19:47 - loss: 0.2216 - acc: 0.91 - ETA: 19:57 - loss: 0.2217 - acc: 0.91 - ETA: 20:06 - loss: 0.2216 - acc: 0.91 - ETA: 20:11 - loss: 0.2195 - acc: 0.91 - ETA: 20:19 - loss: 0.2182 - acc: 0.91 - ETA: 20:23 - loss: 0.2185 - acc: 0.91 - ETA: 20:23 - loss: 0.2168 - acc: 0.91 - ETA: 20:27 - loss: 0.2161 - acc: 0.91 - ETA: 20:29 - loss: 0.2148 - acc: 0.91 - ETA: 20:31 - loss: 0.2146 - acc: 0.91 - ETA: 20:33 - loss: 0.2128 - acc: 0.91 - ETA: 20:34 - loss: 0.2105 - acc: 0.91 - ETA: 20:37 - loss: 0.2138 - acc: 0.91 - ETA: 20:39 - loss: 0.2118 - acc: 0.91 - ETA: 20:40 - loss: 0.2107 - acc: 0.91 - ETA: 20:41 - loss: 0.2117 - acc: 0.91 - ETA: 20:42 - loss: 0.2120 - acc: 0.91 - ETA: 20:44 - loss: 0.2129 - acc: 0.91 - ETA: 20:44 - loss: 0.2117 - acc: 0.91 - ETA: 20:43 - loss: 0.2116 - acc: 0.91 - ETA: 20:42 - loss: 0.2118 - acc: 0.91 - ETA: 20:42 - loss: 0.2115 - acc: 0.91 - ETA: 20:40 - loss: 0.2111 - acc: 0.91 - ETA: 20:39 - loss: 0.2088 - acc: 0.91 - ETA: 20:35 - loss: 0.2072 - acc: 0.91 - ETA: 20:31 - loss: 0.2089 - acc: 0.91 - ETA: 20:30 - loss: 0.2109 - acc: 0.91 - ETA: 20:29 - loss: 0.2088 - acc: 0.91 - ETA: 20:26 - loss: 0.2072 - acc: 0.91 - ETA: 20:24 - loss: 0.2051 - acc: 0.91 - ETA: 20:20 - loss: 0.2040 - acc: 0.91 - ETA: 20:18 - loss: 0.2038 - acc: 0.91 - ETA: 20:16 - loss: 0.2041 - acc: 0.91 - ETA: 20:13 - loss: 0.2036 - acc: 0.91 - ETA: 20:09 - loss: 0.2020 - acc: 0.91 - ETA: 20:06 - loss: 0.2014 - acc: 0.91 - ETA: 20:02 - loss: 0.2024 - acc: 0.91 - ETA: 19:59 - loss: 0.2016 - acc: 0.91 - ETA: 19:56 - loss: 0.2018 - acc: 0.91 - ETA: 19:53 - loss: 0.2016 - acc: 0.91 - ETA: 19:49 - loss: 0.2008 - acc: 0.91 - ETA: 19:46 - loss: 0.2010 - acc: 0.91 - ETA: 19:42 - loss: 0.2006 - acc: 0.91 - ETA: 19:39 - loss: 0.2010 - acc: 0.91 - ETA: 19:35 - loss: 0.1995 - acc: 0.92 - ETA: 19:31 - loss: 0.2012 - acc: 0.91 - ETA: 19:27 - loss: 0.2036 - acc: 0.91 - ETA: 19:23 - loss: 0.2041 - acc: 0.91 - ETA: 19:20 - loss: 0.2042 - acc: 0.91 - ETA: 19:16 - loss: 0.2050 - acc: 0.91 - ETA: 19:11 - loss: 0.2053 - acc: 0.91 - ETA: 19:05 - loss: 0.2068 - acc: 0.91 - ETA: 19:00 - loss: 0.2079 - acc: 0.91 - ETA: 18:55 - loss: 0.2074 - acc: 0.91 - ETA: 18:51 - loss: 0.2077 - acc: 0.91 - ETA: 18:47 - loss: 0.2068 - acc: 0.91 - ETA: 18:42 - loss: 0.2067 - acc: 0.91 - ETA: 18:37 - loss: 0.2078 - acc: 0.91 - ETA: 18:32 - loss: 0.2069 - acc: 0.91 - ETA: 18:28 - loss: 0.2065 - acc: 0.91 - ETA: 18:23 - loss: 0.2051 - acc: 0.91 - ETA: 18:19 - loss: 0.2045 - acc: 0.91 - ETA: 18:14 - loss: 0.2049 - acc: 0.91 - ETA: 18:09 - loss: 0.2054 - acc: 0.91 - ETA: 18:04 - loss: 0.2063 - acc: 0.91 - ETA: 18:00 - loss: 0.2082 - acc: 0.91 - ETA: 17:55 - loss: 0.2074 - acc: 0.91 - ETA: 17:50 - loss: 0.2070 - acc: 0.91 - ETA: 17:45 - loss: 0.2079 - acc: 0.91 - ETA: 17:40 - loss: 0.2082 - acc: 0.91 - ETA: 17:35 - loss: 0.2079 - acc: 0.91 - ETA: 17:30 - loss: 0.2088 - acc: 0.91 - ETA: 17:25 - loss: 0.2092 - acc: 0.91 - ETA: 17:20 - loss: 0.2089 - acc: 0.91 - ETA: 17:15 - loss: 0.2094 - acc: 0.91 - ETA: 17:10 - loss: 0.2091 - acc: 0.91 - ETA: 17:05 - loss: 0.2108 - acc: 0.91 - ETA: 17:00 - loss: 0.2106 - acc: 0.91 - ETA: 16:55 - loss: 0.2104 - acc: 0.91 - ETA: 16:51 - loss: 0.2103 - acc: 0.91 - ETA: 16:46 - loss: 0.2104 - acc: 0.91 - ETA: 16:40 - loss: 0.2110 - acc: 0.91 - ETA: 16:35 - loss: 0.2112 - acc: 0.91 - ETA: 16:30 - loss: 0.2110 - acc: 0.91 - ETA: 16:25 - loss: 0.2106 - acc: 0.91 - ETA: 16:20 - loss: 0.2109 - acc: 0.91 - ETA: 16:14 - loss: 0.2115 - acc: 0.91 - ETA: 16:09 - loss: 0.2114 - acc: 0.91 - ETA: 16:04 - loss: 0.2121 - acc: 0.91 - ETA: 15:58 - loss: 0.2124 - acc: 0.91 - ETA: 15:53 - loss: 0.2120 - acc: 0.91 - ETA: 15:48 - loss: 0.2119 - acc: 0.91 - ETA: 15:43 - loss: 0.2110 - acc: 0.91 - ETA: 15:38 - loss: 0.2120 - acc: 0.91 - ETA: 15:33 - loss: 0.2126 - acc: 0.91 - ETA: 15:27 - loss: 0.2121 - acc: 0.91 - ETA: 15:22 - loss: 0.2120 - acc: 0.91 - ETA: 15:17 - loss: 0.2128 - acc: 0.91 - ETA: 15:11 - loss: 0.2124 - acc: 0.91 - ETA: 15:06 - loss: 0.2122 - acc: 0.91 - ETA: 15:00 - loss: 0.2127 - acc: 0.91 - ETA: 14:55 - loss: 0.2134 - acc: 0.91 - ETA: 14:50 - loss: 0.2128 - acc: 0.91 - ETA: 14:44 - loss: 0.2132 - acc: 0.91 - ETA: 14:38 - loss: 0.2127 - acc: 0.91 - ETA: 14:33 - loss: 0.2124 - acc: 0.91 - ETA: 14:27 - loss: 0.2128 - acc: 0.91 - ETA: 14:22 - loss: 0.2129 - acc: 0.91 - ETA: 14:17 - loss: 0.2122 - acc: 0.91 - ETA: 14:11 - loss: 0.2123 - acc: 0.91 - ETA: 14:06 - loss: 0.2123 - acc: 0.91 - ETA: 14:01 - loss: 0.2119 - acc: 0.91 - ETA: 13:55 - loss: 0.2113 - acc: 0.91 - ETA: 13:50 - loss: 0.2117 - acc: 0.91 - ETA: 13:44 - loss: 0.2111 - acc: 0.91 - ETA: 13:39 - loss: 0.2108 - acc: 0.91 - ETA: 13:33 - loss: 0.2105 - acc: 0.91 - ETA: 13:28 - loss: 0.2110 - acc: 0.91 - ETA: 13:22 - loss: 0.2107 - acc: 0.91 - ETA: 13:17 - loss: 0.2104 - acc: 0.91 - ETA: 13:11 - loss: 0.2103 - acc: 0.91 - ETA: 13:06 - loss: 0.2101 - acc: 0.91 - ETA: 13:00 - loss: 0.2095 - acc: 0.91 - ETA: 12:55 - loss: 0.2094 - acc: 0.91 - ETA: 12:49 - loss: 0.2102 - acc: 0.91 - ETA: 12:43 - loss: 0.2102 - acc: 0.91 - ETA: 12:37 - loss: 0.2102 - acc: 0.91 - ETA: 12:32 - loss: 0.2098 - acc: 0.91 - ETA: 12:26 - loss: 0.2097 - acc: 0.91 - ETA: 12:20 - loss: 0.2096 - acc: 0.91 - ETA: 12:15 - loss: 0.2093 - acc: 0.91 - ETA: 12:09 - loss: 0.2093 - acc: 0.91 - ETA: 12:03 - loss: 0.2094 - acc: 0.91 - ETA: 11:58 - loss: 0.2094 - acc: 0.91 - ETA: 11:52 - loss: 0.2094 - acc: 0.91 - ETA: 11:47 - loss: 0.2094 - acc: 0.91 - ETA: 11:41 - loss: 0.2098 - acc: 0.91 - ETA: 11:35 - loss: 0.2101 - acc: 0.91 - ETA: 11:30 - loss: 0.2096 - acc: 0.91 - ETA: 11:24 - loss: 0.2093 - acc: 0.91 - ETA: 11:18 - loss: 0.2088 - acc: 0.91 - ETA: 11:13 - loss: 0.2085 - acc: 0.91 - ETA: 11:07 - loss: 0.2080 - acc: 0.91 - ETA: 11:01 - loss: 0.2081 - acc: 0.91 - ETA: 10:56 - loss: 0.2083 - acc: 0.91 - ETA: 10:50 - loss: 0.2083 - acc: 0.91 - ETA: 10:44 - loss: 0.2081 - acc: 0.91 - ETA: 10:39 - loss: 0.2081 - acc: 0.91 - ETA: 10:33 - loss: 0.2083 - acc: 0.91 - ETA: 10:27 - loss: 0.2079 - acc: 0.91 - ETA: 10:21 - loss: 0.2086 - acc: 0.91 - ETA: 10:16 - loss: 0.2089 - acc: 0.91 - ETA: 10:10 - loss: 0.2089 - acc: 0.91 - ETA: 10:05 - loss: 0.2091 - acc: 0.91 - ETA: 9:59 - loss: 0.2092 - acc: 0.9154 - ETA: 9:53 - loss: 0.2089 - acc: 0.915 - ETA: 9:47 - loss: 0.2089 - acc: 0.915 - ETA: 9:41 - loss: 0.2088 - acc: 0.915 - ETA: 9:36 - loss: 0.2086 - acc: 0.915 - ETA: 9:30 - loss: 0.2084 - acc: 0.915 - ETA: 9:24 - loss: 0.2086 - acc: 0.915 - ETA: 9:18 - loss: 0.2085 - acc: 0.915 - ETA: 9:13 - loss: 0.2083 - acc: 0.915 - ETA: 9:07 - loss: 0.2088 - acc: 0.915 - ETA: 9:01 - loss: 0.2083 - acc: 0.916 - ETA: 8:55 - loss: 0.2082 - acc: 0.916 - ETA: 8:50 - loss: 0.2085 - acc: 0.915 - ETA: 8:44 - loss: 0.2087 - acc: 0.915 - ETA: 8:38 - loss: 0.2087 - acc: 0.915 - ETA: 8:33 - loss: 0.2089 - acc: 0.915 - ETA: 8:27 - loss: 0.2083 - acc: 0.916 - ETA: 8:21 - loss: 0.2084 - acc: 0.916 - ETA: 8:16 - loss: 0.2081 - acc: 0.916 - ETA: 8:10 - loss: 0.2076 - acc: 0.916 - ETA: 8:04 - loss: 0.2073 - acc: 0.916 - ETA: 7:58 - loss: 0.2074 - acc: 0.916 - ETA: 7:52 - loss: 0.2074 - acc: 0.916 - ETA: 7:46 - loss: 0.2075 - acc: 0.916 - ETA: 7:41 - loss: 0.2070 - acc: 0.916 - ETA: 7:35 - loss: 0.2070 - acc: 0.916 - ETA: 7:29 - loss: 0.2068 - acc: 0.916 - ETA: 7:23 - loss: 0.2069 - acc: 0.916 - ETA: 7:17 - loss: 0.2067 - acc: 0.916 - ETA: 7:11 - loss: 0.2066 - acc: 0.916 - ETA: 7:06 - loss: 0.2069 - acc: 0.916 - ETA: 7:00 - loss: 0.2070 - acc: 0.916 - ETA: 6:54 - loss: 0.2070 - acc: 0.916 - ETA: 6:48 - loss: 0.2066 - acc: 0.916 - ETA: 6:42 - loss: 0.2071 - acc: 0.916 - ETA: 6:37 - loss: 0.2071 - acc: 0.916 - ETA: 6:31 - loss: 0.2072 - acc: 0.916 - ETA: 6:25 - loss: 0.2071 - acc: 0.9164"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - ETA: 6:19 - loss: 0.2073 - acc: 0.916 - ETA: 6:13 - loss: 0.2073 - acc: 0.916 - ETA: 6:08 - loss: 0.2075 - acc: 0.916 - ETA: 6:02 - loss: 0.2070 - acc: 0.916 - ETA: 5:56 - loss: 0.2074 - acc: 0.916 - ETA: 5:50 - loss: 0.2075 - acc: 0.916 - ETA: 5:44 - loss: 0.2076 - acc: 0.916 - ETA: 5:39 - loss: 0.2076 - acc: 0.916 - ETA: 5:33 - loss: 0.2075 - acc: 0.916 - ETA: 5:27 - loss: 0.2072 - acc: 0.916 - ETA: 5:21 - loss: 0.2070 - acc: 0.916 - ETA: 5:16 - loss: 0.2071 - acc: 0.916 - ETA: 5:10 - loss: 0.2066 - acc: 0.916 - ETA: 5:04 - loss: 0.2064 - acc: 0.916 - ETA: 4:58 - loss: 0.2064 - acc: 0.916 - ETA: 4:52 - loss: 0.2063 - acc: 0.916 - ETA: 4:46 - loss: 0.2063 - acc: 0.916 - ETA: 4:40 - loss: 0.2060 - acc: 0.916 - ETA: 4:35 - loss: 0.2065 - acc: 0.916 - ETA: 4:29 - loss: 0.2062 - acc: 0.916 - ETA: 4:23 - loss: 0.2061 - acc: 0.916 - ETA: 4:17 - loss: 0.2058 - acc: 0.916 - ETA: 4:11 - loss: 0.2059 - acc: 0.916 - ETA: 4:05 - loss: 0.2061 - acc: 0.916 - ETA: 4:00 - loss: 0.2059 - acc: 0.916 - ETA: 3:54 - loss: 0.2059 - acc: 0.916 - ETA: 3:48 - loss: 0.2060 - acc: 0.916 - ETA: 3:42 - loss: 0.2060 - acc: 0.916 - ETA: 3:36 - loss: 0.2061 - acc: 0.916 - ETA: 3:30 - loss: 0.2057 - acc: 0.916 - ETA: 3:25 - loss: 0.2056 - acc: 0.916 - ETA: 3:19 - loss: 0.2055 - acc: 0.917 - ETA: 3:13 - loss: 0.2052 - acc: 0.917 - ETA: 3:07 - loss: 0.2052 - acc: 0.917 - ETA: 3:01 - loss: 0.2052 - acc: 0.917 - ETA: 2:55 - loss: 0.2053 - acc: 0.917 - ETA: 2:50 - loss: 0.2051 - acc: 0.917 - ETA: 2:44 - loss: 0.2047 - acc: 0.917 - ETA: 2:38 - loss: 0.2050 - acc: 0.917 - ETA: 2:32 - loss: 0.2050 - acc: 0.917 - ETA: 2:26 - loss: 0.2050 - acc: 0.917 - ETA: 2:20 - loss: 0.2050 - acc: 0.917 - ETA: 2:14 - loss: 0.2049 - acc: 0.917 - ETA: 2:09 - loss: 0.2050 - acc: 0.917 - ETA: 2:03 - loss: 0.2050 - acc: 0.917 - ETA: 1:57 - loss: 0.2048 - acc: 0.917 - ETA: 1:51 - loss: 0.2050 - acc: 0.917 - ETA: 1:45 - loss: 0.2049 - acc: 0.917 - ETA: 1:39 - loss: 0.2049 - acc: 0.917 - ETA: 1:33 - loss: 0.2050 - acc: 0.917 - ETA: 1:27 - loss: 0.2048 - acc: 0.917 - ETA: 1:22 - loss: 0.2051 - acc: 0.917 - ETA: 1:16 - loss: 0.2052 - acc: 0.917 - ETA: 1:10 - loss: 0.2052 - acc: 0.917 - ETA: 1:04 - loss: 0.2049 - acc: 0.917 - ETA: 58s - loss: 0.2051 - acc: 0.917 - ETA: 52s - loss: 0.2048 - acc: 0.91 - ETA: 46s - loss: 0.2047 - acc: 0.91 - ETA: 41s - loss: 0.2042 - acc: 0.91 - ETA: 35s - loss: 0.2047 - acc: 0.91 - ETA: 29s - loss: 0.2048 - acc: 0.91 - ETA: 23s - loss: 0.2047 - acc: 0.91 - ETA: 17s - loss: 0.2047 - acc: 0.91 - ETA: 11s - loss: 0.2044 - acc: 0.91 - ETA: 5s - loss: 0.2044 - acc: 0.9174 - 1803s 6s/step - loss: 0.2050 - acc: 0.9170 - val_loss: 0.1900 - val_acc: 0.9238\n",
      "Epoch 3/5\n",
      "  1/282 [..............................] - ETA: 56s - loss: 0.2430 - acc: 0.8960"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.268533). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2/282 [..............................] - ETA: 1:40 - loss: 0.2265 - acc: 0.9034"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.348185). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3/282 [..............................] - ETA: 2:15 - loss: 0.2010 - acc: 0.9157"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.371896). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4/282 [..............................] - ETA: 2:20 - loss: 0.2423 - acc: 0.8983"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.320214). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5/282 [..............................] - ETA: 2:05 - loss: 0.2399 - acc: 0.9001"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.332005). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6/282 [..............................] - ETA: 2:10 - loss: 0.2304 - acc: 0.9059"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.351950). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8/282 [..............................] - ETA: 2:16 - loss: 0.2167 - acc: 0.912 - ETA: 2:20 - loss: 0.2098 - acc: 0.9165"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.379781). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9/282 [..............................] - ETA: 2:28 - loss: 0.2163 - acc: 0.9163"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.387666). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12/282 [>.............................] - ETA: 2:33 - loss: 0.2173 - acc: 0.915 - ETA: 2:35 - loss: 0.2092 - acc: 0.918 - ETA: 3:39 - loss: 0.2190 - acc: 0.9130"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.357321). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13/282 [>.............................] - ETA: 6:29 - loss: 0.2123 - acc: 0.9159"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.337376). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14/282 [>.............................] - ETA: 8:52 - loss: 0.2133 - acc: 0.9145"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.365206). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15/282 [>.............................] - ETA: 10:54 - loss: 0.2110 - acc: 0.9157"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.383276). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17/282 [>.............................] - ETA: 12:48 - loss: 0.2105 - acc: 0.91 - ETA: 14:15 - loss: 0.2068 - acc: 0.9176"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.373018). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19/282 [=>............................] - ETA: 15:39 - loss: 0.2064 - acc: 0.91 - ETA: 16:49 - loss: 0.2051 - acc: 0.9174"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.354948). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20/282 [=>............................] - ETA: 17:52 - loss: 0.1992 - acc: 0.9198"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.352995). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 24/282 [=>............................] - ETA: 18:46 - loss: 0.2020 - acc: 0.91 - ETA: 19:35 - loss: 0.2048 - acc: 0.91 - ETA: 19:51 - loss: 0.2035 - acc: 0.91 - ETA: 20:02 - loss: 0.2068 - acc: 0.9169"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.216292). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228/282 [=======================>......] - ETA: 20:08 - loss: 0.2071 - acc: 0.91 - ETA: 20:17 - loss: 0.2110 - acc: 0.91 - ETA: 20:24 - loss: 0.2110 - acc: 0.91 - ETA: 20:28 - loss: 0.2132 - acc: 0.91 - ETA: 20:34 - loss: 0.2149 - acc: 0.91 - ETA: 20:38 - loss: 0.2147 - acc: 0.91 - ETA: 20:42 - loss: 0.2126 - acc: 0.91 - ETA: 20:46 - loss: 0.2109 - acc: 0.91 - ETA: 20:50 - loss: 0.2096 - acc: 0.91 - ETA: 20:50 - loss: 0.2108 - acc: 0.91 - ETA: 20:52 - loss: 0.2115 - acc: 0.91 - ETA: 20:54 - loss: 0.2133 - acc: 0.91 - ETA: 20:54 - loss: 0.2129 - acc: 0.91 - ETA: 20:53 - loss: 0.2122 - acc: 0.91 - ETA: 20:54 - loss: 0.2121 - acc: 0.91 - ETA: 20:55 - loss: 0.2124 - acc: 0.91 - ETA: 20:56 - loss: 0.2129 - acc: 0.91 - ETA: 20:54 - loss: 0.2117 - acc: 0.91 - ETA: 20:52 - loss: 0.2104 - acc: 0.91 - ETA: 20:51 - loss: 0.2084 - acc: 0.91 - ETA: 20:48 - loss: 0.2087 - acc: 0.91 - ETA: 20:47 - loss: 0.2061 - acc: 0.91 - ETA: 20:44 - loss: 0.2049 - acc: 0.91 - ETA: 20:42 - loss: 0.2073 - acc: 0.91 - ETA: 20:40 - loss: 0.2053 - acc: 0.91 - ETA: 20:38 - loss: 0.2063 - acc: 0.91 - ETA: 20:35 - loss: 0.2052 - acc: 0.91 - ETA: 20:32 - loss: 0.2051 - acc: 0.91 - ETA: 20:29 - loss: 0.2037 - acc: 0.91 - ETA: 20:28 - loss: 0.2030 - acc: 0.91 - ETA: 20:25 - loss: 0.2025 - acc: 0.91 - ETA: 20:22 - loss: 0.2013 - acc: 0.91 - ETA: 20:19 - loss: 0.2004 - acc: 0.91 - ETA: 20:15 - loss: 0.2000 - acc: 0.91 - ETA: 20:12 - loss: 0.1996 - acc: 0.91 - ETA: 20:09 - loss: 0.1994 - acc: 0.91 - ETA: 20:06 - loss: 0.2002 - acc: 0.91 - ETA: 20:02 - loss: 0.2000 - acc: 0.91 - ETA: 19:59 - loss: 0.2002 - acc: 0.91 - ETA: 19:55 - loss: 0.1998 - acc: 0.91 - ETA: 19:51 - loss: 0.2004 - acc: 0.91 - ETA: 19:47 - loss: 0.2009 - acc: 0.91 - ETA: 19:43 - loss: 0.2000 - acc: 0.91 - ETA: 19:39 - loss: 0.2027 - acc: 0.91 - ETA: 19:36 - loss: 0.2032 - acc: 0.91 - ETA: 19:32 - loss: 0.2024 - acc: 0.91 - ETA: 19:28 - loss: 0.2018 - acc: 0.91 - ETA: 19:24 - loss: 0.2029 - acc: 0.91 - ETA: 19:20 - loss: 0.2036 - acc: 0.91 - ETA: 19:16 - loss: 0.2030 - acc: 0.91 - ETA: 19:13 - loss: 0.2030 - acc: 0.91 - ETA: 19:08 - loss: 0.2023 - acc: 0.91 - ETA: 19:05 - loss: 0.2017 - acc: 0.91 - ETA: 19:01 - loss: 0.2038 - acc: 0.91 - ETA: 18:56 - loss: 0.2032 - acc: 0.91 - ETA: 18:51 - loss: 0.2044 - acc: 0.91 - ETA: 18:46 - loss: 0.2057 - acc: 0.91 - ETA: 18:41 - loss: 0.2058 - acc: 0.91 - ETA: 18:37 - loss: 0.2064 - acc: 0.91 - ETA: 18:32 - loss: 0.2068 - acc: 0.91 - ETA: 18:27 - loss: 0.2076 - acc: 0.91 - ETA: 18:22 - loss: 0.2080 - acc: 0.91 - ETA: 18:18 - loss: 0.2080 - acc: 0.91 - ETA: 18:13 - loss: 0.2083 - acc: 0.91 - ETA: 18:09 - loss: 0.2107 - acc: 0.91 - ETA: 18:04 - loss: 0.2118 - acc: 0.91 - ETA: 18:00 - loss: 0.2115 - acc: 0.91 - ETA: 17:55 - loss: 0.2122 - acc: 0.91 - ETA: 17:51 - loss: 0.2111 - acc: 0.91 - ETA: 17:46 - loss: 0.2124 - acc: 0.91 - ETA: 17:42 - loss: 0.2125 - acc: 0.91 - ETA: 17:36 - loss: 0.2127 - acc: 0.91 - ETA: 17:32 - loss: 0.2131 - acc: 0.91 - ETA: 17:27 - loss: 0.2127 - acc: 0.91 - ETA: 17:22 - loss: 0.2134 - acc: 0.91 - ETA: 17:16 - loss: 0.2133 - acc: 0.91 - ETA: 17:11 - loss: 0.2136 - acc: 0.91 - ETA: 17:08 - loss: 0.2141 - acc: 0.91 - ETA: 17:02 - loss: 0.2137 - acc: 0.91 - ETA: 16:58 - loss: 0.2138 - acc: 0.91 - ETA: 16:53 - loss: 0.2137 - acc: 0.91 - ETA: 16:48 - loss: 0.2143 - acc: 0.91 - ETA: 16:42 - loss: 0.2136 - acc: 0.91 - ETA: 16:37 - loss: 0.2143 - acc: 0.91 - ETA: 16:32 - loss: 0.2145 - acc: 0.91 - ETA: 16:26 - loss: 0.2140 - acc: 0.91 - ETA: 16:21 - loss: 0.2135 - acc: 0.91 - ETA: 16:17 - loss: 0.2132 - acc: 0.91 - ETA: 16:11 - loss: 0.2135 - acc: 0.91 - ETA: 16:06 - loss: 0.2129 - acc: 0.91 - ETA: 16:00 - loss: 0.2125 - acc: 0.91 - ETA: 15:55 - loss: 0.2118 - acc: 0.91 - ETA: 15:50 - loss: 0.2111 - acc: 0.91 - ETA: 15:45 - loss: 0.2104 - acc: 0.91 - ETA: 15:39 - loss: 0.2095 - acc: 0.91 - ETA: 15:34 - loss: 0.2090 - acc: 0.91 - ETA: 15:28 - loss: 0.2084 - acc: 0.91 - ETA: 15:22 - loss: 0.2080 - acc: 0.91 - ETA: 15:17 - loss: 0.2076 - acc: 0.91 - ETA: 15:11 - loss: 0.2072 - acc: 0.91 - ETA: 15:06 - loss: 0.2080 - acc: 0.91 - ETA: 15:01 - loss: 0.2081 - acc: 0.91 - ETA: 14:55 - loss: 0.2079 - acc: 0.91 - ETA: 14:50 - loss: 0.2086 - acc: 0.91 - ETA: 14:44 - loss: 0.2086 - acc: 0.91 - ETA: 14:39 - loss: 0.2084 - acc: 0.91 - ETA: 14:33 - loss: 0.2080 - acc: 0.91 - ETA: 14:28 - loss: 0.2082 - acc: 0.91 - ETA: 14:23 - loss: 0.2084 - acc: 0.91 - ETA: 14:17 - loss: 0.2087 - acc: 0.91 - ETA: 14:12 - loss: 0.2086 - acc: 0.91 - ETA: 14:06 - loss: 0.2083 - acc: 0.91 - ETA: 14:00 - loss: 0.2082 - acc: 0.91 - ETA: 13:55 - loss: 0.2075 - acc: 0.91 - ETA: 13:49 - loss: 0.2076 - acc: 0.91 - ETA: 13:44 - loss: 0.2087 - acc: 0.91 - ETA: 13:39 - loss: 0.2085 - acc: 0.91 - ETA: 13:33 - loss: 0.2087 - acc: 0.91 - ETA: 13:27 - loss: 0.2083 - acc: 0.91 - ETA: 13:22 - loss: 0.2087 - acc: 0.91 - ETA: 13:16 - loss: 0.2086 - acc: 0.91 - ETA: 13:10 - loss: 0.2084 - acc: 0.91 - ETA: 13:05 - loss: 0.2079 - acc: 0.91 - ETA: 12:59 - loss: 0.2079 - acc: 0.91 - ETA: 12:53 - loss: 0.2076 - acc: 0.91 - ETA: 12:48 - loss: 0.2078 - acc: 0.91 - ETA: 12:42 - loss: 0.2076 - acc: 0.91 - ETA: 12:37 - loss: 0.2083 - acc: 0.91 - ETA: 12:31 - loss: 0.2090 - acc: 0.91 - ETA: 12:25 - loss: 0.2093 - acc: 0.91 - ETA: 12:20 - loss: 0.2091 - acc: 0.91 - ETA: 12:14 - loss: 0.2093 - acc: 0.91 - ETA: 12:08 - loss: 0.2095 - acc: 0.91 - ETA: 12:03 - loss: 0.2093 - acc: 0.91 - ETA: 11:57 - loss: 0.2090 - acc: 0.91 - ETA: 11:51 - loss: 0.2087 - acc: 0.91 - ETA: 11:46 - loss: 0.2085 - acc: 0.91 - ETA: 11:40 - loss: 0.2086 - acc: 0.91 - ETA: 11:34 - loss: 0.2089 - acc: 0.91 - ETA: 11:29 - loss: 0.2087 - acc: 0.91 - ETA: 11:23 - loss: 0.2088 - acc: 0.91 - ETA: 11:17 - loss: 0.2087 - acc: 0.91 - ETA: 11:11 - loss: 0.2095 - acc: 0.91 - ETA: 11:05 - loss: 0.2092 - acc: 0.91 - ETA: 11:00 - loss: 0.2088 - acc: 0.91 - ETA: 10:54 - loss: 0.2085 - acc: 0.91 - ETA: 10:48 - loss: 0.2080 - acc: 0.91 - ETA: 10:42 - loss: 0.2078 - acc: 0.91 - ETA: 10:36 - loss: 0.2082 - acc: 0.91 - ETA: 10:30 - loss: 0.2079 - acc: 0.91 - ETA: 10:25 - loss: 0.2080 - acc: 0.91 - ETA: 10:19 - loss: 0.2078 - acc: 0.91 - ETA: 10:13 - loss: 0.2076 - acc: 0.91 - ETA: 10:07 - loss: 0.2077 - acc: 0.91 - ETA: 10:02 - loss: 0.2075 - acc: 0.91 - ETA: 9:56 - loss: 0.2081 - acc: 0.9160 - ETA: 9:50 - loss: 0.2079 - acc: 0.916 - ETA: 9:44 - loss: 0.2075 - acc: 0.916 - ETA: 9:38 - loss: 0.2072 - acc: 0.916 - ETA: 9:32 - loss: 0.2069 - acc: 0.916 - ETA: 9:27 - loss: 0.2074 - acc: 0.916 - ETA: 9:21 - loss: 0.2078 - acc: 0.916 - ETA: 9:15 - loss: 0.2075 - acc: 0.916 - ETA: 9:09 - loss: 0.2072 - acc: 0.916 - ETA: 9:03 - loss: 0.2068 - acc: 0.916 - ETA: 8:58 - loss: 0.2069 - acc: 0.916 - ETA: 8:52 - loss: 0.2068 - acc: 0.916 - ETA: 8:46 - loss: 0.2067 - acc: 0.916 - ETA: 8:40 - loss: 0.2064 - acc: 0.916 - ETA: 8:35 - loss: 0.2065 - acc: 0.916 - ETA: 8:29 - loss: 0.2062 - acc: 0.916 - ETA: 8:23 - loss: 0.2061 - acc: 0.916 - ETA: 8:17 - loss: 0.2056 - acc: 0.917 - ETA: 8:11 - loss: 0.2059 - acc: 0.916 - ETA: 8:05 - loss: 0.2055 - acc: 0.917 - ETA: 8:00 - loss: 0.2059 - acc: 0.917 - ETA: 7:54 - loss: 0.2059 - acc: 0.917 - ETA: 7:48 - loss: 0.2057 - acc: 0.917 - ETA: 7:42 - loss: 0.2060 - acc: 0.916 - ETA: 7:36 - loss: 0.2059 - acc: 0.916 - ETA: 7:30 - loss: 0.2061 - acc: 0.916 - ETA: 7:24 - loss: 0.2062 - acc: 0.916 - ETA: 7:19 - loss: 0.2066 - acc: 0.916 - ETA: 7:13 - loss: 0.2064 - acc: 0.916 - ETA: 7:07 - loss: 0.2063 - acc: 0.916 - ETA: 7:01 - loss: 0.2060 - acc: 0.916 - ETA: 6:55 - loss: 0.2058 - acc: 0.917 - ETA: 6:50 - loss: 0.2055 - acc: 0.917 - ETA: 6:44 - loss: 0.2057 - acc: 0.917 - ETA: 6:38 - loss: 0.2062 - acc: 0.916 - ETA: 6:32 - loss: 0.2063 - acc: 0.916 - ETA: 6:26 - loss: 0.2063 - acc: 0.916 - ETA: 6:21 - loss: 0.2061 - acc: 0.916 - ETA: 6:15 - loss: 0.2061 - acc: 0.916 - ETA: 6:09 - loss: 0.2057 - acc: 0.917 - ETA: 6:03 - loss: 0.2059 - acc: 0.916 - ETA: 5:58 - loss: 0.2061 - acc: 0.916 - ETA: 5:52 - loss: 0.2061 - acc: 0.916 - ETA: 5:46 - loss: 0.2056 - acc: 0.917 - ETA: 5:40 - loss: 0.2052 - acc: 0.917 - ETA: 5:34 - loss: 0.2057 - acc: 0.917 - ETA: 5:28 - loss: 0.2063 - acc: 0.916 - ETA: 5:22 - loss: 0.2061 - acc: 0.917 - ETA: 5:16 - loss: 0.2061 - acc: 0.9169"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - ETA: 5:11 - loss: 0.2059 - acc: 0.917 - ETA: 5:05 - loss: 0.2058 - acc: 0.917 - ETA: 4:59 - loss: 0.2059 - acc: 0.917 - ETA: 4:53 - loss: 0.2059 - acc: 0.917 - ETA: 4:47 - loss: 0.2061 - acc: 0.917 - ETA: 4:41 - loss: 0.2059 - acc: 0.917 - ETA: 4:35 - loss: 0.2057 - acc: 0.917 - ETA: 4:30 - loss: 0.2054 - acc: 0.917 - ETA: 4:24 - loss: 0.2056 - acc: 0.917 - ETA: 4:18 - loss: 0.2059 - acc: 0.917 - ETA: 4:12 - loss: 0.2059 - acc: 0.917 - ETA: 4:06 - loss: 0.2058 - acc: 0.917 - ETA: 4:00 - loss: 0.2062 - acc: 0.917 - ETA: 3:55 - loss: 0.2059 - acc: 0.917 - ETA: 3:49 - loss: 0.2058 - acc: 0.917 - ETA: 3:43 - loss: 0.2056 - acc: 0.917 - ETA: 3:37 - loss: 0.2052 - acc: 0.917 - ETA: 3:31 - loss: 0.2050 - acc: 0.917 - ETA: 3:25 - loss: 0.2049 - acc: 0.917 - ETA: 3:19 - loss: 0.2047 - acc: 0.917 - ETA: 3:14 - loss: 0.2050 - acc: 0.917 - ETA: 3:08 - loss: 0.2054 - acc: 0.917 - ETA: 3:02 - loss: 0.2053 - acc: 0.917 - ETA: 2:56 - loss: 0.2054 - acc: 0.917 - ETA: 2:50 - loss: 0.2050 - acc: 0.917 - ETA: 2:44 - loss: 0.2048 - acc: 0.917 - ETA: 2:38 - loss: 0.2048 - acc: 0.917 - ETA: 2:32 - loss: 0.2051 - acc: 0.917 - ETA: 2:27 - loss: 0.2055 - acc: 0.917 - ETA: 2:21 - loss: 0.2053 - acc: 0.917 - ETA: 2:15 - loss: 0.2049 - acc: 0.917 - ETA: 2:09 - loss: 0.2048 - acc: 0.917 - ETA: 2:03 - loss: 0.2045 - acc: 0.917 - ETA: 1:57 - loss: 0.2044 - acc: 0.918 - ETA: 1:51 - loss: 0.2040 - acc: 0.918 - ETA: 1:46 - loss: 0.2040 - acc: 0.918 - ETA: 1:40 - loss: 0.2037 - acc: 0.918 - ETA: 1:34 - loss: 0.2038 - acc: 0.918 - ETA: 1:28 - loss: 0.2043 - acc: 0.917 - ETA: 1:22 - loss: 0.2042 - acc: 0.918 - ETA: 1:16 - loss: 0.2046 - acc: 0.917 - ETA: 1:10 - loss: 0.2045 - acc: 0.917 - ETA: 1:04 - loss: 0.2046 - acc: 0.917 - ETA: 58s - loss: 0.2045 - acc: 0.917 - ETA: 53s - loss: 0.2043 - acc: 0.91 - ETA: 47s - loss: 0.2044 - acc: 0.91 - ETA: 41s - loss: 0.2043 - acc: 0.91 - ETA: 35s - loss: 0.2048 - acc: 0.91 - ETA: 29s - loss: 0.2054 - acc: 0.91 - ETA: 23s - loss: 0.2051 - acc: 0.91 - ETA: 17s - loss: 0.2048 - acc: 0.91 - ETA: 11s - loss: 0.2047 - acc: 0.91 - ETA: 5s - loss: 0.2049 - acc: 0.9175 - 1810s 6s/step - loss: 0.2051 - acc: 0.9174 - val_loss: 0.1888 - val_acc: 0.9254\n",
      "Epoch 4/5\n",
      "  1/282 [..............................] - ETA: 56s - loss: 0.1821 - acc: 0.9301"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.243145). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2/282 [..............................] - ETA: 1:38 - loss: 0.1664 - acc: 0.9321"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.341293). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8/282 [..............................] - ETA: 2:17 - loss: 0.1902 - acc: 0.923 - ETA: 1:57 - loss: 0.2157 - acc: 0.915 - ETA: 1:47 - loss: 0.2114 - acc: 0.916 - ETA: 1:40 - loss: 0.1956 - acc: 0.922 - ETA: 1:51 - loss: 0.2034 - acc: 0.919 - ETA: 2:00 - loss: 0.2071 - acc: 0.9191"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.334462). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9/282 [..............................] - ETA: 2:08 - loss: 0.2055 - acc: 0.9198"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.382947). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10/282 [>.............................] - ETA: 2:12 - loss: 0.2154 - acc: 0.9150"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.343317). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13/282 [>.............................] - ETA: 2:14 - loss: 0.2125 - acc: 0.915 - ETA: 3:32 - loss: 0.2072 - acc: 0.918 - ETA: 6:19 - loss: 0.2129 - acc: 0.9155"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.404364). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14/282 [>.............................] - ETA: 8:50 - loss: 0.2126 - acc: 0.9158"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.406234). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16/282 [>.............................] - ETA: 10:54 - loss: 0.2059 - acc: 0.91 - ETA: 12:43 - loss: 0.2032 - acc: 0.9192"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.390110). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17/282 [>.............................] - ETA: 14:14 - loss: 0.2029 - acc: 0.9188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.384818). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19/282 [=>............................] - ETA: 15:33 - loss: 0.1992 - acc: 0.92 - ETA: 16:44 - loss: 0.1976 - acc: 0.9208"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.282697). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223/282 [======================>.......] - ETA: 17:48 - loss: 0.1975 - acc: 0.92 - ETA: 18:47 - loss: 0.2049 - acc: 0.91 - ETA: 19:37 - loss: 0.2027 - acc: 0.91 - ETA: 19:50 - loss: 0.2022 - acc: 0.91 - ETA: 19:58 - loss: 0.1979 - acc: 0.92 - ETA: 20:05 - loss: 0.1968 - acc: 0.92 - ETA: 20:13 - loss: 0.1951 - acc: 0.92 - ETA: 20:20 - loss: 0.1943 - acc: 0.92 - ETA: 20:26 - loss: 0.1934 - acc: 0.92 - ETA: 20:32 - loss: 0.1929 - acc: 0.92 - ETA: 20:34 - loss: 0.1909 - acc: 0.92 - ETA: 20:35 - loss: 0.1900 - acc: 0.92 - ETA: 20:39 - loss: 0.1899 - acc: 0.92 - ETA: 20:41 - loss: 0.1888 - acc: 0.92 - ETA: 20:45 - loss: 0.1884 - acc: 0.92 - ETA: 20:46 - loss: 0.1890 - acc: 0.92 - ETA: 20:49 - loss: 0.1886 - acc: 0.92 - ETA: 20:50 - loss: 0.1865 - acc: 0.92 - ETA: 20:50 - loss: 0.1877 - acc: 0.92 - ETA: 20:50 - loss: 0.1859 - acc: 0.92 - ETA: 20:50 - loss: 0.1890 - acc: 0.92 - ETA: 20:49 - loss: 0.1881 - acc: 0.92 - ETA: 20:46 - loss: 0.1868 - acc: 0.92 - ETA: 20:45 - loss: 0.1877 - acc: 0.92 - ETA: 20:44 - loss: 0.1880 - acc: 0.92 - ETA: 20:41 - loss: 0.1868 - acc: 0.92 - ETA: 20:38 - loss: 0.1867 - acc: 0.92 - ETA: 20:37 - loss: 0.1854 - acc: 0.92 - ETA: 20:34 - loss: 0.1840 - acc: 0.92 - ETA: 20:31 - loss: 0.1835 - acc: 0.92 - ETA: 20:30 - loss: 0.1843 - acc: 0.92 - ETA: 20:27 - loss: 0.1850 - acc: 0.92 - ETA: 20:24 - loss: 0.1869 - acc: 0.92 - ETA: 20:20 - loss: 0.1858 - acc: 0.92 - ETA: 20:16 - loss: 0.1847 - acc: 0.92 - ETA: 20:13 - loss: 0.1858 - acc: 0.92 - ETA: 20:09 - loss: 0.1854 - acc: 0.92 - ETA: 20:07 - loss: 0.1870 - acc: 0.92 - ETA: 20:03 - loss: 0.1863 - acc: 0.92 - ETA: 20:00 - loss: 0.1861 - acc: 0.92 - ETA: 19:57 - loss: 0.1864 - acc: 0.92 - ETA: 19:53 - loss: 0.1858 - acc: 0.92 - ETA: 19:51 - loss: 0.1861 - acc: 0.92 - ETA: 19:48 - loss: 0.1851 - acc: 0.92 - ETA: 19:45 - loss: 0.1853 - acc: 0.92 - ETA: 19:41 - loss: 0.1847 - acc: 0.92 - ETA: 19:38 - loss: 0.1857 - acc: 0.92 - ETA: 19:35 - loss: 0.1887 - acc: 0.92 - ETA: 19:31 - loss: 0.1903 - acc: 0.92 - ETA: 19:27 - loss: 0.1890 - acc: 0.92 - ETA: 19:23 - loss: 0.1884 - acc: 0.92 - ETA: 19:19 - loss: 0.1880 - acc: 0.92 - ETA: 19:15 - loss: 0.1878 - acc: 0.92 - ETA: 19:11 - loss: 0.1881 - acc: 0.92 - ETA: 19:08 - loss: 0.1881 - acc: 0.92 - ETA: 19:03 - loss: 0.1881 - acc: 0.92 - ETA: 18:59 - loss: 0.1878 - acc: 0.92 - ETA: 18:53 - loss: 0.1874 - acc: 0.92 - ETA: 18:49 - loss: 0.1868 - acc: 0.92 - ETA: 18:44 - loss: 0.1861 - acc: 0.92 - ETA: 18:39 - loss: 0.1861 - acc: 0.92 - ETA: 18:35 - loss: 0.1868 - acc: 0.92 - ETA: 18:31 - loss: 0.1859 - acc: 0.92 - ETA: 18:26 - loss: 0.1854 - acc: 0.92 - ETA: 18:22 - loss: 0.1843 - acc: 0.92 - ETA: 18:17 - loss: 0.1838 - acc: 0.92 - ETA: 18:12 - loss: 0.1832 - acc: 0.92 - ETA: 18:08 - loss: 0.1839 - acc: 0.92 - ETA: 18:03 - loss: 0.1844 - acc: 0.92 - ETA: 17:59 - loss: 0.1835 - acc: 0.92 - ETA: 17:53 - loss: 0.1836 - acc: 0.92 - ETA: 17:48 - loss: 0.1828 - acc: 0.92 - ETA: 17:44 - loss: 0.1837 - acc: 0.92 - ETA: 17:38 - loss: 0.1848 - acc: 0.92 - ETA: 17:32 - loss: 0.1854 - acc: 0.92 - ETA: 17:27 - loss: 0.1850 - acc: 0.92 - ETA: 17:22 - loss: 0.1844 - acc: 0.92 - ETA: 17:17 - loss: 0.1853 - acc: 0.92 - ETA: 17:13 - loss: 0.1861 - acc: 0.92 - ETA: 17:08 - loss: 0.1863 - acc: 0.92 - ETA: 17:03 - loss: 0.1878 - acc: 0.92 - ETA: 16:58 - loss: 0.1901 - acc: 0.92 - ETA: 16:52 - loss: 0.1898 - acc: 0.92 - ETA: 16:47 - loss: 0.1906 - acc: 0.92 - ETA: 16:42 - loss: 0.1899 - acc: 0.92 - ETA: 16:38 - loss: 0.1896 - acc: 0.92 - ETA: 16:33 - loss: 0.1897 - acc: 0.92 - ETA: 16:28 - loss: 0.1903 - acc: 0.92 - ETA: 16:23 - loss: 0.1906 - acc: 0.92 - ETA: 16:17 - loss: 0.1905 - acc: 0.92 - ETA: 16:12 - loss: 0.1903 - acc: 0.92 - ETA: 16:07 - loss: 0.1902 - acc: 0.92 - ETA: 16:02 - loss: 0.1901 - acc: 0.92 - ETA: 15:57 - loss: 0.1898 - acc: 0.92 - ETA: 15:51 - loss: 0.1901 - acc: 0.92 - ETA: 15:46 - loss: 0.1897 - acc: 0.92 - ETA: 15:41 - loss: 0.1898 - acc: 0.92 - ETA: 15:35 - loss: 0.1895 - acc: 0.92 - ETA: 15:30 - loss: 0.1896 - acc: 0.92 - ETA: 15:25 - loss: 0.1895 - acc: 0.92 - ETA: 15:20 - loss: 0.1890 - acc: 0.92 - ETA: 15:14 - loss: 0.1902 - acc: 0.92 - ETA: 15:09 - loss: 0.1909 - acc: 0.92 - ETA: 15:03 - loss: 0.1908 - acc: 0.92 - ETA: 14:58 - loss: 0.1903 - acc: 0.92 - ETA: 14:53 - loss: 0.1902 - acc: 0.92 - ETA: 14:48 - loss: 0.1902 - acc: 0.92 - ETA: 14:42 - loss: 0.1903 - acc: 0.92 - ETA: 14:37 - loss: 0.1908 - acc: 0.92 - ETA: 14:32 - loss: 0.1904 - acc: 0.92 - ETA: 14:27 - loss: 0.1906 - acc: 0.92 - ETA: 14:21 - loss: 0.1905 - acc: 0.92 - ETA: 14:16 - loss: 0.1905 - acc: 0.92 - ETA: 14:10 - loss: 0.1906 - acc: 0.92 - ETA: 14:05 - loss: 0.1911 - acc: 0.92 - ETA: 13:59 - loss: 0.1908 - acc: 0.92 - ETA: 13:54 - loss: 0.1902 - acc: 0.92 - ETA: 13:49 - loss: 0.1900 - acc: 0.92 - ETA: 13:43 - loss: 0.1905 - acc: 0.92 - ETA: 13:38 - loss: 0.1910 - acc: 0.92 - ETA: 13:33 - loss: 0.1911 - acc: 0.92 - ETA: 13:27 - loss: 0.1907 - acc: 0.92 - ETA: 13:22 - loss: 0.1915 - acc: 0.92 - ETA: 13:16 - loss: 0.1916 - acc: 0.92 - ETA: 13:11 - loss: 0.1922 - acc: 0.92 - ETA: 13:05 - loss: 0.1920 - acc: 0.92 - ETA: 13:00 - loss: 0.1917 - acc: 0.92 - ETA: 12:54 - loss: 0.1918 - acc: 0.92 - ETA: 12:49 - loss: 0.1922 - acc: 0.92 - ETA: 12:43 - loss: 0.1919 - acc: 0.92 - ETA: 12:38 - loss: 0.1921 - acc: 0.92 - ETA: 12:32 - loss: 0.1921 - acc: 0.92 - ETA: 12:27 - loss: 0.1918 - acc: 0.92 - ETA: 12:21 - loss: 0.1920 - acc: 0.92 - ETA: 12:16 - loss: 0.1915 - acc: 0.92 - ETA: 12:10 - loss: 0.1911 - acc: 0.92 - ETA: 12:05 - loss: 0.1906 - acc: 0.92 - ETA: 11:59 - loss: 0.1918 - acc: 0.92 - ETA: 11:53 - loss: 0.1921 - acc: 0.92 - ETA: 11:48 - loss: 0.1916 - acc: 0.92 - ETA: 11:43 - loss: 0.1922 - acc: 0.92 - ETA: 11:38 - loss: 0.1929 - acc: 0.92 - ETA: 11:32 - loss: 0.1930 - acc: 0.92 - ETA: 11:26 - loss: 0.1931 - acc: 0.92 - ETA: 11:20 - loss: 0.1930 - acc: 0.92 - ETA: 11:14 - loss: 0.1932 - acc: 0.92 - ETA: 11:09 - loss: 0.1934 - acc: 0.92 - ETA: 11:03 - loss: 0.1931 - acc: 0.92 - ETA: 10:58 - loss: 0.1935 - acc: 0.92 - ETA: 10:52 - loss: 0.1935 - acc: 0.92 - ETA: 10:46 - loss: 0.1935 - acc: 0.92 - ETA: 10:40 - loss: 0.1934 - acc: 0.92 - ETA: 10:35 - loss: 0.1938 - acc: 0.92 - ETA: 10:29 - loss: 0.1934 - acc: 0.92 - ETA: 10:24 - loss: 0.1935 - acc: 0.92 - ETA: 10:18 - loss: 0.1939 - acc: 0.92 - ETA: 10:12 - loss: 0.1947 - acc: 0.92 - ETA: 10:07 - loss: 0.1949 - acc: 0.92 - ETA: 10:01 - loss: 0.1948 - acc: 0.92 - ETA: 9:55 - loss: 0.1947 - acc: 0.9207 - ETA: 9:50 - loss: 0.1954 - acc: 0.920 - ETA: 9:44 - loss: 0.1951 - acc: 0.920 - ETA: 9:38 - loss: 0.1952 - acc: 0.920 - ETA: 9:32 - loss: 0.1953 - acc: 0.920 - ETA: 9:27 - loss: 0.1953 - acc: 0.920 - ETA: 9:21 - loss: 0.1951 - acc: 0.920 - ETA: 9:15 - loss: 0.1952 - acc: 0.920 - ETA: 9:09 - loss: 0.1953 - acc: 0.920 - ETA: 9:04 - loss: 0.1949 - acc: 0.920 - ETA: 8:58 - loss: 0.1952 - acc: 0.920 - ETA: 8:53 - loss: 0.1951 - acc: 0.920 - ETA: 8:47 - loss: 0.1949 - acc: 0.920 - ETA: 8:41 - loss: 0.1948 - acc: 0.920 - ETA: 8:36 - loss: 0.1953 - acc: 0.920 - ETA: 8:30 - loss: 0.1952 - acc: 0.920 - ETA: 8:25 - loss: 0.1950 - acc: 0.920 - ETA: 8:19 - loss: 0.1952 - acc: 0.920 - ETA: 8:13 - loss: 0.1952 - acc: 0.920 - ETA: 8:07 - loss: 0.1953 - acc: 0.920 - ETA: 8:02 - loss: 0.1951 - acc: 0.920 - ETA: 7:56 - loss: 0.1949 - acc: 0.920 - ETA: 7:50 - loss: 0.1951 - acc: 0.920 - ETA: 7:45 - loss: 0.1946 - acc: 0.920 - ETA: 7:39 - loss: 0.1949 - acc: 0.920 - ETA: 7:33 - loss: 0.1950 - acc: 0.920 - ETA: 7:27 - loss: 0.1948 - acc: 0.920 - ETA: 7:22 - loss: 0.1949 - acc: 0.920 - ETA: 7:16 - loss: 0.1948 - acc: 0.920 - ETA: 7:10 - loss: 0.1947 - acc: 0.920 - ETA: 7:05 - loss: 0.1947 - acc: 0.920 - ETA: 6:59 - loss: 0.1949 - acc: 0.920 - ETA: 6:53 - loss: 0.1948 - acc: 0.920 - ETA: 6:47 - loss: 0.1951 - acc: 0.920 - ETA: 6:42 - loss: 0.1950 - acc: 0.920 - ETA: 6:36 - loss: 0.1948 - acc: 0.920 - ETA: 6:30 - loss: 0.1947 - acc: 0.920 - ETA: 6:24 - loss: 0.1946 - acc: 0.920 - ETA: 6:18 - loss: 0.1945 - acc: 0.920 - ETA: 6:13 - loss: 0.1946 - acc: 0.920 - ETA: 6:07 - loss: 0.1944 - acc: 0.920 - ETA: 6:01 - loss: 0.1943 - acc: 0.920 - ETA: 5:55 - loss: 0.1943 - acc: 0.921 - ETA: 5:50 - loss: 0.1946 - acc: 0.920 - ETA: 5:44 - loss: 0.1951 - acc: 0.9206"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - ETA: 5:38 - loss: 0.1951 - acc: 0.920 - ETA: 5:32 - loss: 0.1955 - acc: 0.920 - ETA: 5:27 - loss: 0.1955 - acc: 0.920 - ETA: 5:21 - loss: 0.1955 - acc: 0.920 - ETA: 5:15 - loss: 0.1954 - acc: 0.920 - ETA: 5:09 - loss: 0.1952 - acc: 0.920 - ETA: 5:03 - loss: 0.1952 - acc: 0.920 - ETA: 4:58 - loss: 0.1949 - acc: 0.920 - ETA: 4:52 - loss: 0.1946 - acc: 0.920 - ETA: 4:46 - loss: 0.1947 - acc: 0.920 - ETA: 4:40 - loss: 0.1945 - acc: 0.920 - ETA: 4:35 - loss: 0.1942 - acc: 0.921 - ETA: 4:29 - loss: 0.1947 - acc: 0.920 - ETA: 4:23 - loss: 0.1947 - acc: 0.920 - ETA: 4:17 - loss: 0.1948 - acc: 0.920 - ETA: 4:11 - loss: 0.1948 - acc: 0.920 - ETA: 4:05 - loss: 0.1950 - acc: 0.920 - ETA: 3:59 - loss: 0.1949 - acc: 0.920 - ETA: 3:54 - loss: 0.1945 - acc: 0.920 - ETA: 3:48 - loss: 0.1943 - acc: 0.920 - ETA: 3:42 - loss: 0.1939 - acc: 0.921 - ETA: 3:36 - loss: 0.1949 - acc: 0.920 - ETA: 3:30 - loss: 0.1949 - acc: 0.920 - ETA: 3:25 - loss: 0.1949 - acc: 0.920 - ETA: 3:19 - loss: 0.1952 - acc: 0.920 - ETA: 3:13 - loss: 0.1951 - acc: 0.920 - ETA: 3:07 - loss: 0.1951 - acc: 0.920 - ETA: 3:01 - loss: 0.1948 - acc: 0.920 - ETA: 2:55 - loss: 0.1947 - acc: 0.920 - ETA: 2:50 - loss: 0.1946 - acc: 0.920 - ETA: 2:44 - loss: 0.1944 - acc: 0.921 - ETA: 2:38 - loss: 0.1946 - acc: 0.920 - ETA: 2:32 - loss: 0.1948 - acc: 0.920 - ETA: 2:26 - loss: 0.1947 - acc: 0.920 - ETA: 2:20 - loss: 0.1946 - acc: 0.921 - ETA: 2:14 - loss: 0.1946 - acc: 0.921 - ETA: 2:09 - loss: 0.1943 - acc: 0.921 - ETA: 2:03 - loss: 0.1941 - acc: 0.921 - ETA: 1:57 - loss: 0.1943 - acc: 0.921 - ETA: 1:51 - loss: 0.1942 - acc: 0.921 - ETA: 1:45 - loss: 0.1939 - acc: 0.921 - ETA: 1:39 - loss: 0.1935 - acc: 0.921 - ETA: 1:33 - loss: 0.1934 - acc: 0.921 - ETA: 1:28 - loss: 0.1935 - acc: 0.921 - ETA: 1:22 - loss: 0.1940 - acc: 0.921 - ETA: 1:16 - loss: 0.1936 - acc: 0.921 - ETA: 1:10 - loss: 0.1936 - acc: 0.921 - ETA: 1:04 - loss: 0.1941 - acc: 0.921 - ETA: 58s - loss: 0.1942 - acc: 0.921 - ETA: 52s - loss: 0.1942 - acc: 0.92 - ETA: 46s - loss: 0.1943 - acc: 0.92 - ETA: 41s - loss: 0.1946 - acc: 0.92 - ETA: 35s - loss: 0.1951 - acc: 0.92 - ETA: 29s - loss: 0.1950 - acc: 0.92 - ETA: 23s - loss: 0.1950 - acc: 0.92 - ETA: 17s - loss: 0.1956 - acc: 0.92 - ETA: 11s - loss: 0.1954 - acc: 0.92 - ETA: 5s - loss: 0.1952 - acc: 0.9209 - 1806s 6s/step - loss: 0.1949 - acc: 0.9210 - val_loss: 0.1963 - val_acc: 0.9185\n",
      "Epoch 5/5\n",
      "  1/282 [..............................] - ETA: 57s - loss: 0.3196 - acc: 0.8679"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.350567). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2/282 [..............................] - ETA: 1:55 - loss: 0.2612 - acc: 0.8971"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.394500). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3/282 [..............................] - ETA: 2:31 - loss: 0.2853 - acc: 0.8875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.398120). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4/282 [..............................] - ETA: 2:42 - loss: 0.2554 - acc: 0.9037"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.381179). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6/282 [..............................] - ETA: 2:45 - loss: 0.2539 - acc: 0.903 - ETA: 2:45 - loss: 0.2524 - acc: 0.9025"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.405323). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9/282 [..............................] - ETA: 2:51 - loss: 0.2509 - acc: 0.902 - ETA: 2:37 - loss: 0.2450 - acc: 0.905 - ETA: 2:26 - loss: 0.2495 - acc: 0.9026"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.364239). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10/282 [>.............................] - ETA: 2:24 - loss: 0.2363 - acc: 0.9091"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.357403). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11/282 [>.............................] - ETA: 2:16 - loss: 0.2328 - acc: 0.9115"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.316104). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17/282 [>.............................] - ETA: 3:37 - loss: 0.2388 - acc: 0.907 - ETA: 6:31 - loss: 0.2307 - acc: 0.910 - ETA: 8:55 - loss: 0.2288 - acc: 0.911 - ETA: 11:06 - loss: 0.2277 - acc: 0.91 - ETA: 12:52 - loss: 0.2248 - acc: 0.91 - ETA: 14:26 - loss: 0.2257 - acc: 0.9117"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.278016). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19/282 [=>............................] - ETA: 15:46 - loss: 0.2256 - acc: 0.91 - ETA: 16:54 - loss: 0.2237 - acc: 0.9114"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.184068). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20/282 [=>............................] - ETA: 18:01 - loss: 0.2210 - acc: 0.9126"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.196761). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224/282 [======================>.......] - ETA: 18:53 - loss: 0.2243 - acc: 0.91 - ETA: 19:38 - loss: 0.2232 - acc: 0.91 - ETA: 19:52 - loss: 0.2223 - acc: 0.91 - ETA: 20:04 - loss: 0.2200 - acc: 0.91 - ETA: 20:14 - loss: 0.2176 - acc: 0.91 - ETA: 20:23 - loss: 0.2209 - acc: 0.91 - ETA: 20:31 - loss: 0.2171 - acc: 0.91 - ETA: 20:39 - loss: 0.2158 - acc: 0.91 - ETA: 20:45 - loss: 0.2142 - acc: 0.91 - ETA: 20:52 - loss: 0.2144 - acc: 0.91 - ETA: 20:58 - loss: 0.2144 - acc: 0.91 - ETA: 21:06 - loss: 0.2147 - acc: 0.91 - ETA: 21:09 - loss: 0.2133 - acc: 0.91 - ETA: 21:12 - loss: 0.2143 - acc: 0.91 - ETA: 21:13 - loss: 0.2135 - acc: 0.91 - ETA: 21:14 - loss: 0.2134 - acc: 0.91 - ETA: 21:14 - loss: 0.2118 - acc: 0.91 - ETA: 21:15 - loss: 0.2091 - acc: 0.91 - ETA: 21:16 - loss: 0.2071 - acc: 0.91 - ETA: 21:16 - loss: 0.2049 - acc: 0.91 - ETA: 21:15 - loss: 0.2061 - acc: 0.91 - ETA: 21:12 - loss: 0.2047 - acc: 0.91 - ETA: 21:10 - loss: 0.2039 - acc: 0.91 - ETA: 21:07 - loss: 0.2033 - acc: 0.91 - ETA: 21:06 - loss: 0.2027 - acc: 0.91 - ETA: 21:03 - loss: 0.2007 - acc: 0.91 - ETA: 20:59 - loss: 0.1991 - acc: 0.91 - ETA: 20:57 - loss: 0.1994 - acc: 0.91 - ETA: 20:55 - loss: 0.1972 - acc: 0.92 - ETA: 20:52 - loss: 0.1977 - acc: 0.92 - ETA: 20:51 - loss: 0.1966 - acc: 0.92 - ETA: 20:49 - loss: 0.1978 - acc: 0.92 - ETA: 20:46 - loss: 0.1975 - acc: 0.92 - ETA: 20:43 - loss: 0.1985 - acc: 0.92 - ETA: 20:40 - loss: 0.1975 - acc: 0.92 - ETA: 20:37 - loss: 0.1983 - acc: 0.91 - ETA: 20:33 - loss: 0.1980 - acc: 0.92 - ETA: 20:30 - loss: 0.1989 - acc: 0.91 - ETA: 20:26 - loss: 0.1980 - acc: 0.92 - ETA: 20:22 - loss: 0.1971 - acc: 0.92 - ETA: 20:17 - loss: 0.1962 - acc: 0.92 - ETA: 20:14 - loss: 0.1952 - acc: 0.92 - ETA: 20:11 - loss: 0.1947 - acc: 0.92 - ETA: 20:07 - loss: 0.1941 - acc: 0.92 - ETA: 20:03 - loss: 0.1945 - acc: 0.92 - ETA: 19:59 - loss: 0.1975 - acc: 0.92 - ETA: 19:54 - loss: 0.1971 - acc: 0.92 - ETA: 19:50 - loss: 0.1989 - acc: 0.92 - ETA: 19:45 - loss: 0.1993 - acc: 0.91 - ETA: 19:41 - loss: 0.2000 - acc: 0.91 - ETA: 19:36 - loss: 0.2006 - acc: 0.91 - ETA: 19:32 - loss: 0.2004 - acc: 0.91 - ETA: 19:27 - loss: 0.2002 - acc: 0.91 - ETA: 19:22 - loss: 0.2009 - acc: 0.91 - ETA: 19:17 - loss: 0.2018 - acc: 0.91 - ETA: 19:11 - loss: 0.2013 - acc: 0.91 - ETA: 19:07 - loss: 0.2016 - acc: 0.91 - ETA: 19:02 - loss: 0.2019 - acc: 0.91 - ETA: 18:57 - loss: 0.2018 - acc: 0.91 - ETA: 18:53 - loss: 0.2018 - acc: 0.91 - ETA: 18:48 - loss: 0.2013 - acc: 0.91 - ETA: 18:43 - loss: 0.2007 - acc: 0.91 - ETA: 18:39 - loss: 0.2006 - acc: 0.91 - ETA: 18:34 - loss: 0.2001 - acc: 0.91 - ETA: 18:29 - loss: 0.2000 - acc: 0.91 - ETA: 18:24 - loss: 0.2000 - acc: 0.91 - ETA: 18:19 - loss: 0.1993 - acc: 0.91 - ETA: 18:14 - loss: 0.1988 - acc: 0.91 - ETA: 18:10 - loss: 0.1987 - acc: 0.91 - ETA: 18:05 - loss: 0.1999 - acc: 0.91 - ETA: 18:01 - loss: 0.1991 - acc: 0.91 - ETA: 17:56 - loss: 0.2001 - acc: 0.91 - ETA: 17:51 - loss: 0.1995 - acc: 0.91 - ETA: 17:47 - loss: 0.1994 - acc: 0.91 - ETA: 17:42 - loss: 0.1996 - acc: 0.91 - ETA: 17:37 - loss: 0.2006 - acc: 0.91 - ETA: 17:32 - loss: 0.2010 - acc: 0.91 - ETA: 17:27 - loss: 0.2010 - acc: 0.91 - ETA: 17:22 - loss: 0.2011 - acc: 0.91 - ETA: 17:17 - loss: 0.2016 - acc: 0.91 - ETA: 17:12 - loss: 0.2011 - acc: 0.91 - ETA: 17:07 - loss: 0.2012 - acc: 0.91 - ETA: 17:01 - loss: 0.2005 - acc: 0.91 - ETA: 16:56 - loss: 0.2008 - acc: 0.91 - ETA: 16:51 - loss: 0.2009 - acc: 0.91 - ETA: 16:46 - loss: 0.2006 - acc: 0.91 - ETA: 16:41 - loss: 0.1998 - acc: 0.91 - ETA: 16:36 - loss: 0.1997 - acc: 0.91 - ETA: 16:30 - loss: 0.1992 - acc: 0.91 - ETA: 16:24 - loss: 0.1989 - acc: 0.91 - ETA: 16:19 - loss: 0.1979 - acc: 0.92 - ETA: 16:13 - loss: 0.1977 - acc: 0.92 - ETA: 16:08 - loss: 0.1970 - acc: 0.92 - ETA: 16:03 - loss: 0.1966 - acc: 0.92 - ETA: 15:58 - loss: 0.1960 - acc: 0.92 - ETA: 15:52 - loss: 0.1966 - acc: 0.92 - ETA: 15:47 - loss: 0.1959 - acc: 0.92 - ETA: 15:42 - loss: 0.1950 - acc: 0.92 - ETA: 15:37 - loss: 0.1946 - acc: 0.92 - ETA: 15:31 - loss: 0.1945 - acc: 0.92 - ETA: 15:26 - loss: 0.1950 - acc: 0.92 - ETA: 15:20 - loss: 0.1944 - acc: 0.92 - ETA: 15:15 - loss: 0.1940 - acc: 0.92 - ETA: 15:10 - loss: 0.1934 - acc: 0.92 - ETA: 15:05 - loss: 0.1930 - acc: 0.92 - ETA: 14:59 - loss: 0.1937 - acc: 0.92 - ETA: 14:54 - loss: 0.1940 - acc: 0.92 - ETA: 14:49 - loss: 0.1936 - acc: 0.92 - ETA: 14:43 - loss: 0.1931 - acc: 0.92 - ETA: 14:37 - loss: 0.1927 - acc: 0.92 - ETA: 14:32 - loss: 0.1928 - acc: 0.92 - ETA: 14:26 - loss: 0.1925 - acc: 0.92 - ETA: 14:20 - loss: 0.1930 - acc: 0.92 - ETA: 14:14 - loss: 0.1928 - acc: 0.92 - ETA: 14:09 - loss: 0.1930 - acc: 0.92 - ETA: 14:03 - loss: 0.1928 - acc: 0.92 - ETA: 13:57 - loss: 0.1924 - acc: 0.92 - ETA: 13:51 - loss: 0.1924 - acc: 0.92 - ETA: 13:46 - loss: 0.1923 - acc: 0.92 - ETA: 13:40 - loss: 0.1929 - acc: 0.92 - ETA: 13:35 - loss: 0.1926 - acc: 0.92 - ETA: 13:29 - loss: 0.1928 - acc: 0.92 - ETA: 13:23 - loss: 0.1928 - acc: 0.92 - ETA: 13:18 - loss: 0.1935 - acc: 0.92 - ETA: 13:12 - loss: 0.1941 - acc: 0.92 - ETA: 13:07 - loss: 0.1946 - acc: 0.92 - ETA: 13:01 - loss: 0.1948 - acc: 0.92 - ETA: 12:56 - loss: 0.1946 - acc: 0.92 - ETA: 12:50 - loss: 0.1951 - acc: 0.92 - ETA: 12:44 - loss: 0.1946 - acc: 0.92 - ETA: 12:39 - loss: 0.1945 - acc: 0.92 - ETA: 12:33 - loss: 0.1940 - acc: 0.92 - ETA: 12:27 - loss: 0.1943 - acc: 0.92 - ETA: 12:22 - loss: 0.1943 - acc: 0.92 - ETA: 12:16 - loss: 0.1941 - acc: 0.92 - ETA: 12:11 - loss: 0.1947 - acc: 0.92 - ETA: 12:05 - loss: 0.1952 - acc: 0.92 - ETA: 12:00 - loss: 0.1951 - acc: 0.92 - ETA: 11:54 - loss: 0.1950 - acc: 0.92 - ETA: 11:48 - loss: 0.1948 - acc: 0.92 - ETA: 11:42 - loss: 0.1950 - acc: 0.92 - ETA: 11:37 - loss: 0.1950 - acc: 0.92 - ETA: 11:31 - loss: 0.1944 - acc: 0.92 - ETA: 11:25 - loss: 0.1944 - acc: 0.92 - ETA: 11:20 - loss: 0.1945 - acc: 0.92 - ETA: 11:14 - loss: 0.1943 - acc: 0.92 - ETA: 11:08 - loss: 0.1945 - acc: 0.92 - ETA: 11:02 - loss: 0.1940 - acc: 0.92 - ETA: 10:56 - loss: 0.1940 - acc: 0.92 - ETA: 10:51 - loss: 0.1937 - acc: 0.92 - ETA: 10:45 - loss: 0.1938 - acc: 0.92 - ETA: 10:39 - loss: 0.1936 - acc: 0.92 - ETA: 10:34 - loss: 0.1935 - acc: 0.92 - ETA: 10:28 - loss: 0.1936 - acc: 0.92 - ETA: 10:22 - loss: 0.1937 - acc: 0.92 - ETA: 10:17 - loss: 0.1936 - acc: 0.92 - ETA: 10:11 - loss: 0.1937 - acc: 0.92 - ETA: 10:05 - loss: 0.1935 - acc: 0.92 - ETA: 10:00 - loss: 0.1936 - acc: 0.92 - ETA: 9:54 - loss: 0.1934 - acc: 0.9223 - ETA: 9:48 - loss: 0.1932 - acc: 0.922 - ETA: 9:42 - loss: 0.1934 - acc: 0.922 - ETA: 9:37 - loss: 0.1933 - acc: 0.922 - ETA: 9:31 - loss: 0.1929 - acc: 0.922 - ETA: 9:25 - loss: 0.1929 - acc: 0.922 - ETA: 9:19 - loss: 0.1926 - acc: 0.922 - ETA: 9:14 - loss: 0.1922 - acc: 0.922 - ETA: 9:08 - loss: 0.1922 - acc: 0.922 - ETA: 9:02 - loss: 0.1921 - acc: 0.922 - ETA: 8:56 - loss: 0.1924 - acc: 0.922 - ETA: 8:50 - loss: 0.1924 - acc: 0.922 - ETA: 8:45 - loss: 0.1925 - acc: 0.922 - ETA: 8:39 - loss: 0.1930 - acc: 0.922 - ETA: 8:33 - loss: 0.1931 - acc: 0.922 - ETA: 8:27 - loss: 0.1928 - acc: 0.922 - ETA: 8:22 - loss: 0.1932 - acc: 0.922 - ETA: 8:16 - loss: 0.1934 - acc: 0.922 - ETA: 8:10 - loss: 0.1932 - acc: 0.922 - ETA: 8:04 - loss: 0.1936 - acc: 0.922 - ETA: 7:58 - loss: 0.1934 - acc: 0.922 - ETA: 7:53 - loss: 0.1933 - acc: 0.922 - ETA: 7:47 - loss: 0.1936 - acc: 0.922 - ETA: 7:41 - loss: 0.1933 - acc: 0.922 - ETA: 7:35 - loss: 0.1931 - acc: 0.922 - ETA: 7:29 - loss: 0.1932 - acc: 0.922 - ETA: 7:24 - loss: 0.1933 - acc: 0.922 - ETA: 7:18 - loss: 0.1927 - acc: 0.922 - ETA: 7:12 - loss: 0.1927 - acc: 0.922 - ETA: 7:06 - loss: 0.1929 - acc: 0.922 - ETA: 7:01 - loss: 0.1929 - acc: 0.922 - ETA: 6:55 - loss: 0.1930 - acc: 0.922 - ETA: 6:49 - loss: 0.1932 - acc: 0.922 - ETA: 6:43 - loss: 0.1936 - acc: 0.922 - ETA: 6:37 - loss: 0.1938 - acc: 0.921 - ETA: 6:31 - loss: 0.1937 - acc: 0.921 - ETA: 6:26 - loss: 0.1940 - acc: 0.921 - ETA: 6:20 - loss: 0.1940 - acc: 0.921 - ETA: 6:14 - loss: 0.1937 - acc: 0.922 - ETA: 6:08 - loss: 0.1935 - acc: 0.922 - ETA: 6:02 - loss: 0.1936 - acc: 0.921 - ETA: 5:56 - loss: 0.1937 - acc: 0.921 - ETA: 5:51 - loss: 0.1940 - acc: 0.921 - ETA: 5:45 - loss: 0.1940 - acc: 0.921 - ETA: 5:39 - loss: 0.1943 - acc: 0.9215"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - ETA: 5:33 - loss: 0.1942 - acc: 0.921 - ETA: 5:27 - loss: 0.1946 - acc: 0.921 - ETA: 5:21 - loss: 0.1948 - acc: 0.921 - ETA: 5:16 - loss: 0.1950 - acc: 0.921 - ETA: 5:10 - loss: 0.1945 - acc: 0.921 - ETA: 5:04 - loss: 0.1943 - acc: 0.921 - ETA: 4:58 - loss: 0.1940 - acc: 0.921 - ETA: 4:52 - loss: 0.1937 - acc: 0.921 - ETA: 4:46 - loss: 0.1935 - acc: 0.921 - ETA: 4:40 - loss: 0.1943 - acc: 0.921 - ETA: 4:35 - loss: 0.1944 - acc: 0.921 - ETA: 4:29 - loss: 0.1943 - acc: 0.921 - ETA: 4:23 - loss: 0.1943 - acc: 0.921 - ETA: 4:17 - loss: 0.1945 - acc: 0.921 - ETA: 4:11 - loss: 0.1946 - acc: 0.921 - ETA: 4:05 - loss: 0.1949 - acc: 0.921 - ETA: 4:00 - loss: 0.1947 - acc: 0.921 - ETA: 3:54 - loss: 0.1946 - acc: 0.921 - ETA: 3:48 - loss: 0.1944 - acc: 0.921 - ETA: 3:42 - loss: 0.1942 - acc: 0.921 - ETA: 3:36 - loss: 0.1941 - acc: 0.921 - ETA: 3:30 - loss: 0.1941 - acc: 0.921 - ETA: 3:24 - loss: 0.1942 - acc: 0.921 - ETA: 3:18 - loss: 0.1942 - acc: 0.921 - ETA: 3:13 - loss: 0.1946 - acc: 0.921 - ETA: 3:07 - loss: 0.1944 - acc: 0.921 - ETA: 3:01 - loss: 0.1946 - acc: 0.921 - ETA: 2:55 - loss: 0.1947 - acc: 0.921 - ETA: 2:49 - loss: 0.1951 - acc: 0.921 - ETA: 2:43 - loss: 0.1950 - acc: 0.921 - ETA: 2:38 - loss: 0.1949 - acc: 0.921 - ETA: 2:32 - loss: 0.1947 - acc: 0.921 - ETA: 2:26 - loss: 0.1945 - acc: 0.921 - ETA: 2:20 - loss: 0.1942 - acc: 0.921 - ETA: 2:14 - loss: 0.1943 - acc: 0.921 - ETA: 2:08 - loss: 0.1948 - acc: 0.921 - ETA: 2:03 - loss: 0.1945 - acc: 0.921 - ETA: 1:57 - loss: 0.1945 - acc: 0.921 - ETA: 1:51 - loss: 0.1946 - acc: 0.921 - ETA: 1:45 - loss: 0.1946 - acc: 0.921 - ETA: 1:39 - loss: 0.1945 - acc: 0.921 - ETA: 1:33 - loss: 0.1945 - acc: 0.921 - ETA: 1:27 - loss: 0.1942 - acc: 0.921 - ETA: 1:22 - loss: 0.1942 - acc: 0.921 - ETA: 1:16 - loss: 0.1941 - acc: 0.921 - ETA: 1:10 - loss: 0.1945 - acc: 0.921 - ETA: 1:04 - loss: 0.1945 - acc: 0.921 - ETA: 58s - loss: 0.1946 - acc: 0.921 - ETA: 52s - loss: 0.1947 - acc: 0.92 - ETA: 46s - loss: 0.1948 - acc: 0.92 - ETA: 41s - loss: 0.1949 - acc: 0.92 - ETA: 35s - loss: 0.1950 - acc: 0.92 - ETA: 29s - loss: 0.1953 - acc: 0.92 - ETA: 23s - loss: 0.1955 - acc: 0.92 - ETA: 17s - loss: 0.1957 - acc: 0.92 - ETA: 11s - loss: 0.1956 - acc: 0.92 - ETA: 5s - loss: 0.1959 - acc: 0.9207 - 1772s 6s/step - loss: 0.1959 - acc: 0.9207 - val_loss: 0.1914 - val_acc: 0.9236\n",
      "Model training time: 151.6 minutes\n"
     ]
    }
   ],
   "source": [
    "columns = ['is_tissue','slide_path','is_tumor','is_all_tumor','tile_loc']\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 5\n",
    "\n",
    "for i in range(len(slide_4_test)):\n",
    "    \n",
    "    # [1] dataset , 2 pos, 2 neg, mean ratio = 3:1\n",
    "    four_samples = pd.DataFrame(columns = columns)\n",
    "    four_image_path = list()\n",
    "    four_mask_path = list()    \n",
    "    for j in range(4):\n",
    "        image_path = image_paths[slide_4_test[i][j]][1:] # 이 부분은 data 읽을때 고치자 ( [1:] 빼야함)\n",
    "        mask_path = tumor_mask_paths[slide_4_test[i][j]][1:] # 이 부분은 data 읽을때 고치자\n",
    "        samples = find_patches_from_slide(image_path, mask_path)\n",
    "        \n",
    "        four_samples = four_samples.append(samples)   \n",
    "        four_image_path.append(image_path)\n",
    "        four_mask_path.append(mask_path)\n",
    "    NUM_SAMPLES = len(four_samples)\n",
    "    if NUM_SAMPLES > 10000:\n",
    "        NUM_SAMPLES = 10000\n",
    "    \n",
    "    samples = four_samples.sample(NUM_SAMPLES, random_state=42)\n",
    "    samples.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    tumor_samples = four_samples[four_samples.is_tumor == True]\n",
    "    print(len(tumor_samples))\n",
    "    non_tumor_samples = four_samples[four_samples.is_tumor == False]\n",
    "    print(len(non_tumor_samples))\n",
    "    non_tumor_samples_3_ratio = non_tumor_samples.sample(len(tumor_samples) * 3, random_state = 42,replace=True)\n",
    "    \n",
    "    all_sample = tumor_samples.append(non_tumor_samples_3_ratio)\n",
    "\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "    for train_index, test_index in split.split(samples, samples[\"is_tumor\"]):\n",
    "            train_samples = samples.loc[train_index]\n",
    "            validation_samples = samples.loc[test_index]\n",
    "    \n",
    "    train_generator = gen_imgs(four_image_path,four_mask_path,train_samples, BATCH_SIZE)\n",
    "    validation_generator = gen_imgs(four_image_path,four_mask_path,validation_samples, BATCH_SIZE)\n",
    "    \n",
    "    train_start_time = datetime.now()\n",
    "    history = model.fit_generator(train_generator, np.ceil(len(train_samples) / BATCH_SIZE),\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=np.ceil(len(validation_samples) / BATCH_SIZE),\n",
    "        epochs=N_EPOCHS)\n",
    "    if file_handles != []:\n",
    "        for fh in file_handles:\n",
    "            fh.close()\n",
    "    file_handles=[]\n",
    "    #del train_generator\n",
    "    #del validation_generator\n",
    "    train_end_time = datetime.now()\n",
    "    print(\"Model training time: %.1f minutes\" % ((train_end_time - train_start_time).seconds / 60,))\n",
    "    model.save('s_512.h5')\n",
    "    # split\n",
    "    # data gen : all_image_path, all_mask_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth dimensions:  (5316, 10007)\n",
      "truth_4_dimension_size: (333, 626)\n",
      "slide4_size (167, 313)\n",
      "Total patches in slide: 25627\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-fc2f5eb18adf>\u001b[0m in \u001b[0;36mgen_imgs\u001b[1;34m(all_image_path, all_mask_path, samples, batch_size, patch_size, shuffle)\u001b[0m\n\u001b[0;32m    282\u001b[0m                 \u001b[0mhorizontal_flip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m                 \u001b[0mvertical_flip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m                 brightness_range =(0.4,1.)).flow(X_train,y=y_train,batch_size=batch_size))\n\u001b[0m\u001b[0;32m    285\u001b[0m             \u001b[1;31m#print(X_train.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m             \u001b[1;31m#print(y_train.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;31m# The transformation of images is not under thread lock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;31m# so it can be done in parallel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras_preprocessing\\image\\numpy_array_iterator.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[1;34m(self, index_array)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 gen time: 17.8 minutes\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'example_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-30222dae9c4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_from_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mpred_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpred_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexample_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'example_X' is not defined"
     ]
    }
   ],
   "source": [
    "ipath = all_image_path[0]\n",
    "tpath = all_mask_path[0]\n",
    "\n",
    "all_tissue_samples = find_patches_from_slide(ipath,tpath)\n",
    "print('Total patches in slide: %d' % len(all_tissue_samples)) \n",
    "all_tissue_samples.iloc[:10]\n",
    "all_tissue_samples.is_tumor.value_counts() \n",
    "test_start_time = datetime.now()\n",
    "sample_gen = gen_imgs(all_image_path,all_mask_path,all_tissue_samples, 5000, shuffle=True)\n",
    "%time example_X, example_y  = next(sample_gen)\n",
    "test_end_time = datetime.now()\n",
    "print(\"5000 gen time: %.1f minutes\" % ((test_end_time - test_start_time).seconds / 60,))\n",
    "\n",
    "start_x = PATCH_SIZE//4\n",
    "start_y = PATCH_SIZE//4\n",
    "pred_size = PATCH_SIZE//2\n",
    "\n",
    "test_start_time = datetime.now()\n",
    "preds = []\n",
    "labels = []\n",
    "for i in range(5000):\n",
    "    prediction = predict_from_model(example_X[i],model)\n",
    "    pred_X = np.zeros((pred_size,pred_size))\n",
    "    y = example_y[i].argmax\n",
    "    for x in range(start_x,start_x+pred_size):\n",
    "        for y in range(start_y, start_y+pred_size):\n",
    "            pred_X[x-start_x][y-start_y] = prediction[x][y]\n",
    "            \n",
    "    pred_s = pd.Series(pred_X.flatten())\n",
    "    max_p = np.max(pred_s)\n",
    "    \n",
    "    y = np.max(example_y[i].argmax(axis=2))\n",
    "    preds.append(max_p)\n",
    "    labels.append(y)\n",
    "test_end_time = datetime.now()\n",
    "print(\"Model test time: %.1f minutes\" % ((test_end_time - test_start_time).seconds / 60,))    \n",
    "fpr, tpr, thresholds = metrics.roc_curve(labels,preds,pos_label=1)\n",
    "print(metrics.auc(fpr,tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipath = all_image_path[0]\n",
    "tpath = all_mask_path[0]\n",
    "\n",
    "all_tissue_samples = find_patches_from_slide(ipath,tpath)\n",
    "print('Total patches in slide: %d' % len(all_tissue_samples)) \n",
    "all_tissue_samples.iloc[:10]\n",
    "all_tissue_samples.is_tumor.value_counts() \n",
    "test_start_time = datetime.now()\n",
    "sample_gen = gen_imgs(all_image_path,all_mask_path,all_tissue_samples, 5000, shuffle=True)\n",
    "%time example_X, example_y  = next(sample_gen)\n",
    "test_end_time = datetime.now()\n",
    "print(\"5000 gen time: %.1f minutes\" % ((test_end_time - test_start_time).seconds / 60,))\n",
    "\n",
    "start_x = PATCH_SIZE//4\n",
    "start_y = PATCH_SIZE//4\n",
    "pred_size = PATCH_SIZE//2\n",
    "\n",
    "test_start_time = datetime.now()\n",
    "preds = []\n",
    "labels = []\n",
    "for i in range(5000):\n",
    "    prediction = predict_from_model(example_X[i],model)\n",
    "    pred_X = np.zeros((pred_size,pred_size))\n",
    "    y = example_y[i].argmax\n",
    "    for x in range(start_x,start_x+pred_size):\n",
    "        for y in range(start_y, start_y+pred_size):\n",
    "            pred_X[x-start_x][y-start_y] = prediction[x][y]\n",
    "            \n",
    "    pred_s = pd.Series(pred_X.flatten())\n",
    "    max_p = np.max(pred_s)\n",
    "    \n",
    "    y = np.max(example_y[i].argmax(axis=2))\n",
    "    preds.append(max_p)\n",
    "    labels.append(y)\n",
    "test_end_time = datetime.now()\n",
    "print(\"Model test time: %.1f minutes\" % ((test_end_time - test_start_time).seconds / 60,))    \n",
    "fpr, tpr, thresholds = metrics.roc_curve(labels,preds,pos_label=1)\n",
    "print(metrics.auc(fpr,tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipath = all_image_path[0]\n",
    "tpath = all_mask_path[0]\n",
    "\n",
    "all_tissue_samples = find_patches_from_slide(ipath,tpath)\n",
    "print('Total patches in slide: %d' % len(all_tissue_samples)) \n",
    "all_tissue_samples.iloc[:10]\n",
    "all_tissue_samples.is_tumor.value_counts() \n",
    "test_start_time = datetime.now()\n",
    "sample_gen = gen_imgs(all_image_path,all_mask_path,all_tissue_samples, 5000, shuffle=True)\n",
    "%time example_X, example_y  = next(sample_gen)\n",
    "test_end_time = datetime.now()\n",
    "print(\"5000 gen time: %.1f minutes\" % ((test_end_time - test_start_time).seconds / 60,))\n",
    "\n",
    "start_x = PATCH_SIZE//4\n",
    "start_y = PATCH_SIZE//4\n",
    "pred_size = PATCH_SIZE//2\n",
    "\n",
    "test_start_time = datetime.now()\n",
    "preds = []\n",
    "labels = []\n",
    "for i in range(5000):\n",
    "    prediction = predict_from_model(example_X[i],model)\n",
    "    pred_X = np.zeros((pred_size,pred_size))\n",
    "    y = example_y[i].argmax\n",
    "    pred_s = pd.Series(prediction.flatten())\n",
    "    max_p = np.max(pred_s)\n",
    "    \n",
    "    y = np.max(example_y[i].argmax(axis=2))\n",
    "    preds.append(max_p)\n",
    "    labels.append(y)\n",
    "test_end_time = datetime.now()\n",
    "print(\"Model test time: %.1f minutes\" % ((test_end_time - test_start_time).seconds / 60,))    \n",
    "fpr, tpr, thresholds = metrics.roc_curve(labels,preds,pos_label=1)\n",
    "print(metrics.auc(fpr,tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipath = all_image_path[0]\n",
    "tpath = all_mask_path[0]\n",
    "\n",
    "all_tissue_samples = find_patches_from_slide(ipath,tpath)\n",
    "print('Total patches in slide: %d' % len(all_tissue_samples)) \n",
    "all_tissue_samples.iloc[:10]\n",
    "all_tissue_samples.is_tumor.value_counts() \n",
    "test_start_time = datetime.now()\n",
    "sample_gen = gen_imgs(all_image_path,all_mask_path,all_tissue_samples, 5000, shuffle=True)\n",
    "%time example_X, example_y  = next(sample_gen)\n",
    "test_end_time = datetime.now()\n",
    "print(\"5000 gen time: %.1f minutes\" % ((test_end_time - test_start_time).seconds / 60,))\n",
    "\n",
    "start_x = 32\n",
    "start_y = 32\n",
    "pred_size = 192\n",
    "\n",
    "test_start_time = datetime.now()\n",
    "preds = []\n",
    "labels = []\n",
    "for i in range(5000):\n",
    "    prediction = predict_from_model(example_X[i],model)\n",
    "    pred_X = np.zeros((pred_size,pred_size))\n",
    "    y = example_y[i].argmax\n",
    "    for x in range(start_x,start_x+pred_size):\n",
    "        for y in range(start_y, start_y+pred_size):\n",
    "            pred_X[x-start_x][y-start_y] = prediction[x][y]\n",
    "            \n",
    "    pred_s = pd.Series(pred_X.flatten())\n",
    "    max_p = np.max(pred_s)\n",
    "    \n",
    "    y = np.max(example_y[i].argmax(axis=2))\n",
    "    preds.append(max_p)\n",
    "    labels.append(y)\n",
    "test_end_time = datetime.now()\n",
    "print(\"Model test time: %.1f minutes\" % ((test_end_time - test_start_time).seconds / 60,))    \n",
    "fpr, tpr, thresholds = metrics.roc_curve(labels,preds,pos_label=1)\n",
    "print(metrics.auc(fpr,tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004315432452131063"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_pred_x = np.max(preds)\n",
    "max_pred_x\n",
    "\n",
    "min_pred_x = np.min(preds)\n",
    "min_pred_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model test time: 0.0 minutes\n"
     ]
    }
   ],
   "source": [
    "test_start_time = datetime.now()\n",
    "slide = openslide.open_slide(ipath)\n",
    "tiles = DeepZoomGenerator(slide,tile_size=256,overlap=0, limit_bounds=False) \n",
    "test_end_time = datetime.now()\n",
    "print(\"Model test time: %.1f minutes\" % ((test_end_time - test_start_time).seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
